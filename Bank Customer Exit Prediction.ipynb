{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My very first project with supervised deep learning. I have been reading \"Deep Learning with Python\" by fran√ßois chollet. I am not finished with it, but I really wanted to create my own solution on a given data set. \n",
    "\n",
    "The dataset I will be using is https://www.kaggle.com/sonalidasgupta95/churn-prediction-of-bank-customers/downloads/churn-prediction-of-bank-customers.zip/1\n",
    "It will be a binary classification problem to determine if a customer will exit the bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('Churn_Modelling.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just looking at the data, I will parseout columns 0, 1, 2: Row Number, CustomerId, Surname as these features should have no impact on the prediction of someone exiting the bank. Thus, I am going to use CreditScore, Geography, Gender, Age, Tenure, Balance, NumOfProducts, HasCrCard, IsActiveMember, and EstimatedSalary to determine whether the customer will exit. Also, I will preprocess the data that are inconvienient for the network\n",
    "    - Geography: France will be 0, Spain will be 1, and Germany will be 2\n",
    "    - Gender: Female will be 0, Male will be 1\n",
    "    - Balance: Get the mean then divide by the standard deviation\n",
    "    - EstimatedSalary: Get the mean then divide by the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'Exited']\n",
      "[6.1900000e+02 0.0000000e+00 0.0000000e+00 4.2000000e+01 2.0000000e+00\n",
      " 0.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0134888e+05\n",
      " 1.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# Getting Data\n",
    "with open('Churn_Modelling.csv', 'r') as f:\n",
    "    data = list(csv.reader(f, delimiter=';'))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data[i] = data[i][0].split(',')\n",
    "    \n",
    "    # Switching gender to numeric\n",
    "    if data[i][5] == \"Female\":\n",
    "        data[i][5] = 0\n",
    "    elif data[i][5] == \"Male\":\n",
    "        data[i][5] = 1\n",
    "    \n",
    "    # Switching geography to numeric\n",
    "    if data[i][4] == \"France\":\n",
    "        data[i][4] = 0\n",
    "    elif data[i][4] == \"Spain\":\n",
    "        data[i][4] = 1\n",
    "    elif data[i][4] == \"Germany\":\n",
    "        data[i][4] = 2\n",
    "    \n",
    "    data[i].pop(2)\n",
    "    data[i].pop(1)\n",
    "    data[i].pop(0)\n",
    "    \n",
    "    if i != 0:\n",
    "        for j in range(len(data[i])):\n",
    "            data[i][j] = float(data[i][j])\n",
    "            \n",
    "print(data[0])\n",
    "data.pop(0)\n",
    "data = np.array(data, np.float64)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above shows you an example of data formatted in which order. With the data dissected, I am going to cut the data into a training data and testing data. 75% of the data will be used for training and the rest for testing. I will also create a target array for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Splitting the data into train and test\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# 75% of the data will be training data\n",
    "numTrain = math.floor(len(data) * .8)\n",
    "\n",
    "trainData = data[:numTrain]\n",
    "testData = data[numTrain:]\n",
    "\n",
    "trainTarget = []\n",
    "testTarget = []\n",
    "\n",
    "for i in range(len(trainData)):\n",
    "    trainTarget.append(trainData[i][10])\n",
    "for i in range(len(testData)):\n",
    "    testTarget.append(testData[i][10])\n",
    "    \n",
    "trainData = np.delete(trainData, 10, 1)\n",
    "testData = np.delete(testData, 10, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having split up the data, it is ready for it's final data preprocessig. Data features: CreditScore, Balance, and Salary are normalized. By subtracting the feature's mean off it's own and dividing it by the standard deviation, I can make the range of the data smaller. This is so that the network does not encounter wide extreme numbers on input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.38657167  0.          1.         40.          4.         -1.22622699\n",
      "  1.          0.          0.          0.89243784]\n",
      "[ 0.59997706  2.          1.         45.          4.          0.74275906\n",
      "  1.          1.          1.         -1.73830093]\n"
     ]
    }
   ],
   "source": [
    "# Normalization\n",
    "credit = []\n",
    "salary = []\n",
    "balance = []\n",
    "\n",
    "for i in range(len(trainData)):\n",
    "    credit.append(trainData[i][0])\n",
    "    balance.append(trainData[i][5])\n",
    "    salary.append(trainData[i][9])\n",
    "\n",
    "creditMean = np.mean(credit)\n",
    "salaryMean = np.mean(salary)\n",
    "balanceMean = np.mean(balance)\n",
    "creditStd = np.std(credit)\n",
    "salaryStd = np.std(salary)\n",
    "balanceStd = np.std(balance)\n",
    "\n",
    "trainData[:, 0] -= creditMean\n",
    "trainData[:, 0] /= creditStd\n",
    "trainData[:, 5] -= balanceMean\n",
    "trainData[:, 5] /= balanceStd\n",
    "trainData[:, 9] -= salaryMean\n",
    "trainData[:, 9] /= salaryStd\n",
    "\n",
    "testData[:, 0] -= creditMean\n",
    "testData[:, 0] /= creditStd\n",
    "testData[:, 5] -= balanceMean\n",
    "testData[:, 5] /= balanceStd\n",
    "testData[:, 9] -= salaryMean\n",
    "testData[:, 9] /= salaryStd\n",
    "\n",
    "numVal = math.floor(len(trainData) * .10)\n",
    "valData = trainData[:numVal]\n",
    "valTarget = trainTarget[:numVal]\n",
    "trainData = trainData[numVal:]\n",
    "trainTarget = trainTarget[numVal:]\n",
    "\n",
    "print(trainData[0])\n",
    "print(testData[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above shows how the range of the features have drastically decreased. Credit score, salary, and balance are now all normalized and ready to be inputted into our model. \n",
    "The network model has 4 layers:\n",
    "    Input Layer : size of 10,\n",
    "    Hidden Layer 1 : size of 64,\n",
    "    Hidden LAyer 2 : size of 64,\n",
    "    Output LAyer : size of 1.\n",
    "    \n",
    "As for the compilers, I am testing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/300\n",
      "7200/7200 [==============================] - 0s 47us/step - loss: 0.5885 - acc: 0.7851 - val_loss: 0.5454 - val_acc: 0.7850\n",
      "Epoch 2/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.5101 - acc: 0.7997 - val_loss: 0.5039 - val_acc: 0.7850\n",
      "Epoch 3/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4912 - acc: 0.7997 - val_loss: 0.4864 - val_acc: 0.7850\n",
      "Epoch 4/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.4656 - acc: 0.8022 - val_loss: 0.4600 - val_acc: 0.8125\n",
      "Epoch 5/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.4666 - acc: 0.8078 - val_loss: 0.4420 - val_acc: 0.8075\n",
      "Epoch 6/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4470 - acc: 0.8107 - val_loss: 0.4350 - val_acc: 0.8037\n",
      "Epoch 7/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.4476 - acc: 0.8143 - val_loss: 0.4313 - val_acc: 0.8087\n",
      "Epoch 8/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4417 - acc: 0.8168 - val_loss: 0.4416 - val_acc: 0.8137\n",
      "Epoch 9/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4405 - acc: 0.8190 - val_loss: 0.4350 - val_acc: 0.8025\n",
      "Epoch 10/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4447 - acc: 0.8139 - val_loss: 0.4503 - val_acc: 0.7925\n",
      "Epoch 11/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.4495 - acc: 0.8100 - val_loss: 0.4281 - val_acc: 0.8100\n",
      "Epoch 12/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4375 - acc: 0.8142 - val_loss: 0.4415 - val_acc: 0.7963\n",
      "Epoch 13/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.4343 - acc: 0.8176 - val_loss: 0.4280 - val_acc: 0.8163\n",
      "Epoch 14/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.4330 - acc: 0.8181 - val_loss: 0.4288 - val_acc: 0.8137\n",
      "Epoch 15/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.4375 - acc: 0.8160 - val_loss: 0.4295 - val_acc: 0.8075\n",
      "Epoch 16/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4333 - acc: 0.8190 - val_loss: 0.4241 - val_acc: 0.8163\n",
      "Epoch 17/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4330 - acc: 0.8183 - val_loss: 0.4289 - val_acc: 0.8087\n",
      "Epoch 18/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.4303 - acc: 0.8228 - val_loss: 0.4437 - val_acc: 0.8113\n",
      "Epoch 19/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4308 - acc: 0.8185 - val_loss: 0.4351 - val_acc: 0.8000\n",
      "Epoch 20/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4294 - acc: 0.8176 - val_loss: 0.4216 - val_acc: 0.8087\n",
      "Epoch 21/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.4289 - acc: 0.8178 - val_loss: 0.4361 - val_acc: 0.7975\n",
      "Epoch 22/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4299 - acc: 0.8179 - val_loss: 0.4226 - val_acc: 0.8125\n",
      "Epoch 23/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4296 - acc: 0.8187 - val_loss: 0.4309 - val_acc: 0.8050\n",
      "Epoch 24/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4282 - acc: 0.8219 - val_loss: 0.4220 - val_acc: 0.8200\n",
      "Epoch 25/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.4240 - acc: 0.8206 - val_loss: 0.4232 - val_acc: 0.8150\n",
      "Epoch 26/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4237 - acc: 0.8207 - val_loss: 0.4308 - val_acc: 0.8025\n",
      "Epoch 27/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4305 - acc: 0.8181 - val_loss: 0.4329 - val_acc: 0.8025\n",
      "Epoch 28/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4325 - acc: 0.8151 - val_loss: 0.4408 - val_acc: 0.8050\n",
      "Epoch 29/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4304 - acc: 0.8137 - val_loss: 0.4225 - val_acc: 0.8150\n",
      "Epoch 30/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4231 - acc: 0.8214 - val_loss: 0.4251 - val_acc: 0.8137\n",
      "Epoch 31/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4253 - acc: 0.8193 - val_loss: 0.4259 - val_acc: 0.8100\n",
      "Epoch 32/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.4288 - acc: 0.8139 - val_loss: 0.4236 - val_acc: 0.8137\n",
      "Epoch 33/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4312 - acc: 0.8129 - val_loss: 0.4482 - val_acc: 0.7950\n",
      "Epoch 34/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4257 - acc: 0.8197 - val_loss: 0.4282 - val_acc: 0.8087\n",
      "Epoch 35/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.4223 - acc: 0.8221 - val_loss: 0.4205 - val_acc: 0.8163\n",
      "Epoch 36/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.4231 - acc: 0.8199 - val_loss: 0.4439 - val_acc: 0.8013\n",
      "Epoch 37/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4261 - acc: 0.8156 - val_loss: 0.4257 - val_acc: 0.8075\n",
      "Epoch 38/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4199 - acc: 0.8212 - val_loss: 0.4287 - val_acc: 0.8150\n",
      "Epoch 39/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4226 - acc: 0.8167 - val_loss: 0.4164 - val_acc: 0.8163\n",
      "Epoch 40/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.4212 - acc: 0.8204 - val_loss: 0.4215 - val_acc: 0.8175\n",
      "Epoch 41/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.4167 - acc: 0.8225 - val_loss: 0.4125 - val_acc: 0.8163\n",
      "Epoch 42/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4164 - acc: 0.8249 - val_loss: 0.4218 - val_acc: 0.8150\n",
      "Epoch 43/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.4188 - acc: 0.8192 - val_loss: 0.4194 - val_acc: 0.8213\n",
      "Epoch 44/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4160 - acc: 0.8210 - val_loss: 0.4158 - val_acc: 0.8137\n",
      "Epoch 45/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4189 - acc: 0.8193 - val_loss: 0.4167 - val_acc: 0.8213\n",
      "Epoch 46/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4091 - acc: 0.8246 - val_loss: 0.4120 - val_acc: 0.8200\n",
      "Epoch 47/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.4120 - acc: 0.8240 - val_loss: 0.4233 - val_acc: 0.8163\n",
      "Epoch 48/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.4111 - acc: 0.8253 - val_loss: 0.4084 - val_acc: 0.8250\n",
      "Epoch 49/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4057 - acc: 0.8288 - val_loss: 0.4151 - val_acc: 0.8163\n",
      "Epoch 50/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.4056 - acc: 0.8285 - val_loss: 0.4074 - val_acc: 0.8287\n",
      "Epoch 51/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4075 - acc: 0.8242 - val_loss: 0.4090 - val_acc: 0.8200\n",
      "Epoch 52/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4001 - acc: 0.8322 - val_loss: 0.4066 - val_acc: 0.8300\n",
      "Epoch 53/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4032 - acc: 0.8293 - val_loss: 0.4030 - val_acc: 0.8250\n",
      "Epoch 54/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.4037 - acc: 0.8258 - val_loss: 0.4088 - val_acc: 0.8275\n",
      "Epoch 55/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4068 - acc: 0.8258 - val_loss: 0.4045 - val_acc: 0.8200\n",
      "Epoch 56/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3963 - acc: 0.8317 - val_loss: 0.4095 - val_acc: 0.8275\n",
      "Epoch 57/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3977 - acc: 0.8289 - val_loss: 0.4026 - val_acc: 0.8363\n",
      "Epoch 58/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4032 - acc: 0.8301 - val_loss: 0.4014 - val_acc: 0.8200\n",
      "Epoch 59/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3969 - acc: 0.8314 - val_loss: 0.4020 - val_acc: 0.8213\n",
      "Epoch 60/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3904 - acc: 0.8356 - val_loss: 0.4147 - val_acc: 0.8237\n",
      "Epoch 61/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3915 - acc: 0.8326 - val_loss: 0.3973 - val_acc: 0.8237\n",
      "Epoch 62/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3870 - acc: 0.8365 - val_loss: 0.4082 - val_acc: 0.8287\n",
      "Epoch 63/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3869 - acc: 0.8368 - val_loss: 0.3997 - val_acc: 0.8275\n",
      "Epoch 64/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3851 - acc: 0.8365 - val_loss: 0.4047 - val_acc: 0.8287\n",
      "Epoch 65/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3835 - acc: 0.8375 - val_loss: 0.3959 - val_acc: 0.8325\n",
      "Epoch 66/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3854 - acc: 0.8381 - val_loss: 0.3978 - val_acc: 0.8300\n",
      "Epoch 67/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3826 - acc: 0.8369 - val_loss: 0.4047 - val_acc: 0.8313\n",
      "Epoch 68/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3806 - acc: 0.8400 - val_loss: 0.3969 - val_acc: 0.8313\n",
      "Epoch 69/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3824 - acc: 0.8356 - val_loss: 0.3911 - val_acc: 0.8387\n",
      "Epoch 70/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3803 - acc: 0.8410 - val_loss: 0.4134 - val_acc: 0.8275\n",
      "Epoch 71/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3818 - acc: 0.8381 - val_loss: 0.3918 - val_acc: 0.8350\n",
      "Epoch 72/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3824 - acc: 0.8440 - val_loss: 0.4045 - val_acc: 0.8275\n",
      "Epoch 73/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3782 - acc: 0.8397 - val_loss: 0.4050 - val_acc: 0.8313\n",
      "Epoch 74/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3789 - acc: 0.8393 - val_loss: 0.3950 - val_acc: 0.8387\n",
      "Epoch 75/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3771 - acc: 0.8446 - val_loss: 0.3865 - val_acc: 0.8387\n",
      "Epoch 76/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3786 - acc: 0.8421 - val_loss: 0.3942 - val_acc: 0.8413\n",
      "Epoch 77/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3714 - acc: 0.8435 - val_loss: 0.3878 - val_acc: 0.8400\n",
      "Epoch 78/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3751 - acc: 0.8411 - val_loss: 0.3880 - val_acc: 0.8325\n",
      "Epoch 79/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3747 - acc: 0.8404 - val_loss: 0.3822 - val_acc: 0.8387\n",
      "Epoch 80/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3685 - acc: 0.8465 - val_loss: 0.3823 - val_acc: 0.8375\n",
      "Epoch 81/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3686 - acc: 0.8446 - val_loss: 0.3851 - val_acc: 0.8413\n",
      "Epoch 82/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3673 - acc: 0.8465 - val_loss: 0.3934 - val_acc: 0.8350\n",
      "Epoch 83/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3734 - acc: 0.8425 - val_loss: 0.3915 - val_acc: 0.8387\n",
      "Epoch 84/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3657 - acc: 0.8476 - val_loss: 0.3811 - val_acc: 0.8413\n",
      "Epoch 85/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3668 - acc: 0.8476 - val_loss: 0.3829 - val_acc: 0.8438\n",
      "Epoch 86/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3641 - acc: 0.8488 - val_loss: 0.3830 - val_acc: 0.8363\n",
      "Epoch 87/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3650 - acc: 0.8485 - val_loss: 0.3876 - val_acc: 0.8375\n",
      "Epoch 88/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3627 - acc: 0.8508 - val_loss: 0.3803 - val_acc: 0.8425\n",
      "Epoch 89/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3620 - acc: 0.8506 - val_loss: 0.3781 - val_acc: 0.8438\n",
      "Epoch 90/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3609 - acc: 0.8485 - val_loss: 0.3840 - val_acc: 0.8450\n",
      "Epoch 91/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3592 - acc: 0.8507 - val_loss: 0.3803 - val_acc: 0.8413\n",
      "Epoch 92/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3599 - acc: 0.8490 - val_loss: 0.3738 - val_acc: 0.8475\n",
      "Epoch 93/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3593 - acc: 0.8493 - val_loss: 0.3750 - val_acc: 0.8438\n",
      "Epoch 94/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3614 - acc: 0.8522 - val_loss: 0.3803 - val_acc: 0.8438\n",
      "Epoch 95/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3604 - acc: 0.8542 - val_loss: 0.3732 - val_acc: 0.8488\n",
      "Epoch 96/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3574 - acc: 0.8497 - val_loss: 0.3754 - val_acc: 0.8450\n",
      "Epoch 97/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3586 - acc: 0.8524 - val_loss: 0.3761 - val_acc: 0.8413\n",
      "Epoch 98/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3585 - acc: 0.8497 - val_loss: 0.3717 - val_acc: 0.8438\n",
      "Epoch 99/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3558 - acc: 0.8539 - val_loss: 0.3833 - val_acc: 0.8400\n",
      "Epoch 100/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3569 - acc: 0.8512 - val_loss: 0.3704 - val_acc: 0.8438\n",
      "Epoch 101/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3542 - acc: 0.8556 - val_loss: 0.3748 - val_acc: 0.8400\n",
      "Epoch 102/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3589 - acc: 0.8506 - val_loss: 0.3717 - val_acc: 0.8462\n",
      "Epoch 103/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3554 - acc: 0.8500 - val_loss: 0.3770 - val_acc: 0.8413\n",
      "Epoch 104/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3548 - acc: 0.8535 - val_loss: 0.3671 - val_acc: 0.8438\n",
      "Epoch 105/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3532 - acc: 0.8514 - val_loss: 0.3824 - val_acc: 0.8488\n",
      "Epoch 106/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3567 - acc: 0.8511 - val_loss: 0.3686 - val_acc: 0.8450\n",
      "Epoch 107/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3523 - acc: 0.8521 - val_loss: 0.3713 - val_acc: 0.8488\n",
      "Epoch 108/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3506 - acc: 0.8547 - val_loss: 0.3680 - val_acc: 0.8512\n",
      "Epoch 109/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3506 - acc: 0.8546 - val_loss: 0.3666 - val_acc: 0.8488\n",
      "Epoch 110/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3508 - acc: 0.8533 - val_loss: 0.3829 - val_acc: 0.8475\n",
      "Epoch 111/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3562 - acc: 0.8506 - val_loss: 0.3826 - val_acc: 0.8575\n",
      "Epoch 112/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3507 - acc: 0.8553 - val_loss: 0.3682 - val_acc: 0.8475\n",
      "Epoch 113/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3520 - acc: 0.8540 - val_loss: 0.3676 - val_acc: 0.8488\n",
      "Epoch 114/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3508 - acc: 0.8549 - val_loss: 0.4024 - val_acc: 0.8413\n",
      "Epoch 115/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3512 - acc: 0.8539 - val_loss: 0.3633 - val_acc: 0.8500\n",
      "Epoch 116/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3509 - acc: 0.8525 - val_loss: 0.3693 - val_acc: 0.8462\n",
      "Epoch 117/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3560 - acc: 0.8531 - val_loss: 0.3666 - val_acc: 0.8488\n",
      "Epoch 118/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3465 - acc: 0.8567 - val_loss: 0.3672 - val_acc: 0.8512\n",
      "Epoch 119/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3484 - acc: 0.8557 - val_loss: 0.3644 - val_acc: 0.8538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3516 - acc: 0.8518 - val_loss: 0.3640 - val_acc: 0.8488\n",
      "Epoch 121/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3465 - acc: 0.8556 - val_loss: 0.3700 - val_acc: 0.8488\n",
      "Epoch 122/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3456 - acc: 0.8571 - val_loss: 0.3618 - val_acc: 0.8525\n",
      "Epoch 123/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3487 - acc: 0.8553 - val_loss: 0.3610 - val_acc: 0.8512\n",
      "Epoch 124/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3460 - acc: 0.8568 - val_loss: 0.3604 - val_acc: 0.8475\n",
      "Epoch 125/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3495 - acc: 0.8524 - val_loss: 0.3667 - val_acc: 0.8425\n",
      "Epoch 126/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3483 - acc: 0.8529 - val_loss: 0.3627 - val_acc: 0.8488\n",
      "Epoch 127/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3468 - acc: 0.8528 - val_loss: 0.3602 - val_acc: 0.8488\n",
      "Epoch 128/300\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3459 - acc: 0.8560 - val_loss: 0.3648 - val_acc: 0.8462\n",
      "Epoch 129/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3467 - acc: 0.8571 - val_loss: 0.3634 - val_acc: 0.8500\n",
      "Epoch 130/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3454 - acc: 0.8567 - val_loss: 0.3650 - val_acc: 0.8475\n",
      "Epoch 131/300\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3455 - acc: 0.8565 - val_loss: 0.3632 - val_acc: 0.8450\n",
      "Epoch 132/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3467 - acc: 0.8567 - val_loss: 0.3630 - val_acc: 0.8475\n",
      "Epoch 133/300\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3434 - acc: 0.8572 - val_loss: 0.3645 - val_acc: 0.8488\n",
      "Epoch 134/300\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3455 - acc: 0.8561 - val_loss: 0.3611 - val_acc: 0.8562\n",
      "Epoch 135/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3441 - acc: 0.8539 - val_loss: 0.3592 - val_acc: 0.8525\n",
      "Epoch 136/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3460 - acc: 0.8564 - val_loss: 0.3671 - val_acc: 0.8488\n",
      "Epoch 137/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3502 - acc: 0.8501 - val_loss: 0.3655 - val_acc: 0.8550\n",
      "Epoch 138/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3452 - acc: 0.8558 - val_loss: 0.3583 - val_acc: 0.8538\n",
      "Epoch 139/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3440 - acc: 0.8583 - val_loss: 0.3649 - val_acc: 0.8538\n",
      "Epoch 140/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3404 - acc: 0.8592 - val_loss: 0.3597 - val_acc: 0.8500\n",
      "Epoch 141/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3417 - acc: 0.8582 - val_loss: 0.3566 - val_acc: 0.8512\n",
      "Epoch 142/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3458 - acc: 0.8560 - val_loss: 0.3576 - val_acc: 0.8500\n",
      "Epoch 143/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3424 - acc: 0.8574 - val_loss: 0.3574 - val_acc: 0.8525\n",
      "Epoch 144/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3433 - acc: 0.8581 - val_loss: 0.3569 - val_acc: 0.8538\n",
      "Epoch 145/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3409 - acc: 0.8567 - val_loss: 0.3560 - val_acc: 0.8562\n",
      "Epoch 146/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3422 - acc: 0.8571 - val_loss: 0.3640 - val_acc: 0.8475\n",
      "Epoch 147/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3410 - acc: 0.8571 - val_loss: 0.3600 - val_acc: 0.8525\n",
      "Epoch 148/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3397 - acc: 0.8575 - val_loss: 0.3545 - val_acc: 0.8512\n",
      "Epoch 149/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3401 - acc: 0.8582 - val_loss: 0.3659 - val_acc: 0.8575\n",
      "Epoch 150/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3434 - acc: 0.8547 - val_loss: 0.3556 - val_acc: 0.8538\n",
      "Epoch 151/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3401 - acc: 0.8578 - val_loss: 0.3539 - val_acc: 0.8562\n",
      "Epoch 152/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3422 - acc: 0.8576 - val_loss: 0.3602 - val_acc: 0.8562\n",
      "Epoch 153/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3379 - acc: 0.8617 - val_loss: 0.3633 - val_acc: 0.8538\n",
      "Epoch 154/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3426 - acc: 0.8564 - val_loss: 0.3697 - val_acc: 0.8512\n",
      "Epoch 155/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3397 - acc: 0.8586 - val_loss: 0.3534 - val_acc: 0.8525\n",
      "Epoch 156/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3384 - acc: 0.8585 - val_loss: 0.3561 - val_acc: 0.8562\n",
      "Epoch 157/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3405 - acc: 0.8575 - val_loss: 0.3588 - val_acc: 0.8462\n",
      "Epoch 158/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3383 - acc: 0.8587 - val_loss: 0.3567 - val_acc: 0.8550\n",
      "Epoch 159/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3380 - acc: 0.8578 - val_loss: 0.3547 - val_acc: 0.8588\n",
      "Epoch 160/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3377 - acc: 0.8579 - val_loss: 0.3584 - val_acc: 0.8550\n",
      "Epoch 161/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3388 - acc: 0.8604 - val_loss: 0.3584 - val_acc: 0.8562\n",
      "Epoch 162/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3363 - acc: 0.8586 - val_loss: 0.3555 - val_acc: 0.8562\n",
      "Epoch 163/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3375 - acc: 0.8568 - val_loss: 0.3550 - val_acc: 0.8638\n",
      "Epoch 164/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3389 - acc: 0.8587 - val_loss: 0.3554 - val_acc: 0.8562\n",
      "Epoch 165/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3356 - acc: 0.8589 - val_loss: 0.3564 - val_acc: 0.8538\n",
      "Epoch 166/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3349 - acc: 0.8606 - val_loss: 0.3554 - val_acc: 0.8575\n",
      "Epoch 167/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3364 - acc: 0.8578 - val_loss: 0.3598 - val_acc: 0.8462\n",
      "Epoch 168/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3353 - acc: 0.8581 - val_loss: 0.3632 - val_acc: 0.8550\n",
      "Epoch 169/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3395 - acc: 0.8578 - val_loss: 0.3550 - val_acc: 0.8512\n",
      "Epoch 170/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3363 - acc: 0.8597 - val_loss: 0.3551 - val_acc: 0.8550\n",
      "Epoch 171/300\n",
      "7200/7200 [==============================] - ETA: 0s - loss: 0.3361 - acc: 0.863 - 0s 12us/step - loss: 0.3344 - acc: 0.8618 - val_loss: 0.3572 - val_acc: 0.8450\n",
      "Epoch 172/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3336 - acc: 0.8583 - val_loss: 0.3622 - val_acc: 0.8475\n",
      "Epoch 173/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3363 - acc: 0.8586 - val_loss: 0.3847 - val_acc: 0.8488\n",
      "Epoch 174/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3389 - acc: 0.8594 - val_loss: 0.3545 - val_acc: 0.8575\n",
      "Epoch 175/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3373 - acc: 0.8576 - val_loss: 0.3569 - val_acc: 0.8538\n",
      "Epoch 176/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3329 - acc: 0.8621 - val_loss: 0.3514 - val_acc: 0.8612\n",
      "Epoch 177/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3388 - acc: 0.8538 - val_loss: 0.3566 - val_acc: 0.8550\n",
      "Epoch 178/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3321 - acc: 0.8610 - val_loss: 0.3574 - val_acc: 0.8438\n",
      "Epoch 179/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3334 - acc: 0.8589 - val_loss: 0.3513 - val_acc: 0.8600\n",
      "Epoch 180/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3359 - acc: 0.8586 - val_loss: 0.3480 - val_acc: 0.8625\n",
      "Epoch 181/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3322 - acc: 0.8615 - val_loss: 0.3520 - val_acc: 0.8550\n",
      "Epoch 182/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3337 - acc: 0.8606 - val_loss: 0.3578 - val_acc: 0.8600\n",
      "Epoch 183/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3335 - acc: 0.8592 - val_loss: 0.3597 - val_acc: 0.8438\n",
      "Epoch 184/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3323 - acc: 0.8621 - val_loss: 0.3581 - val_acc: 0.8525\n",
      "Epoch 185/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3361 - acc: 0.8596 - val_loss: 0.3558 - val_acc: 0.8462\n",
      "Epoch 186/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3305 - acc: 0.8625 - val_loss: 0.3634 - val_acc: 0.8575\n",
      "Epoch 187/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3357 - acc: 0.8587 - val_loss: 0.3647 - val_acc: 0.8512\n",
      "Epoch 188/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3357 - acc: 0.8572 - val_loss: 0.3526 - val_acc: 0.8538\n",
      "Epoch 189/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3317 - acc: 0.8617 - val_loss: 0.3534 - val_acc: 0.8550\n",
      "Epoch 190/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3306 - acc: 0.8597 - val_loss: 0.3564 - val_acc: 0.8525\n",
      "Epoch 191/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3318 - acc: 0.8596 - val_loss: 0.3493 - val_acc: 0.8600\n",
      "Epoch 192/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3299 - acc: 0.8626 - val_loss: 0.3525 - val_acc: 0.8625\n",
      "Epoch 193/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3305 - acc: 0.8600 - val_loss: 0.3536 - val_acc: 0.8638\n",
      "Epoch 194/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3295 - acc: 0.8603 - val_loss: 0.3594 - val_acc: 0.8450\n",
      "Epoch 195/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3322 - acc: 0.8589 - val_loss: 0.3611 - val_acc: 0.8475\n",
      "Epoch 196/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3312 - acc: 0.8629 - val_loss: 0.3595 - val_acc: 0.8575\n",
      "Epoch 197/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3298 - acc: 0.8617 - val_loss: 0.3586 - val_acc: 0.8575\n",
      "Epoch 198/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3296 - acc: 0.8613 - val_loss: 0.3563 - val_acc: 0.8525\n",
      "Epoch 199/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3304 - acc: 0.8607 - val_loss: 0.3562 - val_acc: 0.8500\n",
      "Epoch 200/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3312 - acc: 0.8590 - val_loss: 0.3564 - val_acc: 0.8612\n",
      "Epoch 201/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3283 - acc: 0.8624 - val_loss: 0.3498 - val_acc: 0.8525\n",
      "Epoch 202/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3304 - acc: 0.8599 - val_loss: 0.3493 - val_acc: 0.8588\n",
      "Epoch 203/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3288 - acc: 0.8606 - val_loss: 0.3518 - val_acc: 0.8575\n",
      "Epoch 204/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3320 - acc: 0.8593 - val_loss: 0.3579 - val_acc: 0.8575\n",
      "Epoch 205/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3277 - acc: 0.8631 - val_loss: 0.3639 - val_acc: 0.8588\n",
      "Epoch 206/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3324 - acc: 0.8604 - val_loss: 0.3638 - val_acc: 0.8462\n",
      "Epoch 207/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3289 - acc: 0.8615 - val_loss: 0.3532 - val_acc: 0.8575\n",
      "Epoch 208/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3265 - acc: 0.8607 - val_loss: 0.3513 - val_acc: 0.8575\n",
      "Epoch 209/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3295 - acc: 0.8593 - val_loss: 0.3661 - val_acc: 0.8550\n",
      "Epoch 210/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3312 - acc: 0.8604 - val_loss: 0.3602 - val_acc: 0.8488\n",
      "Epoch 211/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3289 - acc: 0.8624 - val_loss: 0.3557 - val_acc: 0.8600\n",
      "Epoch 212/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3275 - acc: 0.8617 - val_loss: 0.3583 - val_acc: 0.8638\n",
      "Epoch 213/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3253 - acc: 0.8636 - val_loss: 0.3552 - val_acc: 0.8562\n",
      "Epoch 214/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3264 - acc: 0.8642 - val_loss: 0.3505 - val_acc: 0.8600\n",
      "Epoch 215/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3274 - acc: 0.8628 - val_loss: 0.3499 - val_acc: 0.8638\n",
      "Epoch 216/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3276 - acc: 0.8613 - val_loss: 0.3449 - val_acc: 0.8575\n",
      "Epoch 217/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3295 - acc: 0.8600 - val_loss: 0.3490 - val_acc: 0.8638\n",
      "Epoch 218/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3286 - acc: 0.8597 - val_loss: 0.3528 - val_acc: 0.8612\n",
      "Epoch 219/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3270 - acc: 0.8631 - val_loss: 0.3513 - val_acc: 0.8638\n",
      "Epoch 220/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3256 - acc: 0.8626 - val_loss: 0.3562 - val_acc: 0.8550\n",
      "Epoch 221/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3307 - acc: 0.8600 - val_loss: 0.3568 - val_acc: 0.8538\n",
      "Epoch 222/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3256 - acc: 0.8599 - val_loss: 0.3570 - val_acc: 0.8625\n",
      "Epoch 223/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3286 - acc: 0.8629 - val_loss: 0.3524 - val_acc: 0.8588\n",
      "Epoch 224/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3258 - acc: 0.8608 - val_loss: 0.3502 - val_acc: 0.8612\n",
      "Epoch 225/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3266 - acc: 0.8629 - val_loss: 0.3584 - val_acc: 0.8575\n",
      "Epoch 226/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3289 - acc: 0.8618 - val_loss: 0.3500 - val_acc: 0.8612\n",
      "Epoch 227/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3240 - acc: 0.8651 - val_loss: 0.3534 - val_acc: 0.8525\n",
      "Epoch 228/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3244 - acc: 0.8640 - val_loss: 0.3508 - val_acc: 0.8562\n",
      "Epoch 229/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3241 - acc: 0.8628 - val_loss: 0.3609 - val_acc: 0.8525\n",
      "Epoch 230/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3246 - acc: 0.8649 - val_loss: 0.3579 - val_acc: 0.8588\n",
      "Epoch 231/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3237 - acc: 0.8646 - val_loss: 0.3568 - val_acc: 0.8538\n",
      "Epoch 232/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3224 - acc: 0.8669 - val_loss: 0.3528 - val_acc: 0.8662\n",
      "Epoch 233/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3220 - acc: 0.8651 - val_loss: 0.3471 - val_acc: 0.8550\n",
      "Epoch 234/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3227 - acc: 0.8651 - val_loss: 0.3584 - val_acc: 0.8650\n",
      "Epoch 235/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3233 - acc: 0.8640 - val_loss: 0.3534 - val_acc: 0.8588\n",
      "Epoch 236/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3212 - acc: 0.8681 - val_loss: 0.3558 - val_acc: 0.8550\n",
      "Epoch 237/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3233 - acc: 0.8625 - val_loss: 0.3591 - val_acc: 0.8525\n",
      "Epoch 238/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3214 - acc: 0.8667 - val_loss: 0.3555 - val_acc: 0.8588\n",
      "Epoch 239/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3225 - acc: 0.8651 - val_loss: 0.3505 - val_acc: 0.8625\n",
      "Epoch 240/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3215 - acc: 0.8663 - val_loss: 0.3505 - val_acc: 0.8625\n",
      "Epoch 241/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3191 - acc: 0.8703 - val_loss: 0.3637 - val_acc: 0.8600\n",
      "Epoch 242/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3236 - acc: 0.8656 - val_loss: 0.3709 - val_acc: 0.8575\n",
      "Epoch 243/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3225 - acc: 0.8665 - val_loss: 0.3499 - val_acc: 0.8600\n",
      "Epoch 244/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3221 - acc: 0.8636 - val_loss: 0.3558 - val_acc: 0.8575\n",
      "Epoch 245/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3189 - acc: 0.8681 - val_loss: 0.3480 - val_acc: 0.8612\n",
      "Epoch 246/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3215 - acc: 0.8635 - val_loss: 0.3550 - val_acc: 0.8638\n",
      "Epoch 247/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3193 - acc: 0.8636 - val_loss: 0.3557 - val_acc: 0.8575\n",
      "Epoch 248/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3201 - acc: 0.8682 - val_loss: 0.3520 - val_acc: 0.8588\n",
      "Epoch 249/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3188 - acc: 0.8663 - val_loss: 0.3537 - val_acc: 0.8650\n",
      "Epoch 250/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3182 - acc: 0.8649 - val_loss: 0.3527 - val_acc: 0.8588\n",
      "Epoch 251/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3189 - acc: 0.8675 - val_loss: 0.3541 - val_acc: 0.8500\n",
      "Epoch 252/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3224 - acc: 0.8650 - val_loss: 0.3613 - val_acc: 0.8588\n",
      "Epoch 253/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3184 - acc: 0.8671 - val_loss: 0.3606 - val_acc: 0.8525\n",
      "Epoch 254/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3168 - acc: 0.8671 - val_loss: 0.3514 - val_acc: 0.8638\n",
      "Epoch 255/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3163 - acc: 0.8661 - val_loss: 0.3547 - val_acc: 0.8575\n",
      "Epoch 256/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3177 - acc: 0.8669 - val_loss: 0.3520 - val_acc: 0.8612\n",
      "Epoch 257/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3187 - acc: 0.8676 - val_loss: 0.3518 - val_acc: 0.8600\n",
      "Epoch 258/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3188 - acc: 0.8667 - val_loss: 0.3570 - val_acc: 0.8538\n",
      "Epoch 259/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3168 - acc: 0.8685 - val_loss: 0.3566 - val_acc: 0.8650\n",
      "Epoch 260/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3189 - acc: 0.8667 - val_loss: 0.3543 - val_acc: 0.8550\n",
      "Epoch 261/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3151 - acc: 0.8683 - val_loss: 0.3529 - val_acc: 0.8538\n",
      "Epoch 262/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3138 - acc: 0.8697 - val_loss: 0.3540 - val_acc: 0.8562\n",
      "Epoch 263/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3171 - acc: 0.8649 - val_loss: 0.3625 - val_acc: 0.8612\n",
      "Epoch 264/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3186 - acc: 0.8657 - val_loss: 0.3686 - val_acc: 0.8600\n",
      "Epoch 265/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3162 - acc: 0.8675 - val_loss: 0.3535 - val_acc: 0.8638\n",
      "Epoch 266/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3178 - acc: 0.8675 - val_loss: 0.3507 - val_acc: 0.8638\n",
      "Epoch 267/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3144 - acc: 0.8704 - val_loss: 0.3550 - val_acc: 0.8575\n",
      "Epoch 268/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3141 - acc: 0.8701 - val_loss: 0.3590 - val_acc: 0.8538\n",
      "Epoch 269/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3155 - acc: 0.8653 - val_loss: 0.3523 - val_acc: 0.8588\n",
      "Epoch 270/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3142 - acc: 0.8674 - val_loss: 0.3525 - val_acc: 0.8650\n",
      "Epoch 271/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3143 - acc: 0.8685 - val_loss: 0.3540 - val_acc: 0.8600\n",
      "Epoch 272/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3136 - acc: 0.8697 - val_loss: 0.3528 - val_acc: 0.8625\n",
      "Epoch 273/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3126 - acc: 0.8699 - val_loss: 0.3569 - val_acc: 0.8600\n",
      "Epoch 274/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3108 - acc: 0.8700 - val_loss: 0.3542 - val_acc: 0.8562\n",
      "Epoch 275/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3136 - acc: 0.8692 - val_loss: 0.3549 - val_acc: 0.8625\n",
      "Epoch 276/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3122 - acc: 0.8686 - val_loss: 0.3527 - val_acc: 0.8588\n",
      "Epoch 277/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3115 - acc: 0.8707 - val_loss: 0.3516 - val_acc: 0.8538\n",
      "Epoch 278/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3108 - acc: 0.8699 - val_loss: 0.3708 - val_acc: 0.8588\n",
      "Epoch 279/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3177 - acc: 0.8658 - val_loss: 0.3600 - val_acc: 0.8588\n",
      "Epoch 280/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3139 - acc: 0.8700 - val_loss: 0.3521 - val_acc: 0.8625\n",
      "Epoch 281/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3089 - acc: 0.8714 - val_loss: 0.3551 - val_acc: 0.8562\n",
      "Epoch 282/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3097 - acc: 0.8685 - val_loss: 0.3658 - val_acc: 0.8538\n",
      "Epoch 283/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3114 - acc: 0.8683 - val_loss: 0.3528 - val_acc: 0.8612\n",
      "Epoch 284/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3108 - acc: 0.8724 - val_loss: 0.3588 - val_acc: 0.8538\n",
      "Epoch 285/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3097 - acc: 0.8696 - val_loss: 0.3651 - val_acc: 0.8550\n",
      "Epoch 286/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3101 - acc: 0.8712 - val_loss: 0.3541 - val_acc: 0.8600\n",
      "Epoch 287/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3100 - acc: 0.8701 - val_loss: 0.3637 - val_acc: 0.8600\n",
      "Epoch 288/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3113 - acc: 0.8708 - val_loss: 0.3576 - val_acc: 0.8562\n",
      "Epoch 289/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3131 - acc: 0.8682 - val_loss: 0.3729 - val_acc: 0.8538\n",
      "Epoch 290/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3107 - acc: 0.8704 - val_loss: 0.3525 - val_acc: 0.8600\n",
      "Epoch 291/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3142 - acc: 0.8704 - val_loss: 0.3579 - val_acc: 0.8600\n",
      "Epoch 292/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3090 - acc: 0.8711 - val_loss: 0.3535 - val_acc: 0.8662\n",
      "Epoch 293/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3105 - acc: 0.8711 - val_loss: 0.3514 - val_acc: 0.8588\n",
      "Epoch 294/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3094 - acc: 0.8692 - val_loss: 0.3541 - val_acc: 0.8600\n",
      "Epoch 295/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3076 - acc: 0.8697 - val_loss: 0.3762 - val_acc: 0.8600\n",
      "Epoch 296/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3078 - acc: 0.8710 - val_loss: 0.3609 - val_acc: 0.8575\n",
      "Epoch 297/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3083 - acc: 0.8686 - val_loss: 0.3588 - val_acc: 0.8550\n",
      "Epoch 298/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3043 - acc: 0.8721 - val_loss: 0.3631 - val_acc: 0.8575\n",
      "Epoch 299/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3069 - acc: 0.8722 - val_loss: 0.3545 - val_acc: 0.8638\n",
      "Epoch 300/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3066 - acc: 0.8714 - val_loss: 0.3595 - val_acc: 0.8575\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "epochs = 200\n",
    "model_hist = model.fit(trainData, trainTarget, epochs=300, batch_size=128, validation_data=(valData, valTarget))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_acc = model_hist.history['acc']\n",
    "model_loss = model_hist.history['loss']\n",
    "model_val_acc = model_hist.history['val_acc']\n",
    "model_val_loss = model_hist.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(model_acc) + 1)\n",
    "# b+ is for \"blue cross\"\n",
    "plt.plot(epochs, model_val_loss, 'b+', label='Val Loss')\n",
    "plt.plot(epochs, model_loss, 'b', label='Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To combat overfitting, I have plotted the loss for both validation data set and training data set for over 300 epochs. From the graph, I can vaguely tell that the model is starting to overfit at 150 epochs. At 150 epochs, the loss score is stagnant for valdation data; while the loss score for the training data set keeps decreasing. Thus, I'll retrain the model upto 150 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200 samples, validate on 2000 samples\n",
      "Epoch 1/150\n",
      "7200/7200 [==============================] - 0s 54us/step - loss: 0.5225 - acc: 0.7954 - val_loss: 0.5073 - val_acc: 0.7870\n",
      "Epoch 2/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4833 - acc: 0.8015 - val_loss: 0.5183 - val_acc: 0.7680\n",
      "Epoch 3/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4765 - acc: 0.7985 - val_loss: 0.4708 - val_acc: 0.7935\n",
      "Epoch 4/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4529 - acc: 0.8081 - val_loss: 0.4733 - val_acc: 0.7910\n",
      "Epoch 5/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4479 - acc: 0.8117 - val_loss: 0.4690 - val_acc: 0.7970\n",
      "Epoch 6/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4445 - acc: 0.8143 - val_loss: 0.4721 - val_acc: 0.7885\n",
      "Epoch 7/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4425 - acc: 0.8163 - val_loss: 0.4614 - val_acc: 0.7940\n",
      "Epoch 8/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4431 - acc: 0.8176 - val_loss: 0.4891 - val_acc: 0.7940\n",
      "Epoch 9/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4414 - acc: 0.8153 - val_loss: 0.4641 - val_acc: 0.8020\n",
      "Epoch 10/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4388 - acc: 0.8172 - val_loss: 0.4566 - val_acc: 0.7955\n",
      "Epoch 11/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4386 - acc: 0.8167 - val_loss: 0.4871 - val_acc: 0.7760\n",
      "Epoch 12/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4367 - acc: 0.8185 - val_loss: 0.4642 - val_acc: 0.7910\n",
      "Epoch 13/150\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.4440 - acc: 0.8121 - val_loss: 0.4544 - val_acc: 0.8010\n",
      "Epoch 14/150\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.4352 - acc: 0.8186 - val_loss: 0.4573 - val_acc: 0.7950\n",
      "Epoch 15/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4340 - acc: 0.8186 - val_loss: 0.4515 - val_acc: 0.8010\n",
      "Epoch 16/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4352 - acc: 0.8190 - val_loss: 0.4558 - val_acc: 0.8035\n",
      "Epoch 17/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4306 - acc: 0.8168 - val_loss: 0.4541 - val_acc: 0.8050\n",
      "Epoch 18/150\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.4294 - acc: 0.8214 - val_loss: 0.4488 - val_acc: 0.8035\n",
      "Epoch 19/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.4299 - acc: 0.8194 - val_loss: 0.4477 - val_acc: 0.8050\n",
      "Epoch 20/150\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.4323 - acc: 0.8149 - val_loss: 0.4455 - val_acc: 0.8050\n",
      "Epoch 21/150\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.4283 - acc: 0.8218 - val_loss: 0.4472 - val_acc: 0.8035\n",
      "Epoch 22/150\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.4270 - acc: 0.8178 - val_loss: 0.4423 - val_acc: 0.8025\n",
      "Epoch 23/150\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.4235 - acc: 0.8224 - val_loss: 0.4462 - val_acc: 0.7985\n",
      "Epoch 24/150\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.4233 - acc: 0.8194 - val_loss: 0.4422 - val_acc: 0.8045\n",
      "Epoch 25/150\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.4204 - acc: 0.8201 - val_loss: 0.4377 - val_acc: 0.8095\n",
      "Epoch 26/150\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.4232 - acc: 0.8203 - val_loss: 0.4354 - val_acc: 0.8095\n",
      "Epoch 27/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.4179 - acc: 0.8212 - val_loss: 0.4509 - val_acc: 0.8005\n",
      "Epoch 28/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.4241 - acc: 0.8187 - val_loss: 0.4339 - val_acc: 0.8105\n",
      "Epoch 29/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4222 - acc: 0.8172 - val_loss: 0.4468 - val_acc: 0.8035\n",
      "Epoch 30/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4159 - acc: 0.8231 - val_loss: 0.4295 - val_acc: 0.8110\n",
      "Epoch 31/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4116 - acc: 0.8251 - val_loss: 0.4224 - val_acc: 0.8175\n",
      "Epoch 32/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.4093 - acc: 0.8250 - val_loss: 0.4207 - val_acc: 0.8175\n",
      "Epoch 33/150\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.4037 - acc: 0.8322 - val_loss: 0.4179 - val_acc: 0.8155\n",
      "Epoch 34/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.4022 - acc: 0.8336 - val_loss: 0.4195 - val_acc: 0.8150\n",
      "Epoch 35/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.4029 - acc: 0.8296 - val_loss: 0.4119 - val_acc: 0.8205\n",
      "Epoch 36/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3987 - acc: 0.8304 - val_loss: 0.4065 - val_acc: 0.8275\n",
      "Epoch 37/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3966 - acc: 0.8337 - val_loss: 0.4055 - val_acc: 0.8215\n",
      "Epoch 38/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3960 - acc: 0.8335 - val_loss: 0.4065 - val_acc: 0.8250\n",
      "Epoch 39/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3928 - acc: 0.8383 - val_loss: 0.4054 - val_acc: 0.8170\n",
      "Epoch 40/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3907 - acc: 0.8360 - val_loss: 0.4072 - val_acc: 0.8225\n",
      "Epoch 41/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3932 - acc: 0.8329 - val_loss: 0.3995 - val_acc: 0.8295\n",
      "Epoch 42/150\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3946 - acc: 0.8339 - val_loss: 0.4147 - val_acc: 0.8070\n",
      "Epoch 43/150\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3923 - acc: 0.8357 - val_loss: 0.3973 - val_acc: 0.8255\n",
      "Epoch 44/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3840 - acc: 0.8394 - val_loss: 0.3966 - val_acc: 0.8270\n",
      "Epoch 45/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3887 - acc: 0.8367 - val_loss: 0.3906 - val_acc: 0.8320\n",
      "Epoch 46/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3862 - acc: 0.8394 - val_loss: 0.4069 - val_acc: 0.8125\n",
      "Epoch 47/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3855 - acc: 0.8374 - val_loss: 0.3867 - val_acc: 0.8310\n",
      "Epoch 48/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3832 - acc: 0.8351 - val_loss: 0.3900 - val_acc: 0.8275\n",
      "Epoch 49/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3829 - acc: 0.8393 - val_loss: 0.4106 - val_acc: 0.8135\n",
      "Epoch 50/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3867 - acc: 0.8379 - val_loss: 0.3865 - val_acc: 0.8335\n",
      "Epoch 51/150\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3770 - acc: 0.8400 - val_loss: 0.3810 - val_acc: 0.8300\n",
      "Epoch 52/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3764 - acc: 0.8413 - val_loss: 0.3917 - val_acc: 0.8215\n",
      "Epoch 53/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3812 - acc: 0.8358 - val_loss: 0.4018 - val_acc: 0.8235\n",
      "Epoch 54/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3796 - acc: 0.8378 - val_loss: 0.3815 - val_acc: 0.8350\n",
      "Epoch 55/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3728 - acc: 0.8447 - val_loss: 0.4503 - val_acc: 0.8060\n",
      "Epoch 56/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3788 - acc: 0.8379 - val_loss: 0.4032 - val_acc: 0.8185\n",
      "Epoch 57/150\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3755 - acc: 0.8397 - val_loss: 0.3997 - val_acc: 0.8185\n",
      "Epoch 58/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3839 - acc: 0.8346 - val_loss: 0.3905 - val_acc: 0.8305\n",
      "Epoch 59/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3725 - acc: 0.8442 - val_loss: 0.3783 - val_acc: 0.8280\n",
      "Epoch 60/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3773 - acc: 0.8403 - val_loss: 0.3738 - val_acc: 0.8395\n",
      "Epoch 61/150\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3698 - acc: 0.8440 - val_loss: 0.3778 - val_acc: 0.8335\n",
      "Epoch 62/150\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3678 - acc: 0.8458 - val_loss: 0.3722 - val_acc: 0.8365\n",
      "Epoch 63/150\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3726 - acc: 0.8426 - val_loss: 0.3850 - val_acc: 0.8280\n",
      "Epoch 64/150\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3714 - acc: 0.8458 - val_loss: 0.3717 - val_acc: 0.8350\n",
      "Epoch 65/150\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3651 - acc: 0.8446 - val_loss: 0.3792 - val_acc: 0.8345\n",
      "Epoch 66/150\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3669 - acc: 0.8482 - val_loss: 0.3701 - val_acc: 0.8395\n",
      "Epoch 67/150\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3658 - acc: 0.8457 - val_loss: 0.3695 - val_acc: 0.8405\n",
      "Epoch 68/150\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3647 - acc: 0.8488 - val_loss: 0.4011 - val_acc: 0.8245\n",
      "Epoch 69/150\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3701 - acc: 0.8449 - val_loss: 0.3687 - val_acc: 0.8340\n",
      "Epoch 70/150\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3651 - acc: 0.8450 - val_loss: 0.3685 - val_acc: 0.8370\n",
      "Epoch 71/150\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3619 - acc: 0.8483 - val_loss: 0.3799 - val_acc: 0.8240\n",
      "Epoch 72/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3720 - acc: 0.8439 - val_loss: 0.3913 - val_acc: 0.8265\n",
      "Epoch 73/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3700 - acc: 0.8422 - val_loss: 0.3862 - val_acc: 0.8370\n",
      "Epoch 74/150\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3659 - acc: 0.8472 - val_loss: 0.3873 - val_acc: 0.8285\n",
      "Epoch 75/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3728 - acc: 0.8438 - val_loss: 0.3683 - val_acc: 0.8375\n",
      "Epoch 76/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3623 - acc: 0.8482 - val_loss: 0.3720 - val_acc: 0.8345\n",
      "Epoch 77/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3604 - acc: 0.8471 - val_loss: 0.3766 - val_acc: 0.8340\n",
      "Epoch 78/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3587 - acc: 0.8485 - val_loss: 0.3697 - val_acc: 0.8355\n",
      "Epoch 79/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3620 - acc: 0.8462 - val_loss: 0.3665 - val_acc: 0.8415\n",
      "Epoch 80/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3581 - acc: 0.8517 - val_loss: 0.3795 - val_acc: 0.8355\n",
      "Epoch 81/150\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3590 - acc: 0.8478 - val_loss: 0.3740 - val_acc: 0.8415\n",
      "Epoch 82/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3609 - acc: 0.8476 - val_loss: 0.3888 - val_acc: 0.8265\n",
      "Epoch 83/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3613 - acc: 0.8488 - val_loss: 0.3643 - val_acc: 0.8410\n",
      "Epoch 84/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3560 - acc: 0.8507 - val_loss: 0.3791 - val_acc: 0.8305\n",
      "Epoch 85/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3540 - acc: 0.8506 - val_loss: 0.3696 - val_acc: 0.8395\n",
      "Epoch 86/150\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3559 - acc: 0.8521 - val_loss: 0.4013 - val_acc: 0.8240\n",
      "Epoch 87/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3591 - acc: 0.8492 - val_loss: 0.3660 - val_acc: 0.8370\n",
      "Epoch 88/150\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3620 - acc: 0.8472 - val_loss: 0.3652 - val_acc: 0.8420\n",
      "Epoch 89/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3555 - acc: 0.8496 - val_loss: 0.3648 - val_acc: 0.8420\n",
      "Epoch 90/150\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3566 - acc: 0.8488 - val_loss: 0.3664 - val_acc: 0.8390\n",
      "Epoch 91/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3530 - acc: 0.8522 - val_loss: 0.3632 - val_acc: 0.8405\n",
      "Epoch 92/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3558 - acc: 0.8522 - val_loss: 0.3635 - val_acc: 0.8360\n",
      "Epoch 93/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3535 - acc: 0.8538 - val_loss: 0.3728 - val_acc: 0.8405\n",
      "Epoch 94/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3526 - acc: 0.8521 - val_loss: 0.3633 - val_acc: 0.8420\n",
      "Epoch 95/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3563 - acc: 0.8497 - val_loss: 0.3665 - val_acc: 0.8420\n",
      "Epoch 96/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3560 - acc: 0.8506 - val_loss: 0.3695 - val_acc: 0.8370\n",
      "Epoch 97/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3532 - acc: 0.8535 - val_loss: 0.3642 - val_acc: 0.8390\n",
      "Epoch 98/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3586 - acc: 0.8482 - val_loss: 0.3611 - val_acc: 0.8390\n",
      "Epoch 99/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3519 - acc: 0.8535 - val_loss: 0.3628 - val_acc: 0.8420\n",
      "Epoch 100/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3516 - acc: 0.8533 - val_loss: 0.3710 - val_acc: 0.8425\n",
      "Epoch 101/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3587 - acc: 0.8483 - val_loss: 0.3615 - val_acc: 0.8420\n",
      "Epoch 102/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3546 - acc: 0.8529 - val_loss: 0.3692 - val_acc: 0.8410\n",
      "Epoch 103/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3592 - acc: 0.8507 - val_loss: 0.3613 - val_acc: 0.8420\n",
      "Epoch 104/150\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3503 - acc: 0.8549 - val_loss: 0.3622 - val_acc: 0.8410\n",
      "Epoch 105/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3524 - acc: 0.8539 - val_loss: 0.3638 - val_acc: 0.8420\n",
      "Epoch 106/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3519 - acc: 0.8535 - val_loss: 0.3576 - val_acc: 0.8425\n",
      "Epoch 107/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3501 - acc: 0.8533 - val_loss: 0.3860 - val_acc: 0.8360\n",
      "Epoch 108/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3500 - acc: 0.8522 - val_loss: 0.3743 - val_acc: 0.8325\n",
      "Epoch 109/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3494 - acc: 0.8525 - val_loss: 0.3648 - val_acc: 0.8415\n",
      "Epoch 110/150\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3484 - acc: 0.8557 - val_loss: 0.3715 - val_acc: 0.8405\n",
      "Epoch 111/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3494 - acc: 0.8553 - val_loss: 0.3656 - val_acc: 0.8385\n",
      "Epoch 112/150\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3489 - acc: 0.8553 - val_loss: 0.3620 - val_acc: 0.8405\n",
      "Epoch 113/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3489 - acc: 0.8556 - val_loss: 0.3608 - val_acc: 0.8390\n",
      "Epoch 114/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3492 - acc: 0.8525 - val_loss: 0.3660 - val_acc: 0.8415\n",
      "Epoch 115/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3487 - acc: 0.8536 - val_loss: 0.3636 - val_acc: 0.8425\n",
      "Epoch 116/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3472 - acc: 0.8544 - val_loss: 0.3684 - val_acc: 0.8370\n",
      "Epoch 117/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3561 - acc: 0.8499 - val_loss: 0.3653 - val_acc: 0.8395\n",
      "Epoch 118/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3454 - acc: 0.8582 - val_loss: 0.3654 - val_acc: 0.8420\n",
      "Epoch 119/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3490 - acc: 0.8558 - val_loss: 0.3576 - val_acc: 0.8400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3448 - acc: 0.8575 - val_loss: 0.3660 - val_acc: 0.8410\n",
      "Epoch 121/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3469 - acc: 0.8571 - val_loss: 0.3724 - val_acc: 0.8385\n",
      "Epoch 122/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3481 - acc: 0.8546 - val_loss: 0.3572 - val_acc: 0.8450\n",
      "Epoch 123/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3448 - acc: 0.8572 - val_loss: 0.3702 - val_acc: 0.8390\n",
      "Epoch 124/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3470 - acc: 0.8572 - val_loss: 0.3578 - val_acc: 0.8430\n",
      "Epoch 125/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3459 - acc: 0.8539 - val_loss: 0.3619 - val_acc: 0.8425\n",
      "Epoch 126/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3491 - acc: 0.8539 - val_loss: 0.3554 - val_acc: 0.8440\n",
      "Epoch 127/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3463 - acc: 0.8574 - val_loss: 0.3594 - val_acc: 0.8430\n",
      "Epoch 128/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3438 - acc: 0.8582 - val_loss: 0.3642 - val_acc: 0.8380\n",
      "Epoch 129/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3525 - acc: 0.8546 - val_loss: 0.3642 - val_acc: 0.8385\n",
      "Epoch 130/150\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3462 - acc: 0.8544 - val_loss: 0.3557 - val_acc: 0.8425\n",
      "Epoch 131/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3438 - acc: 0.8549 - val_loss: 0.3622 - val_acc: 0.8425\n",
      "Epoch 132/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3420 - acc: 0.8576 - val_loss: 0.3752 - val_acc: 0.8400\n",
      "Epoch 133/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3490 - acc: 0.8558 - val_loss: 0.3620 - val_acc: 0.8430\n",
      "Epoch 134/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3460 - acc: 0.8536 - val_loss: 0.3541 - val_acc: 0.8450\n",
      "Epoch 135/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3451 - acc: 0.8543 - val_loss: 0.3567 - val_acc: 0.8435\n",
      "Epoch 136/150\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3411 - acc: 0.8576 - val_loss: 0.3678 - val_acc: 0.8395\n",
      "Epoch 137/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3433 - acc: 0.8586 - val_loss: 0.3590 - val_acc: 0.8425\n",
      "Epoch 138/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3417 - acc: 0.8581 - val_loss: 0.3586 - val_acc: 0.8445\n",
      "Epoch 139/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3445 - acc: 0.8576 - val_loss: 0.3575 - val_acc: 0.8415\n",
      "Epoch 140/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3504 - acc: 0.8546 - val_loss: 0.3564 - val_acc: 0.8455\n",
      "Epoch 141/150\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3423 - acc: 0.8583 - val_loss: 0.3689 - val_acc: 0.8380\n",
      "Epoch 142/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3449 - acc: 0.8551 - val_loss: 0.3599 - val_acc: 0.8465\n",
      "Epoch 143/150\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3399 - acc: 0.8593 - val_loss: 0.3586 - val_acc: 0.8440\n",
      "Epoch 144/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3413 - acc: 0.8594 - val_loss: 0.3550 - val_acc: 0.8480\n",
      "Epoch 145/150\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3416 - acc: 0.8582 - val_loss: 0.3552 - val_acc: 0.8490\n",
      "Epoch 146/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3433 - acc: 0.8551 - val_loss: 0.3553 - val_acc: 0.8405\n",
      "Epoch 147/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3449 - acc: 0.8590 - val_loss: 0.3560 - val_acc: 0.8435\n",
      "Epoch 148/150\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3393 - acc: 0.8597 - val_loss: 0.3556 - val_acc: 0.8440\n",
      "Epoch 149/150\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3390 - acc: 0.8574 - val_loss: 0.3700 - val_acc: 0.8425\n",
      "Epoch 150/150\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3395 - acc: 0.8589 - val_loss: 0.3548 - val_acc: 0.8475\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "epochs = 150\n",
    "model_hist = model.fit(trainData, trainTarget, epochs=150, batch_size=128, validation_data=(testData, testTarget))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will plot the loss once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_acc = model_hist.history['acc']\n",
    "model_loss = model_hist.history['loss']\n",
    "model_val_acc = model_hist.history['val_acc']\n",
    "model_val_loss = model_hist.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(model_acc) + 1)\n",
    "# b+ is for \"blue cross\"\n",
    "plt.plot(epochs, model_val_loss, 'b+', label='Val Loss')\n",
    "plt.plot(epochs, model_loss, 'b', label='Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks a little better. Now to evaluate with the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 24us/step\n",
      "Test Loss:  0.3548030288219452\n",
      "Test Accuracy:  0.8475\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(testData, testTarget)\n",
    "print(\"Test Loss: \", test_loss)\n",
    "print(\"Test Accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
