{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My very first project with supervised deep learning. I have been reading \"Deep Learning with Python\" by fran√ßois chollet. I am not finished with it, but I really wanted to create my own solution on a given data set. \n",
    "\n",
    "The dataset I will be using is https://www.kaggle.com/sonalidasgupta95/churn-prediction-of-bank-customers/downloads/churn-prediction-of-bank-customers.zip/1\n",
    "It will be a binary classification problem to determine if a customer will exit the bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('Churn_Modelling.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just looking at the data, I will parseout columns 0, 1, 2: Row Number, CustomerId, Surname as these features should have no impact on the prediction of someone exiting the bank. Thus, I am going to use CreditScore, Geography, Gender, Age, Tenure, Balance, NumOfProducts, HasCrCard, IsActiveMember, and EstimatedSalary to determine whether the customer will exit. Also, I will preprocess the data that are inconvienient for the network\n",
    "    - Geography: France will be 0, Spain will be 1, and Germany will be 2\n",
    "    - Gender: Female will be 0, Male will be 1\n",
    "    - Balance: Get the mean then divide by the standard deviation\n",
    "    - EstimatedSalary: Get the mean then divide by the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'Exited']\n",
      "[6.1900000e+02 0.0000000e+00 0.0000000e+00 4.2000000e+01 2.0000000e+00\n",
      " 0.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0134888e+05\n",
      " 1.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# Getting Data\n",
    "with open('Churn_Modelling.csv', 'r') as f:\n",
    "    data = list(csv.reader(f, delimiter=';'))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data[i] = data[i][0].split(',')\n",
    "    \n",
    "    # Switching gender to numeric\n",
    "    if data[i][5] == \"Female\":\n",
    "        data[i][5] = 0\n",
    "    elif data[i][5] == \"Male\":\n",
    "        data[i][5] = 1\n",
    "    \n",
    "    # Switching geography to numeric\n",
    "    if data[i][4] == \"France\":\n",
    "        data[i][4] = 0\n",
    "    elif data[i][4] == \"Spain\":\n",
    "        data[i][4] = 1\n",
    "    elif data[i][4] == \"Germany\":\n",
    "        data[i][4] = 2\n",
    "    \n",
    "    data[i].pop(2)\n",
    "    data[i].pop(1)\n",
    "    data[i].pop(0)\n",
    "    \n",
    "    if i != 0:\n",
    "        for j in range(len(data[i])):\n",
    "            data[i][j] = float(data[i][j])\n",
    "            \n",
    "print(data[0])\n",
    "data.pop(0)\n",
    "data = np.array(data, np.float64)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above shows you an example of data formatted in which order. With the data dissected, I am going to cut the data into a training data and testing data. 75% of the data will be used for training and the rest for testing. I will also create a target array for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Splitting the data into train and test\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# 75% of the data will be training data\n",
    "numTrain = math.floor(len(data) * .8)\n",
    "\n",
    "trainData = data[:numTrain]\n",
    "testData = data[numTrain:]\n",
    "\n",
    "trainTarget = []\n",
    "testTarget = []\n",
    "\n",
    "for i in range(len(trainData)):\n",
    "    trainTarget.append(trainData[i][10])\n",
    "for i in range(len(testData)):\n",
    "    testTarget.append(testData[i][10])\n",
    "    \n",
    "trainData = np.delete(trainData, 10, 1)\n",
    "testData = np.delete(testData, 10, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having split up the data, it is ready for it's final data preprocessig. Data features: CreditScore, Balance, and Salary are normalized. By subtracting the feature's mean off it's own and dividing it by the standard deviation, I can make the range of the data smaller. This is so that the network does not encounter wide extreme numbers on input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.58269984  2.          0.         29.          3.          0.86699885\n",
      "  1.          1.          0.         -0.67764669]\n",
      "[-0.44833093  2.          1.         36.         10.          0.47968633\n",
      "  2.          0.          0.          1.71082004]\n"
     ]
    }
   ],
   "source": [
    "# Normalization\n",
    "credit = []\n",
    "salary = []\n",
    "balance = []\n",
    "\n",
    "for i in range(len(trainData)):\n",
    "    credit.append(trainData[i][0])\n",
    "    balance.append(trainData[i][5])\n",
    "    salary.append(trainData[i][9])\n",
    "\n",
    "creditMean = np.mean(credit)\n",
    "salaryMean = np.mean(salary)\n",
    "balanceMean = np.mean(balance)\n",
    "creditStd = np.std(credit)\n",
    "salaryStd = np.std(salary)\n",
    "balanceStd = np.std(balance)\n",
    "\n",
    "trainData[:, 0] -= creditMean\n",
    "trainData[:, 0] /= creditStd\n",
    "trainData[:, 5] -= balanceMean\n",
    "trainData[:, 5] /= balanceStd\n",
    "trainData[:, 9] -= salaryMean\n",
    "trainData[:, 9] /= salaryStd\n",
    "\n",
    "testData[:, 0] -= creditMean\n",
    "testData[:, 0] /= creditStd\n",
    "testData[:, 5] -= balanceMean\n",
    "testData[:, 5] /= balanceStd\n",
    "testData[:, 9] -= salaryMean\n",
    "testData[:, 9] /= salaryStd\n",
    "\n",
    "numVal = math.floor(len(trainData) * .10)\n",
    "valData = trainData[:numVal]\n",
    "valTarget = trainTarget[:numVal]\n",
    "trainData = trainData[numVal:]\n",
    "trainTarget = trainTarget[numVal:]\n",
    "\n",
    "print(trainData[0])\n",
    "print(testData[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above shows how the range of the features have drastically decreased. Credit score, salary, and balance are now all normalized and ready to be inputted into our model. \n",
    "The network model has 4 layers:\n",
    "    Input Layer : size of 10,\n",
    "    Hidden Layer 1 : size of 64,\n",
    "    Hidden LAyer 2 : size of 64,\n",
    "    Output LAyer : size of 1.\n",
    "    \n",
    "As for the compilers, I am testing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/300\n",
      "7200/7200 [==============================] - 0s 51us/step - loss: 0.5842 - acc: 0.7833 - val_loss: 0.5211 - val_acc: 0.8025\n",
      "Epoch 2/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.5182 - acc: 0.7939 - val_loss: 0.4816 - val_acc: 0.8025\n",
      "Epoch 3/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.4883 - acc: 0.7972 - val_loss: 0.4461 - val_acc: 0.8025\n",
      "Epoch 4/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4685 - acc: 0.7994 - val_loss: 0.4339 - val_acc: 0.8337\n",
      "Epoch 5/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.4624 - acc: 0.7994 - val_loss: 0.4201 - val_acc: 0.8325\n",
      "Epoch 6/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4553 - acc: 0.8039 - val_loss: 0.4188 - val_acc: 0.8350\n",
      "Epoch 7/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4486 - acc: 0.8092 - val_loss: 0.4131 - val_acc: 0.8325\n",
      "Epoch 8/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4456 - acc: 0.8099 - val_loss: 0.4095 - val_acc: 0.8250\n",
      "Epoch 9/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4442 - acc: 0.8106 - val_loss: 0.4126 - val_acc: 0.8213\n",
      "Epoch 10/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4455 - acc: 0.8079 - val_loss: 0.4115 - val_acc: 0.8313\n",
      "Epoch 11/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4449 - acc: 0.8082 - val_loss: 0.4143 - val_acc: 0.8350\n",
      "Epoch 12/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4483 - acc: 0.8079 - val_loss: 0.4551 - val_acc: 0.8050\n",
      "Epoch 13/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.4529 - acc: 0.8022 - val_loss: 0.4089 - val_acc: 0.8287\n",
      "Epoch 14/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4412 - acc: 0.8086 - val_loss: 0.4067 - val_acc: 0.8275\n",
      "Epoch 15/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4424 - acc: 0.8078 - val_loss: 0.4069 - val_acc: 0.8225\n",
      "Epoch 16/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4444 - acc: 0.8071 - val_loss: 0.4086 - val_acc: 0.8300\n",
      "Epoch 17/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4387 - acc: 0.8099 - val_loss: 0.4131 - val_acc: 0.8150\n",
      "Epoch 18/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4465 - acc: 0.8058 - val_loss: 0.4187 - val_acc: 0.8263\n",
      "Epoch 19/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4431 - acc: 0.8064 - val_loss: 0.4078 - val_acc: 0.8300\n",
      "Epoch 20/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.4425 - acc: 0.8072 - val_loss: 0.4141 - val_acc: 0.8337\n",
      "Epoch 21/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.4383 - acc: 0.8115 - val_loss: 0.4059 - val_acc: 0.8300\n",
      "Epoch 22/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4412 - acc: 0.8071 - val_loss: 0.4075 - val_acc: 0.8350\n",
      "Epoch 23/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4332 - acc: 0.8117 - val_loss: 0.4063 - val_acc: 0.8337\n",
      "Epoch 24/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.4308 - acc: 0.8106 - val_loss: 0.4010 - val_acc: 0.8275\n",
      "Epoch 25/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4266 - acc: 0.8140 - val_loss: 0.3969 - val_acc: 0.8300\n",
      "Epoch 26/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4239 - acc: 0.8161 - val_loss: 0.4067 - val_acc: 0.8350\n",
      "Epoch 27/300\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.4200 - acc: 0.8178 - val_loss: 0.4007 - val_acc: 0.8287\n",
      "Epoch 28/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.4207 - acc: 0.8157 - val_loss: 0.3902 - val_acc: 0.8275\n",
      "Epoch 29/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4181 - acc: 0.8200 - val_loss: 0.3905 - val_acc: 0.8313\n",
      "Epoch 30/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4102 - acc: 0.8242 - val_loss: 0.3857 - val_acc: 0.8325\n",
      "Epoch 31/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.4080 - acc: 0.8242 - val_loss: 0.3867 - val_acc: 0.8413\n",
      "Epoch 32/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.4090 - acc: 0.8260 - val_loss: 0.3864 - val_acc: 0.8337\n",
      "Epoch 33/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4112 - acc: 0.8211 - val_loss: 0.3836 - val_acc: 0.8400\n",
      "Epoch 34/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4035 - acc: 0.8297 - val_loss: 0.3918 - val_acc: 0.8475\n",
      "Epoch 35/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4014 - acc: 0.8275 - val_loss: 0.3944 - val_acc: 0.8425\n",
      "Epoch 36/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4047 - acc: 0.8257 - val_loss: 0.3814 - val_acc: 0.8438\n",
      "Epoch 37/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.4012 - acc: 0.8279 - val_loss: 0.3805 - val_acc: 0.8387\n",
      "Epoch 38/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3995 - acc: 0.8262 - val_loss: 0.3807 - val_acc: 0.8387\n",
      "Epoch 39/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3937 - acc: 0.8314 - val_loss: 0.3732 - val_acc: 0.8413\n",
      "Epoch 40/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4017 - acc: 0.8235 - val_loss: 0.3942 - val_acc: 0.8237\n",
      "Epoch 41/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3949 - acc: 0.8294 - val_loss: 0.3787 - val_acc: 0.8350\n",
      "Epoch 42/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3923 - acc: 0.8304 - val_loss: 0.3732 - val_acc: 0.8425\n",
      "Epoch 43/300\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.3936 - acc: 0.8314 - val_loss: 0.3743 - val_acc: 0.8413\n",
      "Epoch 44/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3861 - acc: 0.8346 - val_loss: 0.3786 - val_acc: 0.8387\n",
      "Epoch 45/300\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3886 - acc: 0.8353 - val_loss: 0.3684 - val_acc: 0.8450\n",
      "Epoch 46/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3841 - acc: 0.8369 - val_loss: 0.3766 - val_acc: 0.8425\n",
      "Epoch 47/300\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3872 - acc: 0.8321 - val_loss: 0.3740 - val_acc: 0.8438\n",
      "Epoch 48/300\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 0.3867 - acc: 0.8336 - val_loss: 0.3800 - val_acc: 0.8363\n",
      "Epoch 49/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3829 - acc: 0.8385 - val_loss: 0.3797 - val_acc: 0.8450\n",
      "Epoch 50/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3852 - acc: 0.8328 - val_loss: 0.3713 - val_acc: 0.8413\n",
      "Epoch 51/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3890 - acc: 0.8318 - val_loss: 0.3665 - val_acc: 0.8450\n",
      "Epoch 52/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3806 - acc: 0.8393 - val_loss: 0.3740 - val_acc: 0.8375\n",
      "Epoch 53/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3789 - acc: 0.8379 - val_loss: 0.3677 - val_acc: 0.8425\n",
      "Epoch 54/300\n",
      "7200/7200 [==============================] - 0s 26us/step - loss: 0.3795 - acc: 0.8401 - val_loss: 0.3692 - val_acc: 0.8450\n",
      "Epoch 55/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3835 - acc: 0.8399 - val_loss: 0.3663 - val_acc: 0.8450\n",
      "Epoch 56/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3788 - acc: 0.8372 - val_loss: 0.3718 - val_acc: 0.8425\n",
      "Epoch 57/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3791 - acc: 0.8397 - val_loss: 0.3630 - val_acc: 0.8438\n",
      "Epoch 58/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3837 - acc: 0.8342 - val_loss: 0.3621 - val_acc: 0.8438\n",
      "Epoch 59/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3808 - acc: 0.8374 - val_loss: 0.3679 - val_acc: 0.8450\n",
      "Epoch 60/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 0s 25us/step - loss: 0.3827 - acc: 0.8387 - val_loss: 0.3684 - val_acc: 0.8438\n",
      "Epoch 61/300\n",
      "7200/7200 [==============================] - 0s 21us/step - loss: 0.3765 - acc: 0.8383 - val_loss: 0.3896 - val_acc: 0.8425\n",
      "Epoch 62/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3754 - acc: 0.8390 - val_loss: 0.3716 - val_acc: 0.8425\n",
      "Epoch 63/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3720 - acc: 0.8426 - val_loss: 0.3661 - val_acc: 0.8462\n",
      "Epoch 64/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3724 - acc: 0.8411 - val_loss: 0.3603 - val_acc: 0.8500\n",
      "Epoch 65/300\n",
      "7200/7200 [==============================] - 0s 21us/step - loss: 0.3751 - acc: 0.8424 - val_loss: 0.3702 - val_acc: 0.8450\n",
      "Epoch 66/300\n",
      "7200/7200 [==============================] - 0s 28us/step - loss: 0.3736 - acc: 0.8436 - val_loss: 0.3695 - val_acc: 0.8488\n",
      "Epoch 67/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3722 - acc: 0.8426 - val_loss: 0.3643 - val_acc: 0.8525\n",
      "Epoch 68/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3734 - acc: 0.8419 - val_loss: 0.4063 - val_acc: 0.8313\n",
      "Epoch 69/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3748 - acc: 0.8396 - val_loss: 0.3570 - val_acc: 0.8512\n",
      "Epoch 70/300\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3680 - acc: 0.8454 - val_loss: 0.3677 - val_acc: 0.8512\n",
      "Epoch 71/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3726 - acc: 0.8433 - val_loss: 0.3596 - val_acc: 0.8575\n",
      "Epoch 72/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3707 - acc: 0.8443 - val_loss: 0.3647 - val_acc: 0.8525\n",
      "Epoch 73/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3667 - acc: 0.8476 - val_loss: 0.3629 - val_acc: 0.8512\n",
      "Epoch 74/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3683 - acc: 0.8436 - val_loss: 0.3652 - val_acc: 0.8488\n",
      "Epoch 75/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3667 - acc: 0.8451 - val_loss: 0.3605 - val_acc: 0.8525\n",
      "Epoch 76/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3652 - acc: 0.8474 - val_loss: 0.3657 - val_acc: 0.8438\n",
      "Epoch 77/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3694 - acc: 0.8444 - val_loss: 0.3660 - val_acc: 0.8525\n",
      "Epoch 78/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3656 - acc: 0.8471 - val_loss: 0.4049 - val_acc: 0.8337\n",
      "Epoch 79/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3701 - acc: 0.8457 - val_loss: 0.3532 - val_acc: 0.8525\n",
      "Epoch 80/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3642 - acc: 0.8478 - val_loss: 0.3559 - val_acc: 0.8475\n",
      "Epoch 81/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3707 - acc: 0.8438 - val_loss: 0.3731 - val_acc: 0.8413\n",
      "Epoch 82/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3640 - acc: 0.8493 - val_loss: 0.3613 - val_acc: 0.8538\n",
      "Epoch 83/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3645 - acc: 0.8485 - val_loss: 0.3574 - val_acc: 0.8512\n",
      "Epoch 84/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3662 - acc: 0.8468 - val_loss: 0.3709 - val_acc: 0.8512\n",
      "Epoch 85/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3629 - acc: 0.8508 - val_loss: 0.3534 - val_acc: 0.8525\n",
      "Epoch 86/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3628 - acc: 0.8481 - val_loss: 0.3677 - val_acc: 0.8488\n",
      "Epoch 87/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3663 - acc: 0.8481 - val_loss: 0.3515 - val_acc: 0.8562\n",
      "Epoch 88/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3608 - acc: 0.8519 - val_loss: 0.3580 - val_acc: 0.8538\n",
      "Epoch 89/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3595 - acc: 0.8499 - val_loss: 0.3595 - val_acc: 0.8550\n",
      "Epoch 90/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3593 - acc: 0.8515 - val_loss: 0.3611 - val_acc: 0.8550\n",
      "Epoch 91/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3656 - acc: 0.8446 - val_loss: 0.3738 - val_acc: 0.8425\n",
      "Epoch 92/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3628 - acc: 0.8515 - val_loss: 0.3546 - val_acc: 0.8575\n",
      "Epoch 93/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3599 - acc: 0.8493 - val_loss: 0.3564 - val_acc: 0.8500\n",
      "Epoch 94/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3648 - acc: 0.8469 - val_loss: 0.3823 - val_acc: 0.8450\n",
      "Epoch 95/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3622 - acc: 0.8521 - val_loss: 0.3537 - val_acc: 0.8612\n",
      "Epoch 96/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3568 - acc: 0.8522 - val_loss: 0.3592 - val_acc: 0.8550\n",
      "Epoch 97/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3583 - acc: 0.8526 - val_loss: 0.3648 - val_acc: 0.8550\n",
      "Epoch 98/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3610 - acc: 0.8510 - val_loss: 0.3593 - val_acc: 0.8575\n",
      "Epoch 99/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3573 - acc: 0.8522 - val_loss: 0.3546 - val_acc: 0.8600\n",
      "Epoch 100/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3582 - acc: 0.8493 - val_loss: 0.3570 - val_acc: 0.8575\n",
      "Epoch 101/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3620 - acc: 0.8475 - val_loss: 0.3575 - val_acc: 0.8625\n",
      "Epoch 102/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3559 - acc: 0.8550 - val_loss: 0.3476 - val_acc: 0.8600\n",
      "Epoch 103/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3559 - acc: 0.8526 - val_loss: 0.3508 - val_acc: 0.8575\n",
      "Epoch 104/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3584 - acc: 0.8510 - val_loss: 0.3554 - val_acc: 0.8538\n",
      "Epoch 105/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3589 - acc: 0.8507 - val_loss: 0.3649 - val_acc: 0.8612\n",
      "Epoch 106/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3561 - acc: 0.8511 - val_loss: 0.3569 - val_acc: 0.8625\n",
      "Epoch 107/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3553 - acc: 0.8542 - val_loss: 0.3609 - val_acc: 0.8625\n",
      "Epoch 108/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3572 - acc: 0.8514 - val_loss: 0.3669 - val_acc: 0.8562\n",
      "Epoch 109/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3528 - acc: 0.8528 - val_loss: 0.3487 - val_acc: 0.8612\n",
      "Epoch 110/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3529 - acc: 0.8533 - val_loss: 0.3604 - val_acc: 0.8538\n",
      "Epoch 111/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3558 - acc: 0.8540 - val_loss: 0.3821 - val_acc: 0.8488\n",
      "Epoch 112/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3507 - acc: 0.8546 - val_loss: 0.3584 - val_acc: 0.8625\n",
      "Epoch 113/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3537 - acc: 0.8525 - val_loss: 0.3803 - val_acc: 0.8500\n",
      "Epoch 114/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3532 - acc: 0.8493 - val_loss: 0.3697 - val_acc: 0.8562\n",
      "Epoch 115/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3525 - acc: 0.8547 - val_loss: 0.3475 - val_acc: 0.8662\n",
      "Epoch 116/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3535 - acc: 0.8549 - val_loss: 0.3601 - val_acc: 0.8588\n",
      "Epoch 117/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3536 - acc: 0.8518 - val_loss: 0.3486 - val_acc: 0.8662\n",
      "Epoch 118/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3502 - acc: 0.8569 - val_loss: 0.3485 - val_acc: 0.8588\n",
      "Epoch 119/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3532 - acc: 0.8515 - val_loss: 0.3424 - val_acc: 0.8625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3493 - acc: 0.8560 - val_loss: 0.3495 - val_acc: 0.8600\n",
      "Epoch 121/300\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 0.3495 - acc: 0.8554 - val_loss: 0.3440 - val_acc: 0.8662\n",
      "Epoch 122/300\n",
      "7200/7200 [==============================] - 0s 21us/step - loss: 0.3511 - acc: 0.8539 - val_loss: 0.3429 - val_acc: 0.8662\n",
      "Epoch 123/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3510 - acc: 0.8539 - val_loss: 0.3502 - val_acc: 0.8625\n",
      "Epoch 124/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3510 - acc: 0.8538 - val_loss: 0.3496 - val_acc: 0.8612\n",
      "Epoch 125/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3487 - acc: 0.8553 - val_loss: 0.3412 - val_acc: 0.8650\n",
      "Epoch 126/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3473 - acc: 0.8553 - val_loss: 0.3413 - val_acc: 0.8725\n",
      "Epoch 127/300\n",
      "7200/7200 [==============================] - 0s 26us/step - loss: 0.3499 - acc: 0.8546 - val_loss: 0.3465 - val_acc: 0.8675\n",
      "Epoch 128/300\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3498 - acc: 0.8568 - val_loss: 0.3441 - val_acc: 0.8638\n",
      "Epoch 129/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3507 - acc: 0.8553 - val_loss: 0.3381 - val_acc: 0.8662\n",
      "Epoch 130/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3463 - acc: 0.8568 - val_loss: 0.3467 - val_acc: 0.8638\n",
      "Epoch 131/300\n",
      "7200/7200 [==============================] - 0s 23us/step - loss: 0.3469 - acc: 0.8565 - val_loss: 0.3423 - val_acc: 0.8662\n",
      "Epoch 132/300\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 0.3447 - acc: 0.8572 - val_loss: 0.3822 - val_acc: 0.8525\n",
      "Epoch 133/300\n",
      "7200/7200 [==============================] - 0s 22us/step - loss: 0.3533 - acc: 0.8525 - val_loss: 0.3974 - val_acc: 0.8363\n",
      "Epoch 134/300\n",
      "7200/7200 [==============================] - 0s 21us/step - loss: 0.3541 - acc: 0.8519 - val_loss: 0.3470 - val_acc: 0.8662\n",
      "Epoch 135/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3495 - acc: 0.8538 - val_loss: 0.3429 - val_acc: 0.8700\n",
      "Epoch 136/300\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3452 - acc: 0.8553 - val_loss: 0.3423 - val_acc: 0.8688\n",
      "Epoch 137/300\n",
      "7200/7200 [==============================] - 0s 24us/step - loss: 0.3447 - acc: 0.8578 - val_loss: 0.3475 - val_acc: 0.8700\n",
      "Epoch 138/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3441 - acc: 0.8585 - val_loss: 0.3447 - val_acc: 0.8575\n",
      "Epoch 139/300\n",
      "7200/7200 [==============================] - 0s 44us/step - loss: 0.3442 - acc: 0.8574 - val_loss: 0.3652 - val_acc: 0.8550\n",
      "Epoch 140/300\n",
      "7200/7200 [==============================] - 0s 28us/step - loss: 0.3452 - acc: 0.8578 - val_loss: 0.3423 - val_acc: 0.8625\n",
      "Epoch 141/300\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 0.3441 - acc: 0.8558 - val_loss: 0.3461 - val_acc: 0.8625\n",
      "Epoch 142/300\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 0.3450 - acc: 0.8576 - val_loss: 0.3406 - val_acc: 0.8625\n",
      "Epoch 143/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3461 - acc: 0.8578 - val_loss: 0.3527 - val_acc: 0.8588\n",
      "Epoch 144/300\n",
      "7200/7200 [==============================] - 0s 28us/step - loss: 0.3481 - acc: 0.8551 - val_loss: 0.3535 - val_acc: 0.8638\n",
      "Epoch 145/300\n",
      "7200/7200 [==============================] - 0s 21us/step - loss: 0.3477 - acc: 0.8556 - val_loss: 0.3417 - val_acc: 0.8675\n",
      "Epoch 146/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3478 - acc: 0.8544 - val_loss: 0.3427 - val_acc: 0.8688\n",
      "Epoch 147/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3433 - acc: 0.8589 - val_loss: 0.3388 - val_acc: 0.8638\n",
      "Epoch 148/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3443 - acc: 0.8590 - val_loss: 0.3985 - val_acc: 0.8350\n",
      "Epoch 149/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3539 - acc: 0.8518 - val_loss: 0.3436 - val_acc: 0.8625\n",
      "Epoch 150/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3497 - acc: 0.8528 - val_loss: 0.3471 - val_acc: 0.8525\n",
      "Epoch 151/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3433 - acc: 0.8589 - val_loss: 0.3342 - val_acc: 0.8750\n",
      "Epoch 152/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3422 - acc: 0.8581 - val_loss: 0.3482 - val_acc: 0.8662\n",
      "Epoch 153/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3420 - acc: 0.8585 - val_loss: 0.3431 - val_acc: 0.8662\n",
      "Epoch 154/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3404 - acc: 0.8606 - val_loss: 0.3450 - val_acc: 0.8650\n",
      "Epoch 155/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3414 - acc: 0.8583 - val_loss: 0.3353 - val_acc: 0.8688\n",
      "Epoch 156/300\n",
      "7200/7200 [==============================] - 0s 33us/step - loss: 0.3436 - acc: 0.8553 - val_loss: 0.3383 - val_acc: 0.8625\n",
      "Epoch 157/300\n",
      "7200/7200 [==============================] - 0s 21us/step - loss: 0.3405 - acc: 0.8565 - val_loss: 0.3372 - val_acc: 0.8725\n",
      "Epoch 158/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3377 - acc: 0.8608 - val_loss: 0.3357 - val_acc: 0.8700\n",
      "Epoch 159/300\n",
      "7200/7200 [==============================] - 0s 22us/step - loss: 0.3428 - acc: 0.8568 - val_loss: 0.3395 - val_acc: 0.8662\n",
      "Epoch 160/300\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3417 - acc: 0.8576 - val_loss: 0.3420 - val_acc: 0.8712\n",
      "Epoch 161/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3398 - acc: 0.8599 - val_loss: 0.3419 - val_acc: 0.8588\n",
      "Epoch 162/300\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3402 - acc: 0.8590 - val_loss: 0.3352 - val_acc: 0.8675\n",
      "Epoch 163/300\n",
      "7200/7200 [==============================] - 0s 21us/step - loss: 0.3422 - acc: 0.8582 - val_loss: 0.3336 - val_acc: 0.8725\n",
      "Epoch 164/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3390 - acc: 0.8590 - val_loss: 0.3421 - val_acc: 0.8625\n",
      "Epoch 165/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3428 - acc: 0.8568 - val_loss: 0.3452 - val_acc: 0.8662\n",
      "Epoch 166/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3420 - acc: 0.8569 - val_loss: 0.3422 - val_acc: 0.8700\n",
      "Epoch 167/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3366 - acc: 0.8607 - val_loss: 0.3304 - val_acc: 0.8675\n",
      "Epoch 168/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3381 - acc: 0.8613 - val_loss: 0.3407 - val_acc: 0.8638\n",
      "Epoch 169/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3491 - acc: 0.8525 - val_loss: 0.3308 - val_acc: 0.8662\n",
      "Epoch 170/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3404 - acc: 0.8582 - val_loss: 0.3331 - val_acc: 0.8725\n",
      "Epoch 171/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3359 - acc: 0.8615 - val_loss: 0.3354 - val_acc: 0.8675\n",
      "Epoch 172/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3373 - acc: 0.8601 - val_loss: 0.3402 - val_acc: 0.8662\n",
      "Epoch 173/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3368 - acc: 0.8600 - val_loss: 0.3377 - val_acc: 0.8762\n",
      "Epoch 174/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3433 - acc: 0.8582 - val_loss: 0.3399 - val_acc: 0.8688\n",
      "Epoch 175/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3401 - acc: 0.8585 - val_loss: 0.3313 - val_acc: 0.8750\n",
      "Epoch 176/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3375 - acc: 0.8583 - val_loss: 0.3330 - val_acc: 0.8650\n",
      "Epoch 177/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3360 - acc: 0.8603 - val_loss: 0.3399 - val_acc: 0.8712\n",
      "Epoch 178/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3371 - acc: 0.8585 - val_loss: 0.3366 - val_acc: 0.8675\n",
      "Epoch 179/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3346 - acc: 0.8603 - val_loss: 0.3310 - val_acc: 0.8650\n",
      "Epoch 180/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3438 - acc: 0.8535 - val_loss: 0.3342 - val_acc: 0.8738\n",
      "Epoch 181/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3349 - acc: 0.8622 - val_loss: 0.3534 - val_acc: 0.8638\n",
      "Epoch 182/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3384 - acc: 0.8594 - val_loss: 0.3416 - val_acc: 0.8712\n",
      "Epoch 183/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3363 - acc: 0.8599 - val_loss: 0.3370 - val_acc: 0.8662\n",
      "Epoch 184/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3366 - acc: 0.8621 - val_loss: 0.3311 - val_acc: 0.8662\n",
      "Epoch 185/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3327 - acc: 0.8621 - val_loss: 0.3381 - val_acc: 0.8675\n",
      "Epoch 186/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3336 - acc: 0.8600 - val_loss: 0.3435 - val_acc: 0.8638\n",
      "Epoch 187/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3433 - acc: 0.8578 - val_loss: 0.3319 - val_acc: 0.8738\n",
      "Epoch 188/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3434 - acc: 0.8572 - val_loss: 0.3417 - val_acc: 0.8675\n",
      "Epoch 189/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3325 - acc: 0.8621 - val_loss: 0.3417 - val_acc: 0.8650\n",
      "Epoch 190/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3325 - acc: 0.8596 - val_loss: 0.3336 - val_acc: 0.8612\n",
      "Epoch 191/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3380 - acc: 0.8611 - val_loss: 0.3334 - val_acc: 0.8725\n",
      "Epoch 192/300\n",
      "7200/7200 [==============================] - 0s 10us/step - loss: 0.3343 - acc: 0.8611 - val_loss: 0.3346 - val_acc: 0.8750\n",
      "Epoch 193/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3331 - acc: 0.8639 - val_loss: 0.3378 - val_acc: 0.8675\n",
      "Epoch 194/300\n",
      "7200/7200 [==============================] - 0s 10us/step - loss: 0.3363 - acc: 0.8569 - val_loss: 0.3430 - val_acc: 0.8712\n",
      "Epoch 195/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3350 - acc: 0.8583 - val_loss: 0.3467 - val_acc: 0.8650\n",
      "Epoch 196/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3337 - acc: 0.8619 - val_loss: 0.3360 - val_acc: 0.8688\n",
      "Epoch 197/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3339 - acc: 0.8632 - val_loss: 0.3387 - val_acc: 0.8650\n",
      "Epoch 198/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3325 - acc: 0.8628 - val_loss: 0.3299 - val_acc: 0.8738\n",
      "Epoch 199/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3310 - acc: 0.8642 - val_loss: 0.3463 - val_acc: 0.8650\n",
      "Epoch 200/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3324 - acc: 0.8613 - val_loss: 0.3292 - val_acc: 0.8700\n",
      "Epoch 201/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3318 - acc: 0.8640 - val_loss: 0.3396 - val_acc: 0.8688\n",
      "Epoch 202/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3336 - acc: 0.8625 - val_loss: 0.3336 - val_acc: 0.8612\n",
      "Epoch 203/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3303 - acc: 0.8647 - val_loss: 0.3388 - val_acc: 0.8750\n",
      "Epoch 204/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3309 - acc: 0.8643 - val_loss: 0.3400 - val_acc: 0.8762\n",
      "Epoch 205/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3378 - acc: 0.8617 - val_loss: 0.3308 - val_acc: 0.8650\n",
      "Epoch 206/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3298 - acc: 0.8624 - val_loss: 0.3363 - val_acc: 0.8712\n",
      "Epoch 207/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3283 - acc: 0.8642 - val_loss: 0.3372 - val_acc: 0.8712\n",
      "Epoch 208/300\n",
      "7200/7200 [==============================] - 0s 21us/step - loss: 0.3330 - acc: 0.8613 - val_loss: 0.3378 - val_acc: 0.8712\n",
      "Epoch 209/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3297 - acc: 0.8614 - val_loss: 0.3315 - val_acc: 0.8712\n",
      "Epoch 210/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3290 - acc: 0.8658 - val_loss: 0.3383 - val_acc: 0.8725\n",
      "Epoch 211/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3286 - acc: 0.8633 - val_loss: 0.3325 - val_acc: 0.8688\n",
      "Epoch 212/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3284 - acc: 0.8635 - val_loss: 0.3283 - val_acc: 0.8712\n",
      "Epoch 213/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3294 - acc: 0.8622 - val_loss: 0.3304 - val_acc: 0.8712\n",
      "Epoch 214/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3271 - acc: 0.8644 - val_loss: 0.3386 - val_acc: 0.8625\n",
      "Epoch 215/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3290 - acc: 0.8619 - val_loss: 0.3336 - val_acc: 0.8688\n",
      "Epoch 216/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3299 - acc: 0.8643 - val_loss: 0.3373 - val_acc: 0.8675\n",
      "Epoch 217/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3290 - acc: 0.8631 - val_loss: 0.3334 - val_acc: 0.8650\n",
      "Epoch 218/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3275 - acc: 0.8650 - val_loss: 0.3432 - val_acc: 0.8575\n",
      "Epoch 219/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3369 - acc: 0.8610 - val_loss: 0.3290 - val_acc: 0.8688\n",
      "Epoch 220/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3312 - acc: 0.8619 - val_loss: 0.3278 - val_acc: 0.8675\n",
      "Epoch 221/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3302 - acc: 0.8656 - val_loss: 0.3340 - val_acc: 0.8650\n",
      "Epoch 222/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3307 - acc: 0.8607 - val_loss: 0.3306 - val_acc: 0.8775\n",
      "Epoch 223/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3321 - acc: 0.8628 - val_loss: 0.3377 - val_acc: 0.8575\n",
      "Epoch 224/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3298 - acc: 0.8619 - val_loss: 0.3333 - val_acc: 0.8712\n",
      "Epoch 225/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3281 - acc: 0.8636 - val_loss: 0.3376 - val_acc: 0.8725\n",
      "Epoch 226/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3286 - acc: 0.8646 - val_loss: 0.3320 - val_acc: 0.8688\n",
      "Epoch 227/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3300 - acc: 0.8622 - val_loss: 0.3342 - val_acc: 0.8762\n",
      "Epoch 228/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3243 - acc: 0.8649 - val_loss: 0.3317 - val_acc: 0.8738\n",
      "Epoch 229/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3252 - acc: 0.8649 - val_loss: 0.3357 - val_acc: 0.8688\n",
      "Epoch 230/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3249 - acc: 0.8635 - val_loss: 0.3382 - val_acc: 0.8625\n",
      "Epoch 231/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3249 - acc: 0.8660 - val_loss: 0.3383 - val_acc: 0.8725\n",
      "Epoch 232/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3292 - acc: 0.8649 - val_loss: 0.3264 - val_acc: 0.8662\n",
      "Epoch 233/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3279 - acc: 0.8644 - val_loss: 0.3323 - val_acc: 0.8712\n",
      "Epoch 234/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3282 - acc: 0.8663 - val_loss: 0.3309 - val_acc: 0.8738\n",
      "Epoch 235/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3239 - acc: 0.8660 - val_loss: 0.3355 - val_acc: 0.8738\n",
      "Epoch 236/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3242 - acc: 0.8661 - val_loss: 0.3314 - val_acc: 0.8675\n",
      "Epoch 237/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3246 - acc: 0.8651 - val_loss: 0.3331 - val_acc: 0.8662\n",
      "Epoch 238/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3256 - acc: 0.8644 - val_loss: 0.3360 - val_acc: 0.8650\n",
      "Epoch 239/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3265 - acc: 0.8667 - val_loss: 0.3314 - val_acc: 0.8688\n",
      "Epoch 240/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3229 - acc: 0.8665 - val_loss: 0.3282 - val_acc: 0.8700\n",
      "Epoch 241/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3299 - acc: 0.8631 - val_loss: 0.3494 - val_acc: 0.8638\n",
      "Epoch 242/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3249 - acc: 0.8632 - val_loss: 0.3283 - val_acc: 0.8750\n",
      "Epoch 243/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3236 - acc: 0.8643 - val_loss: 0.3353 - val_acc: 0.8650\n",
      "Epoch 244/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3265 - acc: 0.8639 - val_loss: 0.3331 - val_acc: 0.8650\n",
      "Epoch 245/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3257 - acc: 0.8646 - val_loss: 0.3449 - val_acc: 0.8625\n",
      "Epoch 246/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3244 - acc: 0.8636 - val_loss: 0.3305 - val_acc: 0.8700\n",
      "Epoch 247/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3241 - acc: 0.8658 - val_loss: 0.3357 - val_acc: 0.8650\n",
      "Epoch 248/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3291 - acc: 0.8644 - val_loss: 0.3361 - val_acc: 0.8750\n",
      "Epoch 249/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3253 - acc: 0.8654 - val_loss: 0.3349 - val_acc: 0.8675\n",
      "Epoch 250/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3273 - acc: 0.8615 - val_loss: 0.3349 - val_acc: 0.8700\n",
      "Epoch 251/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3230 - acc: 0.8642 - val_loss: 0.3255 - val_acc: 0.8612\n",
      "Epoch 252/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3277 - acc: 0.8625 - val_loss: 0.3347 - val_acc: 0.8688\n",
      "Epoch 253/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3254 - acc: 0.8650 - val_loss: 0.3430 - val_acc: 0.8638\n",
      "Epoch 254/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3283 - acc: 0.8647 - val_loss: 0.3336 - val_acc: 0.8712\n",
      "Epoch 255/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3241 - acc: 0.8646 - val_loss: 0.3334 - val_acc: 0.8700\n",
      "Epoch 256/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3250 - acc: 0.8654 - val_loss: 0.3327 - val_acc: 0.8700\n",
      "Epoch 257/300\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3213 - acc: 0.8667 - val_loss: 0.3348 - val_acc: 0.8675\n",
      "Epoch 258/300\n",
      "7200/7200 [==============================] - 0s 21us/step - loss: 0.3256 - acc: 0.8624 - val_loss: 0.3414 - val_acc: 0.8688\n",
      "Epoch 259/300\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3189 - acc: 0.8660 - val_loss: 0.3309 - val_acc: 0.8700\n",
      "Epoch 260/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3225 - acc: 0.8644 - val_loss: 0.3369 - val_acc: 0.8625\n",
      "Epoch 261/300\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3291 - acc: 0.8638 - val_loss: 0.3370 - val_acc: 0.8625\n",
      "Epoch 262/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3226 - acc: 0.8651 - val_loss: 0.3341 - val_acc: 0.8638\n",
      "Epoch 263/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3202 - acc: 0.8679 - val_loss: 0.3347 - val_acc: 0.8688\n",
      "Epoch 264/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3198 - acc: 0.8653 - val_loss: 0.3313 - val_acc: 0.8688\n",
      "Epoch 265/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3192 - acc: 0.8674 - val_loss: 0.3356 - val_acc: 0.8650\n",
      "Epoch 266/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3213 - acc: 0.8644 - val_loss: 0.3364 - val_acc: 0.8750\n",
      "Epoch 267/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3211 - acc: 0.8650 - val_loss: 0.3329 - val_acc: 0.8700\n",
      "Epoch 268/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3207 - acc: 0.8631 - val_loss: 0.3560 - val_acc: 0.8588\n",
      "Epoch 269/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3279 - acc: 0.8649 - val_loss: 0.3336 - val_acc: 0.8688\n",
      "Epoch 270/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3218 - acc: 0.8665 - val_loss: 0.3322 - val_acc: 0.8638\n",
      "Epoch 271/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3262 - acc: 0.8660 - val_loss: 0.3321 - val_acc: 0.8738\n",
      "Epoch 272/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3202 - acc: 0.8682 - val_loss: 0.3434 - val_acc: 0.8675\n",
      "Epoch 273/300\n",
      "7200/7200 [==============================] - ETA: 0s - loss: 0.3170 - acc: 0.867 - 0s 12us/step - loss: 0.3195 - acc: 0.8679 - val_loss: 0.3314 - val_acc: 0.8638\n",
      "Epoch 274/300\n",
      "7200/7200 [==============================] - 0s 11us/step - loss: 0.3168 - acc: 0.8685 - val_loss: 0.3345 - val_acc: 0.8675\n",
      "Epoch 275/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3217 - acc: 0.8656 - val_loss: 0.3354 - val_acc: 0.8700\n",
      "Epoch 276/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3176 - acc: 0.8665 - val_loss: 0.3318 - val_acc: 0.8688\n",
      "Epoch 277/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3184 - acc: 0.8657 - val_loss: 0.3359 - val_acc: 0.8738\n",
      "Epoch 278/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3167 - acc: 0.8663 - val_loss: 0.3405 - val_acc: 0.8700\n",
      "Epoch 279/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3156 - acc: 0.8681 - val_loss: 0.3343 - val_acc: 0.8700\n",
      "Epoch 280/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3166 - acc: 0.8676 - val_loss: 0.3322 - val_acc: 0.8700\n",
      "Epoch 281/300\n",
      "7200/7200 [==============================] - 0s 23us/step - loss: 0.3162 - acc: 0.8676 - val_loss: 0.3466 - val_acc: 0.8650\n",
      "Epoch 282/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3165 - acc: 0.8682 - val_loss: 0.3392 - val_acc: 0.8675\n",
      "Epoch 283/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3186 - acc: 0.8664 - val_loss: 0.3373 - val_acc: 0.8625\n",
      "Epoch 284/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3178 - acc: 0.8672 - val_loss: 0.3431 - val_acc: 0.8675\n",
      "Epoch 285/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3178 - acc: 0.8660 - val_loss: 0.3389 - val_acc: 0.8662\n",
      "Epoch 286/300\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.3175 - acc: 0.8688 - val_loss: 0.3343 - val_acc: 0.8650\n",
      "Epoch 287/300\n",
      "7200/7200 [==============================] - 0s 21us/step - loss: 0.3173 - acc: 0.8653 - val_loss: 0.3482 - val_acc: 0.8575\n",
      "Epoch 288/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3171 - acc: 0.8676 - val_loss: 0.3339 - val_acc: 0.8662\n",
      "Epoch 289/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3165 - acc: 0.8668 - val_loss: 0.3372 - val_acc: 0.8688\n",
      "Epoch 290/300\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3161 - acc: 0.8679 - val_loss: 0.3415 - val_acc: 0.8738\n",
      "Epoch 291/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3162 - acc: 0.8676 - val_loss: 0.3384 - val_acc: 0.8625\n",
      "Epoch 292/300\n",
      "7200/7200 [==============================] - 0s 28us/step - loss: 0.3178 - acc: 0.8660 - val_loss: 0.3425 - val_acc: 0.8662\n",
      "Epoch 293/300\n",
      "7200/7200 [==============================] - ETA: 0s - loss: 0.3153 - acc: 0.867 - 0s 24us/step - loss: 0.3172 - acc: 0.8669 - val_loss: 0.3356 - val_acc: 0.8688\n",
      "Epoch 294/300\n",
      "7200/7200 [==============================] - 0s 21us/step - loss: 0.3169 - acc: 0.8669 - val_loss: 0.3352 - val_acc: 0.8712\n",
      "Epoch 295/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3159 - acc: 0.8665 - val_loss: 0.3488 - val_acc: 0.8600\n",
      "Epoch 296/300\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3163 - acc: 0.8685 - val_loss: 0.3426 - val_acc: 0.8650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3181 - acc: 0.8665 - val_loss: 0.3368 - val_acc: 0.8638\n",
      "Epoch 298/300\n",
      "7200/7200 [==============================] - 0s 12us/step - loss: 0.3145 - acc: 0.8697 - val_loss: 0.3393 - val_acc: 0.8662\n",
      "Epoch 299/300\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3128 - acc: 0.8675 - val_loss: 0.3362 - val_acc: 0.8675\n",
      "Epoch 300/300\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.3152 - acc: 0.8690 - val_loss: 0.3588 - val_acc: 0.8562\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "epochs = 200\n",
    "model_hist = model.fit(trainData, trainTarget, epochs=300, batch_size=128, validation_data=(valData, valTarget))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_acc = model_hist.history['acc']\n",
    "model_loss = model_hist.history['loss']\n",
    "model_val_acc = model_hist.history['val_acc']\n",
    "model_val_loss = model_hist.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(model_acc) + 1)\n",
    "# b+ is for \"blue cross\"\n",
    "plt.plot(epochs, model_val_loss, 'b+', label='Val Loss')\n",
    "plt.plot(epochs, model_loss, 'b', label='Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To combat overfitting, I have plotted the loss for both validation data set and training data set for over 300 epochs. From the graph, I can vaguely tell that the model is starting to overfit at 250 epochs. At 250 epochs, the loss score is stagnant for valdation data; while the loss score for the training data set keeps decreasing. Thus, I'll retrain the model upto 250 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200 samples, validate on 2000 samples\n",
      "Epoch 1/250\n",
      "7200/7200 [==============================] - 0s 55us/step - loss: 0.6964 - acc: 0.7407 - val_loss: 0.5151 - val_acc: 0.8025\n",
      "Epoch 2/250\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.5085 - acc: 0.7939 - val_loss: 0.4870 - val_acc: 0.8025\n",
      "Epoch 3/250\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.4800 - acc: 0.7969 - val_loss: 0.4790 - val_acc: 0.7865\n",
      "Epoch 4/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.4661 - acc: 0.7989 - val_loss: 0.4736 - val_acc: 0.7905\n",
      "Epoch 5/250\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.4576 - acc: 0.7969 - val_loss: 0.4503 - val_acc: 0.8060\n",
      "Epoch 6/250\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.4521 - acc: 0.8068 - val_loss: 0.4530 - val_acc: 0.8070\n",
      "Epoch 7/250\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.4543 - acc: 0.8014 - val_loss: 0.4508 - val_acc: 0.8075\n",
      "Epoch 8/250\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.4477 - acc: 0.8069 - val_loss: 0.4474 - val_acc: 0.8105\n",
      "Epoch 9/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.4463 - acc: 0.8051 - val_loss: 0.4452 - val_acc: 0.8100\n",
      "Epoch 10/250\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.4487 - acc: 0.8046 - val_loss: 0.4765 - val_acc: 0.7840\n",
      "Epoch 11/250\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.4474 - acc: 0.8079 - val_loss: 0.4519 - val_acc: 0.8025\n",
      "Epoch 12/250\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.4424 - acc: 0.8086 - val_loss: 0.4501 - val_acc: 0.8095\n",
      "Epoch 13/250\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4521 - acc: 0.8013 - val_loss: 0.4428 - val_acc: 0.8125\n",
      "Epoch 14/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4460 - acc: 0.8046 - val_loss: 0.4415 - val_acc: 0.8120\n",
      "Epoch 15/250\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4454 - acc: 0.8076 - val_loss: 0.4413 - val_acc: 0.8115\n",
      "Epoch 16/250\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4391 - acc: 0.8132 - val_loss: 0.4455 - val_acc: 0.8060\n",
      "Epoch 17/250\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4394 - acc: 0.8072 - val_loss: 0.4581 - val_acc: 0.8070\n",
      "Epoch 18/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.4394 - acc: 0.8111 - val_loss: 0.4664 - val_acc: 0.7860\n",
      "Epoch 19/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4407 - acc: 0.8078 - val_loss: 0.4554 - val_acc: 0.8085\n",
      "Epoch 20/250\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4458 - acc: 0.8106 - val_loss: 0.4356 - val_acc: 0.8180\n",
      "Epoch 21/250\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.4355 - acc: 0.8104 - val_loss: 0.4363 - val_acc: 0.8170\n",
      "Epoch 22/250\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.4371 - acc: 0.8086 - val_loss: 0.4348 - val_acc: 0.8180\n",
      "Epoch 23/250\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4349 - acc: 0.8100 - val_loss: 0.4351 - val_acc: 0.8165\n",
      "Epoch 24/250\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4352 - acc: 0.8121 - val_loss: 0.4453 - val_acc: 0.8035\n",
      "Epoch 25/250\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4384 - acc: 0.8089 - val_loss: 0.4335 - val_acc: 0.8170\n",
      "Epoch 26/250\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4351 - acc: 0.8087 - val_loss: 0.4680 - val_acc: 0.7915\n",
      "Epoch 27/250\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4331 - acc: 0.8099 - val_loss: 0.4353 - val_acc: 0.8165\n",
      "Epoch 28/250\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.4318 - acc: 0.8107 - val_loss: 0.4333 - val_acc: 0.8165\n",
      "Epoch 29/250\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4311 - acc: 0.8104 - val_loss: 0.4914 - val_acc: 0.7705\n",
      "Epoch 30/250\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4474 - acc: 0.8044 - val_loss: 0.4409 - val_acc: 0.8135\n",
      "Epoch 31/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4370 - acc: 0.8081 - val_loss: 0.4358 - val_acc: 0.8145\n",
      "Epoch 32/250\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4350 - acc: 0.8101 - val_loss: 0.4325 - val_acc: 0.8170\n",
      "Epoch 33/250\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.4244 - acc: 0.8136 - val_loss: 0.4332 - val_acc: 0.8145\n",
      "Epoch 34/250\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.4351 - acc: 0.8118 - val_loss: 0.4438 - val_acc: 0.8020\n",
      "Epoch 35/250\n",
      "7200/7200 [==============================] - 0s 22us/step - loss: 0.4304 - acc: 0.8125 - val_loss: 0.4276 - val_acc: 0.8195\n",
      "Epoch 36/250\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.4364 - acc: 0.8064 - val_loss: 0.4283 - val_acc: 0.8210\n",
      "Epoch 37/250\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.4374 - acc: 0.8101 - val_loss: 0.4268 - val_acc: 0.8195\n",
      "Epoch 38/250\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.4235 - acc: 0.8160 - val_loss: 0.4271 - val_acc: 0.8210\n",
      "Epoch 39/250\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.4233 - acc: 0.8197 - val_loss: 0.4280 - val_acc: 0.8185\n",
      "Epoch 40/250\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.4208 - acc: 0.8169 - val_loss: 0.4236 - val_acc: 0.8195\n",
      "Epoch 41/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.4182 - acc: 0.8197 - val_loss: 0.4316 - val_acc: 0.8175\n",
      "Epoch 42/250\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.4239 - acc: 0.8140 - val_loss: 0.4279 - val_acc: 0.8195\n",
      "Epoch 43/250\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.4245 - acc: 0.8199 - val_loss: 0.4320 - val_acc: 0.8120\n",
      "Epoch 44/250\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.4180 - acc: 0.8169 - val_loss: 0.4210 - val_acc: 0.8245\n",
      "Epoch 45/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.4163 - acc: 0.8186 - val_loss: 0.4195 - val_acc: 0.8265\n",
      "Epoch 46/250\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.4138 - acc: 0.8197 - val_loss: 0.4231 - val_acc: 0.8180\n",
      "Epoch 47/250\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.4268 - acc: 0.8168 - val_loss: 0.4155 - val_acc: 0.8295\n",
      "Epoch 48/250\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.4127 - acc: 0.8215 - val_loss: 0.4152 - val_acc: 0.8275\n",
      "Epoch 49/250\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.4087 - acc: 0.8249 - val_loss: 0.4160 - val_acc: 0.8265\n",
      "Epoch 50/250\n",
      "7200/7200 [==============================] - 0s 21us/step - loss: 0.4115 - acc: 0.8236 - val_loss: 0.4144 - val_acc: 0.8270\n",
      "Epoch 51/250\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.4134 - acc: 0.8228 - val_loss: 0.4184 - val_acc: 0.8215\n",
      "Epoch 52/250\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.4113 - acc: 0.8222 - val_loss: 0.4215 - val_acc: 0.8170\n",
      "Epoch 53/250\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.4164 - acc: 0.8190 - val_loss: 0.4455 - val_acc: 0.8000\n",
      "Epoch 54/250\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.4211 - acc: 0.8176 - val_loss: 0.4280 - val_acc: 0.8135\n",
      "Epoch 55/250\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.4087 - acc: 0.8258 - val_loss: 0.4296 - val_acc: 0.8200\n",
      "Epoch 56/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.4099 - acc: 0.8233 - val_loss: 0.4292 - val_acc: 0.8060\n",
      "Epoch 57/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4006 - acc: 0.8297 - val_loss: 0.4149 - val_acc: 0.8210\n",
      "Epoch 58/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.4003 - acc: 0.8286 - val_loss: 0.4125 - val_acc: 0.8285\n",
      "Epoch 59/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4012 - acc: 0.8249 - val_loss: 0.4012 - val_acc: 0.8335\n",
      "Epoch 60/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3975 - acc: 0.8304 - val_loss: 0.4021 - val_acc: 0.8340\n",
      "Epoch 61/250\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 0.4005 - acc: 0.8288 - val_loss: 0.4011 - val_acc: 0.8280\n",
      "Epoch 62/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3928 - acc: 0.8322 - val_loss: 0.4026 - val_acc: 0.8280\n",
      "Epoch 63/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3973 - acc: 0.8281 - val_loss: 0.4047 - val_acc: 0.8285\n",
      "Epoch 64/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3979 - acc: 0.8307 - val_loss: 0.4014 - val_acc: 0.8305\n",
      "Epoch 65/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3910 - acc: 0.8353 - val_loss: 0.4008 - val_acc: 0.8325\n",
      "Epoch 66/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3889 - acc: 0.8363 - val_loss: 0.3954 - val_acc: 0.8275\n",
      "Epoch 67/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3928 - acc: 0.8329 - val_loss: 0.3976 - val_acc: 0.8330\n",
      "Epoch 68/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3876 - acc: 0.8350 - val_loss: 0.3961 - val_acc: 0.8305\n",
      "Epoch 69/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.4004 - acc: 0.8260 - val_loss: 0.3974 - val_acc: 0.8300\n",
      "Epoch 70/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3893 - acc: 0.8347 - val_loss: 0.4111 - val_acc: 0.8205\n",
      "Epoch 71/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3852 - acc: 0.8393 - val_loss: 0.4042 - val_acc: 0.8285\n",
      "Epoch 72/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3853 - acc: 0.8392 - val_loss: 0.3834 - val_acc: 0.8400\n",
      "Epoch 73/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3805 - acc: 0.8408 - val_loss: 0.3840 - val_acc: 0.8345\n",
      "Epoch 74/250\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3789 - acc: 0.8393 - val_loss: 0.4015 - val_acc: 0.8225\n",
      "Epoch 75/250\n",
      "7200/7200 [==============================] - 0s 41us/step - loss: 0.3768 - acc: 0.8438 - val_loss: 0.3804 - val_acc: 0.8405\n",
      "Epoch 76/250\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3736 - acc: 0.8432 - val_loss: 0.3763 - val_acc: 0.8440\n",
      "Epoch 77/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3726 - acc: 0.8444 - val_loss: 0.3919 - val_acc: 0.8385\n",
      "Epoch 78/250\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.3850 - acc: 0.8394 - val_loss: 0.3898 - val_acc: 0.8355\n",
      "Epoch 79/250\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3747 - acc: 0.8440 - val_loss: 0.3746 - val_acc: 0.8490\n",
      "Epoch 80/250\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3768 - acc: 0.8433 - val_loss: 0.3829 - val_acc: 0.8420\n",
      "Epoch 81/250\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3710 - acc: 0.8418 - val_loss: 0.3798 - val_acc: 0.8440\n",
      "Epoch 82/250\n",
      "7200/7200 [==============================] - 0s 21us/step - loss: 0.3671 - acc: 0.8472 - val_loss: 0.3734 - val_acc: 0.8450\n",
      "Epoch 83/250\n",
      "7200/7200 [==============================] - 0s 21us/step - loss: 0.3657 - acc: 0.8472 - val_loss: 0.3778 - val_acc: 0.8430\n",
      "Epoch 84/250\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3654 - acc: 0.8458 - val_loss: 0.3714 - val_acc: 0.8415\n",
      "Epoch 85/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3729 - acc: 0.8457 - val_loss: 0.3692 - val_acc: 0.8475\n",
      "Epoch 86/250\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3656 - acc: 0.8503 - val_loss: 0.3693 - val_acc: 0.8435\n",
      "Epoch 87/250\n",
      "7200/7200 [==============================] - 0s 30us/step - loss: 0.3665 - acc: 0.8471 - val_loss: 0.4129 - val_acc: 0.8160\n",
      "Epoch 88/250\n",
      "7200/7200 [==============================] - 0s 49us/step - loss: 0.3730 - acc: 0.8428 - val_loss: 0.3728 - val_acc: 0.8495\n",
      "Epoch 89/250\n",
      "7200/7200 [==============================] - 0s 22us/step - loss: 0.3661 - acc: 0.8450 - val_loss: 0.3681 - val_acc: 0.8480\n",
      "Epoch 90/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3617 - acc: 0.8517 - val_loss: 0.3663 - val_acc: 0.8490\n",
      "Epoch 91/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3620 - acc: 0.8499 - val_loss: 0.3648 - val_acc: 0.8490\n",
      "Epoch 92/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3612 - acc: 0.8496 - val_loss: 0.3693 - val_acc: 0.8470\n",
      "Epoch 93/250\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3610 - acc: 0.8488 - val_loss: 0.3652 - val_acc: 0.8495\n",
      "Epoch 94/250\n",
      "7200/7200 [==============================] - ETA: 0s - loss: 0.3600 - acc: 0.849 - 0s 16us/step - loss: 0.3584 - acc: 0.8517 - val_loss: 0.3649 - val_acc: 0.8505\n",
      "Epoch 95/250\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3571 - acc: 0.8499 - val_loss: 0.3657 - val_acc: 0.8485\n",
      "Epoch 96/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3641 - acc: 0.8488 - val_loss: 0.3648 - val_acc: 0.8510\n",
      "Epoch 97/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3621 - acc: 0.8462 - val_loss: 0.3651 - val_acc: 0.8525\n",
      "Epoch 98/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3595 - acc: 0.8499 - val_loss: 0.3633 - val_acc: 0.8520\n",
      "Epoch 99/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3546 - acc: 0.8501 - val_loss: 0.3630 - val_acc: 0.8540\n",
      "Epoch 100/250\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3616 - acc: 0.8486 - val_loss: 0.3641 - val_acc: 0.8525\n",
      "Epoch 101/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3550 - acc: 0.8506 - val_loss: 0.3656 - val_acc: 0.8510\n",
      "Epoch 102/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3616 - acc: 0.8499 - val_loss: 0.3607 - val_acc: 0.8480\n",
      "Epoch 103/250\n",
      "7200/7200 [==============================] - 0s 26us/step - loss: 0.3524 - acc: 0.8512 - val_loss: 0.3613 - val_acc: 0.8505\n",
      "Epoch 104/250\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3551 - acc: 0.8518 - val_loss: 0.3626 - val_acc: 0.8490\n",
      "Epoch 105/250\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3520 - acc: 0.8535 - val_loss: 0.3636 - val_acc: 0.8530\n",
      "Epoch 106/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3520 - acc: 0.8517 - val_loss: 0.3598 - val_acc: 0.8525\n",
      "Epoch 107/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3507 - acc: 0.8536 - val_loss: 0.3689 - val_acc: 0.8435\n",
      "Epoch 108/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3500 - acc: 0.8532 - val_loss: 0.3626 - val_acc: 0.8515\n",
      "Epoch 109/250\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3503 - acc: 0.8551 - val_loss: 0.3585 - val_acc: 0.8510\n",
      "Epoch 110/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3497 - acc: 0.8533 - val_loss: 0.3620 - val_acc: 0.8510\n",
      "Epoch 111/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3503 - acc: 0.8556 - val_loss: 0.3671 - val_acc: 0.8480\n",
      "Epoch 112/250\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3540 - acc: 0.8526 - val_loss: 0.3623 - val_acc: 0.8490\n",
      "Epoch 113/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3543 - acc: 0.8512 - val_loss: 0.3847 - val_acc: 0.8460\n",
      "Epoch 114/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3522 - acc: 0.8526 - val_loss: 0.3588 - val_acc: 0.8515\n",
      "Epoch 115/250\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.3465 - acc: 0.8558 - val_loss: 0.3624 - val_acc: 0.8510\n",
      "Epoch 116/250\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3513 - acc: 0.8524 - val_loss: 0.3630 - val_acc: 0.8475\n",
      "Epoch 117/250\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3489 - acc: 0.8550 - val_loss: 0.3604 - val_acc: 0.8545\n",
      "Epoch 118/250\n",
      "7200/7200 [==============================] - 0s 44us/step - loss: 0.3476 - acc: 0.8569 - val_loss: 0.3686 - val_acc: 0.8545\n",
      "Epoch 119/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 0s 32us/step - loss: 0.3487 - acc: 0.8532 - val_loss: 0.3653 - val_acc: 0.8520\n",
      "Epoch 120/250\n",
      "7200/7200 [==============================] - 0s 24us/step - loss: 0.3539 - acc: 0.8539 - val_loss: 0.3574 - val_acc: 0.8475\n",
      "Epoch 121/250\n",
      "7200/7200 [==============================] - 0s 41us/step - loss: 0.3499 - acc: 0.8544 - val_loss: 0.3622 - val_acc: 0.8525\n",
      "Epoch 122/250\n",
      "7200/7200 [==============================] - 0s 24us/step - loss: 0.3454 - acc: 0.8557 - val_loss: 0.3611 - val_acc: 0.8505\n",
      "Epoch 123/250\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.3449 - acc: 0.8565 - val_loss: 0.3594 - val_acc: 0.8535\n",
      "Epoch 124/250\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3505 - acc: 0.8549 - val_loss: 0.4092 - val_acc: 0.8385\n",
      "Epoch 125/250\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3532 - acc: 0.8529 - val_loss: 0.3600 - val_acc: 0.8535\n",
      "Epoch 126/250\n",
      "7200/7200 [==============================] - 0s 28us/step - loss: 0.3511 - acc: 0.8535 - val_loss: 0.3576 - val_acc: 0.8530\n",
      "Epoch 127/250\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3511 - acc: 0.8561 - val_loss: 0.3585 - val_acc: 0.8530\n",
      "Epoch 128/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3463 - acc: 0.8583 - val_loss: 0.3706 - val_acc: 0.8420\n",
      "Epoch 129/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3485 - acc: 0.8535 - val_loss: 0.3733 - val_acc: 0.8415\n",
      "Epoch 130/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3483 - acc: 0.8567 - val_loss: 0.3586 - val_acc: 0.8540\n",
      "Epoch 131/250\n",
      "7200/7200 [==============================] - 0s 21us/step - loss: 0.3445 - acc: 0.8569 - val_loss: 0.3606 - val_acc: 0.8545\n",
      "Epoch 132/250\n",
      "7200/7200 [==============================] - 0s 30us/step - loss: 0.3484 - acc: 0.8536 - val_loss: 0.3607 - val_acc: 0.8490\n",
      "Epoch 133/250\n",
      "7200/7200 [==============================] - 0s 22us/step - loss: 0.3440 - acc: 0.8600 - val_loss: 0.3602 - val_acc: 0.8565\n",
      "Epoch 134/250\n",
      "7200/7200 [==============================] - 0s 23us/step - loss: 0.3442 - acc: 0.8569 - val_loss: 0.3586 - val_acc: 0.8585\n",
      "Epoch 135/250\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3454 - acc: 0.8560 - val_loss: 0.3638 - val_acc: 0.8505\n",
      "Epoch 136/250\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3443 - acc: 0.8564 - val_loss: 0.3665 - val_acc: 0.8450\n",
      "Epoch 137/250\n",
      "7200/7200 [==============================] - 0s 22us/step - loss: 0.3480 - acc: 0.8518 - val_loss: 0.3564 - val_acc: 0.8560\n",
      "Epoch 138/250\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3435 - acc: 0.8572 - val_loss: 0.3577 - val_acc: 0.8565\n",
      "Epoch 139/250\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.3473 - acc: 0.8553 - val_loss: 0.3616 - val_acc: 0.8555\n",
      "Epoch 140/250\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3440 - acc: 0.8568 - val_loss: 0.3571 - val_acc: 0.8535\n",
      "Epoch 141/250\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3466 - acc: 0.8589 - val_loss: 0.3586 - val_acc: 0.8535\n",
      "Epoch 142/250\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3512 - acc: 0.8518 - val_loss: 0.3578 - val_acc: 0.8525\n",
      "Epoch 143/250\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 0.3426 - acc: 0.8572 - val_loss: 0.3583 - val_acc: 0.8525\n",
      "Epoch 144/250\n",
      "7200/7200 [==============================] - 0s 21us/step - loss: 0.3413 - acc: 0.8564 - val_loss: 0.3578 - val_acc: 0.8545\n",
      "Epoch 145/250\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.3411 - acc: 0.8597 - val_loss: 0.3571 - val_acc: 0.8550\n",
      "Epoch 146/250\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3434 - acc: 0.8574 - val_loss: 0.3619 - val_acc: 0.8510\n",
      "Epoch 147/250\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3457 - acc: 0.8549 - val_loss: 0.3779 - val_acc: 0.8370\n",
      "Epoch 148/250\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3430 - acc: 0.8553 - val_loss: 0.3566 - val_acc: 0.8555\n",
      "Epoch 149/250\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3406 - acc: 0.8593 - val_loss: 0.3570 - val_acc: 0.8560\n",
      "Epoch 150/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3453 - acc: 0.8546 - val_loss: 0.3553 - val_acc: 0.8580\n",
      "Epoch 151/250\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3419 - acc: 0.8576 - val_loss: 0.3663 - val_acc: 0.8440\n",
      "Epoch 152/250\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.3406 - acc: 0.8587 - val_loss: 0.3564 - val_acc: 0.8580\n",
      "Epoch 153/250\n",
      "7200/7200 [==============================] - 0s 22us/step - loss: 0.3392 - acc: 0.8594 - val_loss: 0.3653 - val_acc: 0.8490\n",
      "Epoch 154/250\n",
      "7200/7200 [==============================] - 0s 28us/step - loss: 0.3402 - acc: 0.8554 - val_loss: 0.3722 - val_acc: 0.8390\n",
      "Epoch 155/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3396 - acc: 0.8593 - val_loss: 0.3702 - val_acc: 0.8575\n",
      "Epoch 156/250\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3434 - acc: 0.8571 - val_loss: 0.3643 - val_acc: 0.8490\n",
      "Epoch 157/250\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3403 - acc: 0.8592 - val_loss: 0.3535 - val_acc: 0.8535\n",
      "Epoch 158/250\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.3380 - acc: 0.8596 - val_loss: 0.3594 - val_acc: 0.8585\n",
      "Epoch 159/250\n",
      "7200/7200 [==============================] - 0s 22us/step - loss: 0.3406 - acc: 0.8557 - val_loss: 0.3574 - val_acc: 0.8540\n",
      "Epoch 160/250\n",
      "7200/7200 [==============================] - 0s 22us/step - loss: 0.3406 - acc: 0.8615 - val_loss: 0.3563 - val_acc: 0.8550\n",
      "Epoch 161/250\n",
      "7200/7200 [==============================] - 0s 22us/step - loss: 0.3368 - acc: 0.8594 - val_loss: 0.3649 - val_acc: 0.8470\n",
      "Epoch 162/250\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3396 - acc: 0.8579 - val_loss: 0.3637 - val_acc: 0.8495\n",
      "Epoch 163/250\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.3386 - acc: 0.8600 - val_loss: 0.3606 - val_acc: 0.8555\n",
      "Epoch 164/250\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3450 - acc: 0.8543 - val_loss: 0.3592 - val_acc: 0.8525\n",
      "Epoch 165/250\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3364 - acc: 0.8599 - val_loss: 0.3571 - val_acc: 0.8495\n",
      "Epoch 166/250\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.3422 - acc: 0.8590 - val_loss: 0.3550 - val_acc: 0.8560\n",
      "Epoch 167/250\n",
      "7200/7200 [==============================] - 0s 21us/step - loss: 0.3374 - acc: 0.8599 - val_loss: 0.3538 - val_acc: 0.8560\n",
      "Epoch 168/250\n",
      "7200/7200 [==============================] - 0s 21us/step - loss: 0.3344 - acc: 0.8608 - val_loss: 0.3503 - val_acc: 0.8580\n",
      "Epoch 169/250\n",
      "7200/7200 [==============================] - 0s 23us/step - loss: 0.3369 - acc: 0.8582 - val_loss: 0.3573 - val_acc: 0.8550\n",
      "Epoch 170/250\n",
      "7200/7200 [==============================] - 0s 24us/step - loss: 0.3362 - acc: 0.8585 - val_loss: 0.3560 - val_acc: 0.8575\n",
      "Epoch 171/250\n",
      "7200/7200 [==============================] - 0s 21us/step - loss: 0.3369 - acc: 0.8582 - val_loss: 0.3570 - val_acc: 0.8535\n",
      "Epoch 172/250\n",
      "7200/7200 [==============================] - ETA: 0s - loss: 0.3318 - acc: 0.863 - 0s 21us/step - loss: 0.3356 - acc: 0.8607 - val_loss: 0.3538 - val_acc: 0.8565\n",
      "Epoch 173/250\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.3402 - acc: 0.8608 - val_loss: 0.3619 - val_acc: 0.8555\n",
      "Epoch 174/250\n",
      "7200/7200 [==============================] - 0s 21us/step - loss: 0.3390 - acc: 0.8565 - val_loss: 0.3568 - val_acc: 0.8520\n",
      "Epoch 175/250\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3359 - acc: 0.8607 - val_loss: 0.3502 - val_acc: 0.8605\n",
      "Epoch 176/250\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.3372 - acc: 0.8596 - val_loss: 0.3612 - val_acc: 0.8545\n",
      "Epoch 177/250\n",
      "7200/7200 [==============================] - 0s 32us/step - loss: 0.3336 - acc: 0.8611 - val_loss: 0.3502 - val_acc: 0.8560\n",
      "Epoch 178/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 0s 32us/step - loss: 0.3348 - acc: 0.8569 - val_loss: 0.3556 - val_acc: 0.8540\n",
      "Epoch 179/250\n",
      "7200/7200 [==============================] - 0s 23us/step - loss: 0.3362 - acc: 0.8600 - val_loss: 0.3500 - val_acc: 0.8565\n",
      "Epoch 180/250\n",
      "7200/7200 [==============================] - 0s 22us/step - loss: 0.3357 - acc: 0.8596 - val_loss: 0.3534 - val_acc: 0.8560\n",
      "Epoch 181/250\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3322 - acc: 0.8628 - val_loss: 0.3494 - val_acc: 0.8570\n",
      "Epoch 182/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3313 - acc: 0.8599 - val_loss: 0.3565 - val_acc: 0.8525\n",
      "Epoch 183/250\n",
      "7200/7200 [==============================] - 0s 21us/step - loss: 0.3340 - acc: 0.8603 - val_loss: 0.3552 - val_acc: 0.8560\n",
      "Epoch 184/250\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.3322 - acc: 0.8601 - val_loss: 0.3516 - val_acc: 0.8510\n",
      "Epoch 185/250\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3318 - acc: 0.8600 - val_loss: 0.3613 - val_acc: 0.8525\n",
      "Epoch 186/250\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3325 - acc: 0.8619 - val_loss: 0.3545 - val_acc: 0.8570\n",
      "Epoch 187/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3362 - acc: 0.8585 - val_loss: 0.3530 - val_acc: 0.8510\n",
      "Epoch 188/250\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3289 - acc: 0.8631 - val_loss: 0.3495 - val_acc: 0.8590\n",
      "Epoch 189/250\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3305 - acc: 0.8608 - val_loss: 0.3532 - val_acc: 0.8590\n",
      "Epoch 190/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3281 - acc: 0.8638 - val_loss: 0.3570 - val_acc: 0.8550\n",
      "Epoch 191/250\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3332 - acc: 0.8604 - val_loss: 0.3514 - val_acc: 0.8560\n",
      "Epoch 192/250\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3314 - acc: 0.8601 - val_loss: 0.3504 - val_acc: 0.8605\n",
      "Epoch 193/250\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3301 - acc: 0.8614 - val_loss: 0.3551 - val_acc: 0.8505\n",
      "Epoch 194/250\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3308 - acc: 0.8601 - val_loss: 0.3571 - val_acc: 0.8470\n",
      "Epoch 195/250\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3282 - acc: 0.8619 - val_loss: 0.3499 - val_acc: 0.8575\n",
      "Epoch 196/250\n",
      "7200/7200 [==============================] - 0s 36us/step - loss: 0.3279 - acc: 0.8629 - val_loss: 0.3470 - val_acc: 0.8630\n",
      "Epoch 197/250\n",
      "7200/7200 [==============================] - 0s 40us/step - loss: 0.3277 - acc: 0.8613 - val_loss: 0.3457 - val_acc: 0.8605\n",
      "Epoch 198/250\n",
      "7200/7200 [==============================] - 0s 16us/step - loss: 0.3272 - acc: 0.8628 - val_loss: 0.3488 - val_acc: 0.8525\n",
      "Epoch 199/250\n",
      "7200/7200 [==============================] - 0s 26us/step - loss: 0.3282 - acc: 0.8594 - val_loss: 0.3540 - val_acc: 0.8585\n",
      "Epoch 200/250\n",
      "7200/7200 [==============================] - 0s 21us/step - loss: 0.3278 - acc: 0.8613 - val_loss: 0.3459 - val_acc: 0.8610\n",
      "Epoch 201/250\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3250 - acc: 0.8638 - val_loss: 0.3465 - val_acc: 0.8600\n",
      "Epoch 202/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3243 - acc: 0.8643 - val_loss: 0.3642 - val_acc: 0.8525\n",
      "Epoch 203/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3294 - acc: 0.8600 - val_loss: 0.3447 - val_acc: 0.8605\n",
      "Epoch 204/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3251 - acc: 0.8636 - val_loss: 0.3481 - val_acc: 0.8560\n",
      "Epoch 205/250\n",
      "7200/7200 [==============================] - 0s 29us/step - loss: 0.3269 - acc: 0.8617 - val_loss: 0.3628 - val_acc: 0.8530\n",
      "Epoch 206/250\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3259 - acc: 0.8607 - val_loss: 0.3470 - val_acc: 0.8600\n",
      "Epoch 207/250\n",
      "7200/7200 [==============================] - 0s 18us/step - loss: 0.3249 - acc: 0.8626 - val_loss: 0.3479 - val_acc: 0.8590\n",
      "Epoch 208/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3254 - acc: 0.8600 - val_loss: 0.3491 - val_acc: 0.8595\n",
      "Epoch 209/250\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3236 - acc: 0.8649 - val_loss: 0.3602 - val_acc: 0.8525\n",
      "Epoch 210/250\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3258 - acc: 0.8657 - val_loss: 0.3470 - val_acc: 0.8595\n",
      "Epoch 211/250\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3210 - acc: 0.8650 - val_loss: 0.3438 - val_acc: 0.8575\n",
      "Epoch 212/250\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.3234 - acc: 0.8643 - val_loss: 0.3436 - val_acc: 0.8620\n",
      "Epoch 213/250\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3219 - acc: 0.8664 - val_loss: 0.3446 - val_acc: 0.8625\n",
      "Epoch 214/250\n",
      "7200/7200 [==============================] - 0s 21us/step - loss: 0.3212 - acc: 0.8668 - val_loss: 0.3448 - val_acc: 0.8600\n",
      "Epoch 215/250\n",
      "7200/7200 [==============================] - 0s 23us/step - loss: 0.3201 - acc: 0.8661 - val_loss: 0.3454 - val_acc: 0.8560\n",
      "Epoch 216/250\n",
      "7200/7200 [==============================] - 0s 21us/step - loss: 0.3200 - acc: 0.8636 - val_loss: 0.3491 - val_acc: 0.8560\n",
      "Epoch 217/250\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.3260 - acc: 0.8636 - val_loss: 0.3465 - val_acc: 0.8575\n",
      "Epoch 218/250\n",
      "7200/7200 [==============================] - 0s 28us/step - loss: 0.3235 - acc: 0.8632 - val_loss: 0.3450 - val_acc: 0.8585\n",
      "Epoch 219/250\n",
      "7200/7200 [==============================] - 0s 22us/step - loss: 0.3199 - acc: 0.8676 - val_loss: 0.3469 - val_acc: 0.8600\n",
      "Epoch 220/250\n",
      "7200/7200 [==============================] - 0s 27us/step - loss: 0.3206 - acc: 0.8665 - val_loss: 0.3474 - val_acc: 0.8590\n",
      "Epoch 221/250\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.3213 - acc: 0.8665 - val_loss: 0.3453 - val_acc: 0.8585\n",
      "Epoch 222/250\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.3215 - acc: 0.8640 - val_loss: 0.3451 - val_acc: 0.8590\n",
      "Epoch 223/250\n",
      "7200/7200 [==============================] - 0s 17us/step - loss: 0.3219 - acc: 0.8651 - val_loss: 0.3479 - val_acc: 0.8615\n",
      "Epoch 224/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3196 - acc: 0.8625 - val_loss: 0.3495 - val_acc: 0.8515\n",
      "Epoch 225/250\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3198 - acc: 0.8657 - val_loss: 0.3529 - val_acc: 0.8570\n",
      "Epoch 226/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3181 - acc: 0.8685 - val_loss: 0.3503 - val_acc: 0.8550\n",
      "Epoch 227/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3215 - acc: 0.8671 - val_loss: 0.3478 - val_acc: 0.8605\n",
      "Epoch 228/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3220 - acc: 0.8656 - val_loss: 0.3469 - val_acc: 0.8590\n",
      "Epoch 229/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3180 - acc: 0.8683 - val_loss: 0.3484 - val_acc: 0.8615\n",
      "Epoch 230/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3189 - acc: 0.8656 - val_loss: 0.3479 - val_acc: 0.8620\n",
      "Epoch 231/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3166 - acc: 0.8676 - val_loss: 0.3516 - val_acc: 0.8605\n",
      "Epoch 232/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3190 - acc: 0.8708 - val_loss: 0.3577 - val_acc: 0.8515\n",
      "Epoch 233/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3190 - acc: 0.8643 - val_loss: 0.3616 - val_acc: 0.8530\n",
      "Epoch 234/250\n",
      "7200/7200 [==============================] - 0s 23us/step - loss: 0.3193 - acc: 0.8651 - val_loss: 0.3573 - val_acc: 0.8600\n",
      "Epoch 235/250\n",
      "7200/7200 [==============================] - 0s 20us/step - loss: 0.3149 - acc: 0.8671 - val_loss: 0.3521 - val_acc: 0.8620\n",
      "Epoch 236/250\n",
      "7200/7200 [==============================] - 0s 19us/step - loss: 0.3157 - acc: 0.8678 - val_loss: 0.3452 - val_acc: 0.8630\n",
      "Epoch 237/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3149 - acc: 0.8710 - val_loss: 0.3494 - val_acc: 0.8625\n",
      "Epoch 238/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3164 - acc: 0.8681 - val_loss: 0.3495 - val_acc: 0.8595\n",
      "Epoch 239/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3154 - acc: 0.8683 - val_loss: 0.3439 - val_acc: 0.8630\n",
      "Epoch 240/250\n",
      "7200/7200 [==============================] - 0s 15us/step - loss: 0.3168 - acc: 0.8668 - val_loss: 0.3466 - val_acc: 0.8635\n",
      "Epoch 241/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3146 - acc: 0.8683 - val_loss: 0.3474 - val_acc: 0.8605\n",
      "Epoch 242/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3196 - acc: 0.8650 - val_loss: 0.3458 - val_acc: 0.8615\n",
      "Epoch 243/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3158 - acc: 0.8683 - val_loss: 0.3488 - val_acc: 0.8620\n",
      "Epoch 244/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3165 - acc: 0.8638 - val_loss: 0.3637 - val_acc: 0.8545\n",
      "Epoch 245/250\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3147 - acc: 0.8689 - val_loss: 0.3472 - val_acc: 0.8580\n",
      "Epoch 246/250\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3138 - acc: 0.8682 - val_loss: 0.3452 - val_acc: 0.8585\n",
      "Epoch 247/250\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3153 - acc: 0.8699 - val_loss: 0.3457 - val_acc: 0.8625\n",
      "Epoch 248/250\n",
      "7200/7200 [==============================] - 0s 13us/step - loss: 0.3145 - acc: 0.8686 - val_loss: 0.3499 - val_acc: 0.8590\n",
      "Epoch 249/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3159 - acc: 0.8678 - val_loss: 0.3449 - val_acc: 0.8630\n",
      "Epoch 250/250\n",
      "7200/7200 [==============================] - 0s 14us/step - loss: 0.3133 - acc: 0.8689 - val_loss: 0.3466 - val_acc: 0.8630\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "epochs = 250\n",
    "model_hist = model.fit(trainData, trainTarget, epochs=250, batch_size=128, validation_data=(testData, testTarget))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will plot the loss once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_acc = model_hist.history['acc']\n",
    "model_loss = model_hist.history['loss']\n",
    "model_val_acc = model_hist.history['val_acc']\n",
    "model_val_loss = model_hist.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(model_acc) + 1)\n",
    "# b+ is for \"blue cross\"\n",
    "plt.plot(epochs, model_val_loss, 'b+', label='Val Loss')\n",
    "plt.plot(epochs, model_loss, 'b', label='Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks a little better. Now to evaluate with the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 27us/step\n",
      "Test Loss:  0.3465575888156891\n",
      "Test Accuracy:  0.863\n",
      "Number of exits in test data percentage:  19.75\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(testData, testTarget)\n",
    "print(\"Test Loss: \", test_loss)\n",
    "print(\"Test Accuracy: \", test_acc)\n",
    "\n",
    "numExits = testTarget.count(1)\n",
    "exitPer = numExits/len(testTarget)\n",
    "print(\"Number of exits in test data percentage: \", exitPer*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decent results. The 20% of the clients left the bank in the test data. We have predicted over 85% accurately, meaning that it was more accurate than a machine that always outputs 0 (client did not exit). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
