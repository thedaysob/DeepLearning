{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My very first project with supervised deep learning. I have been reading \"Deep Learning with Python\" by fran√ßois chollet. I am not finished with it, but I really wanted to create my own solution on a given data set. \n",
    "\n",
    "The dataset I will be using is https://www.kaggle.com/sonalidasgupta95/churn-prediction-of-bank-customers/downloads/churn-prediction-of-bank-customers.zip/1\n",
    "It will be a binary classification problem to determine if a customer will exit the bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('Churn_Modelling.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just looking at the data, I will parseout columns 0, 1, 2: Row Number, CustomerId, Surname as these features should have no impact on the prediction of someone exiting the bank. Thus, I am going to use CreditScore, Geography, Gender, Age, Tenure, Balance, NumOfProducts, HasCrCard, IsActiveMember, and EstimatedSalary to determine whether the customer will exit. Also, I will preprocess the data that are inconvienient for the network\n",
    "    - Geography: France will be 0, Spain will be 1, and Germany will be 2\n",
    "    - Gender: Female will be 0, Male will be 1\n",
    "    - Balance: Get the mean then divide by the standard deviation\n",
    "    - EstimatedSalary: Get the mean then divide by the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'Exited']\n",
      "[6.1900000e+02 0.0000000e+00 0.0000000e+00 4.2000000e+01 2.0000000e+00\n",
      " 0.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0134888e+05\n",
      " 1.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# Getting Data\n",
    "with open('Churn_Modelling.csv', 'r') as f:\n",
    "    data = list(csv.reader(f, delimiter=';'))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data[i] = data[i][0].split(',')\n",
    "    \n",
    "    # Switching gender to numeric\n",
    "    if data[i][5] == \"Female\":\n",
    "        data[i][5] = 0\n",
    "    elif data[i][5] == \"Male\":\n",
    "        data[i][5] = 1\n",
    "    \n",
    "    # Switching geography to numeric\n",
    "    if data[i][4] == \"France\":\n",
    "        data[i][4] = 0\n",
    "    elif data[i][4] == \"Spain\":\n",
    "        data[i][4] = 1\n",
    "    elif data[i][4] == \"Germany\":\n",
    "        data[i][4] = 2\n",
    "    \n",
    "    data[i].pop(2)\n",
    "    data[i].pop(1)\n",
    "    data[i].pop(0)\n",
    "    \n",
    "    if i != 0:\n",
    "        for j in range(len(data[i])):\n",
    "            data[i][j] = float(data[i][j])\n",
    "            \n",
    "print(data[0])\n",
    "data.pop(0)\n",
    "data = np.array(data, np.float64)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above shows you an example of data formatted in which order. With the data dissected, I am going to cut the data into a training data and testing data. 75% of the data will be used for training and the rest for testing. I will also create a target array for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Splitting the data into train and test\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# 75% of the data will be training data\n",
    "numTrain = math.floor(len(data) * .75)\n",
    "\n",
    "trainData = data[:numTrain]\n",
    "testData = data[numTrain:]\n",
    "\n",
    "trainTarget = []\n",
    "testTarget = []\n",
    "\n",
    "for i in range(len(trainData)):\n",
    "    trainTarget.append(trainData[i][10])\n",
    "for i in range(len(testData)):\n",
    "    testTarget.append(testData[i][10])\n",
    "    \n",
    "trainData = np.delete(trainData, 10, 1)\n",
    "testData = np.delete(testData, 10, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having split up the data, it is ready for it's final data preprocessig. Data features: CreditScore, Balance, and Salary are normalized. By subtracting the feature's mean off it's own and dividing it by the standard deviation, I can make the range of the data smaller. This is so that the network does not encounter wide extreme numbers on input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "credit = []\n",
    "salary = []\n",
    "balance = []\n",
    "\n",
    "for i in range(len(trainData)):\n",
    "    credit.append(trainData[i][0])\n",
    "    balance.append(trainData[i][5])\n",
    "    salary.append(trainData[i][9])\n",
    "\n",
    "creditMean = np.mean(credit)\n",
    "salaryMean = np.mean(salary)\n",
    "balanceMean = np.mean(balance)\n",
    "creditStd = np.std(credit)\n",
    "salaryStd = np.std(salary)\n",
    "balanceStd = np.std(balance)\n",
    "\n",
    "trainData[:, 0] -= creditMean\n",
    "trainData[:, 0] /= creditStd\n",
    "trainData[:, 5] -= balanceMean\n",
    "trainData[:, 5] /= balanceStd\n",
    "trainData[:, 9] -= salaryMean\n",
    "trainData[:, 9] /= salaryStd\n",
    "\n",
    "print(trainData[:5, 0])\n",
    "print(trainData[:5, 5])\n",
    "print(trainData[:5, 9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above shows how the range of the features have drastically decreased. Credit score, salary, and balance are now all normalized and ready to be inputted into our model. \n",
    "The network model has 4 layers:\n",
    "    Input Layer : size of 10\n",
    "    Hidden Layer 1 : size of 64\n",
    "    Hidden LAyer 2 : size of 32\n",
    "    Output LAyer : size of 1\n",
    "    \n",
    "As for the compilers, I am testing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0728 20:23:53.516337  6680 deprecation_wrapper.py:119] From C:\\Users\\theda\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "W0728 20:23:53.517882  6680 deprecation_wrapper.py:119] From C:\\Users\\theda\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0728 20:23:53.565684  6680 deprecation_wrapper.py:119] From C:\\Users\\theda\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0728 20:23:53.568830  6680 deprecation_wrapper.py:119] From C:\\Users\\theda\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0728 20:23:53.574070  6680 deprecation_wrapper.py:119] From C:\\Users\\theda\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0728 20:23:53.654971  6680 deprecation_wrapper.py:119] From C:\\Users\\theda\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0728 20:23:53.705660  6680 deprecation.py:323] From C:\\Users\\theda\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/20\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 12.7135 - acc: 0.2025 - val_loss: 12.6391 - val_acc: 0.2072\n",
      "Epoch 2/20\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 12.7135 - acc: 0.2025 - val_loss: 12.6391 - val_acc: 0.2072\n",
      "Epoch 3/20\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 12.7135 - acc: 0.2025 - val_loss: 12.6391 - val_acc: 0.2072\n",
      "Epoch 4/20\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 12.7135 - acc: 0.2025 - val_loss: 12.6391 - val_acc: 0.2072\n",
      "Epoch 5/20\n",
      "7500/7500 [==============================] - 0s 11us/step - loss: 12.7135 - acc: 0.2025 - val_loss: 12.6391 - val_acc: 0.2072\n",
      "Epoch 6/20\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 12.7135 - acc: 0.2025 - val_loss: 12.6391 - val_acc: 0.2072\n",
      "Epoch 7/20\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 12.7135 - acc: 0.2025 - val_loss: 12.6391 - val_acc: 0.2072\n",
      "Epoch 8/20\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 12.7135 - acc: 0.2025 - val_loss: 12.6391 - val_acc: 0.2072\n",
      "Epoch 9/20\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 12.7135 - acc: 0.2025 - val_loss: 12.6391 - val_acc: 0.2072\n",
      "Epoch 10/20\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 12.7135 - acc: 0.2025 - val_loss: 12.6391 - val_acc: 0.2072\n",
      "Epoch 11/20\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 12.7135 - acc: 0.2025 - val_loss: 12.6391 - val_acc: 0.2072\n",
      "Epoch 12/20\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 12.7135 - acc: 0.2025 - val_loss: 12.6391 - val_acc: 0.2072\n",
      "Epoch 13/20\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 12.7135 - acc: 0.2025 - val_loss: 12.6391 - val_acc: 0.2072\n",
      "Epoch 14/20\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 12.7135 - acc: 0.2025 - val_loss: 12.6391 - val_acc: 0.2072\n",
      "Epoch 15/20\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 12.7135 - acc: 0.2025 - val_loss: 12.6391 - val_acc: 0.2072\n",
      "Epoch 16/20\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 12.7135 - acc: 0.2025 - val_loss: 12.6391 - val_acc: 0.2072\n",
      "Epoch 17/20\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 12.7135 - acc: 0.2025 - val_loss: 12.6391 - val_acc: 0.2072\n",
      "Epoch 18/20\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 12.7135 - acc: 0.2025 - val_loss: 12.6391 - val_acc: 0.2072\n",
      "Epoch 19/20\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 12.7135 - acc: 0.2025 - val_loss: 12.6391 - val_acc: 0.2072\n",
      "Epoch 20/20\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 12.7135 - acc: 0.2025 - val_loss: 12.6391 - val_acc: 0.2072\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "epochs = 20\n",
    "model_hist = model.fit(trainData, trainTarget, epochs=20, batch_size=128, validation_data=(testData, testTarget))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_acc = model_hist.history['acc']\n",
    "model_loss = model_hist.history['loss']\n",
    "model_val_acc = model_hist.history['val_acc']\n",
    "model_val_loss = model_hist.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(model_acc) + 1)\n",
    "# b+ is for \"blue cross\"\n",
    "plt.plot(epochs, model_val_loss, 'b+', label='Val Loss')\n",
    "plt.plot(epochs, model_loss, 'b', label='Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "trainTarget.count(1)/len(trainTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
