{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breast Cancer Detector\n",
    "8/11/2019\n",
    "Daeseob Lim and Andy Kaplan\n",
    "\n",
    "Dataset: https://www.kaggle.com/merishnasuwal/breast-cancer-prediction-dataset#Breast_cancer_data.csv\n",
    "\n",
    "Introduction:\n",
    "We plan to train a neural network model that predicts breast cancer on patients given measurements of their breast features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_radius</th>\n",
       "      <th>mean_texture</th>\n",
       "      <th>mean_perimeter</th>\n",
       "      <th>mean_area</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_radius  mean_texture  mean_perimeter  mean_area  mean_smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   diagnosis  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('Breast_cancer_data.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the example of the data set above shows, we will be working with breast feature measurements.\n",
    "We will split the data 50% training, 25% validation, and 25% testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Getting Data\n",
    "with open('Breast_cancer_data.csv', 'r') as f:\n",
    "    data = list(csv.reader(f, delimiter=';'))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data[i] = data[i][0].split(',')\n",
    "\n",
    "data.pop(0)\n",
    "data = np.array(data, np.float64)\n",
    "\n",
    "n = math.floor(len(data) * .5)\n",
    "m = math.floor(len(data) * .25)\n",
    "\n",
    "np.random.shuffle(data)\n",
    "trainData = data[:n,:5]\n",
    "trainTarget = data[:n, 5]\n",
    "valData = data[n:n+m, :5]\n",
    "valTarget = data[n:n+m, 5]\n",
    "testData = data[n+m:, :5]\n",
    "testTarget = data[n+m:, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let's check the size of each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data and target length:  284\n",
      "Validation data and target length:  142\n",
      "Testing data and target length:  143\n",
      "Total length of data:  569\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data and target length: \", len(trainData))\n",
    "print(\"Validation data and target length: \", len(valData))\n",
    "print(\"Testing data and target length: \", len(testData))\n",
    "print(\"Total length of data: \", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is now split up into train, validation, and test.\n",
    "Before we move on to constructing the model, some of the data must be normalized to be in the range of [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = []\n",
    "texture = []\n",
    "perimeter = []\n",
    "area = []\n",
    "\n",
    "for i in range(len(trainData)):\n",
    "    radius.append(trainData[i][0])\n",
    "    texture.append(trainData[i][1])\n",
    "    perimeter.append(trainData[i][2])\n",
    "    area.append(trainData[i][3])\n",
    "\n",
    "radiusMean = np.mean(radius)\n",
    "textureMean = np.mean(texture)\n",
    "perimeterMean = np.mean(perimeter)\n",
    "areaMean = np.mean(area)\n",
    "radiusStd = np.std(radius)\n",
    "textureStd = np.std(texture)\n",
    "perimeterStd = np.std(perimeter)\n",
    "areaStd = np.std(area)\n",
    "\n",
    "trainData[:, 0] -= radiusMean\n",
    "trainData[:, 0] /= radiusStd\n",
    "trainData[:, 1] -= textureMean\n",
    "trainData[:, 1] /= textureStd\n",
    "trainData[:, 2] -= perimeterMean\n",
    "trainData[:, 2] /= perimeterStd\n",
    "trainData[:, 3] -= areaMean\n",
    "trainData[:, 3] /= areaStd\n",
    "\n",
    "valData[:, 0] -= radiusMean\n",
    "valData[:, 0] /= radiusStd\n",
    "valData[:, 1] -= textureMean\n",
    "valData[:, 1] /= textureStd\n",
    "valData[:, 2] -= perimeterMean\n",
    "valData[:, 2] /= perimeterStd\n",
    "valData[:, 3] -= areaMean\n",
    "valData[:, 3] /= areaStd\n",
    "\n",
    "testData[:, 0] -= radiusMean\n",
    "testData[:, 0] /= radiusStd\n",
    "testData[:, 1] -= textureMean\n",
    "testData[:, 1] /= textureStd\n",
    "testData[:, 2] -= perimeterMean\n",
    "testData[:, 2] /= perimeterStd\n",
    "testData[:, 3] -= areaMean\n",
    "testData[:, 3] /= areaStd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we will do a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.78788353 0.12083495 0.68747722 0.65318985 0.07445   ]\n"
     ]
    }
   ],
   "source": [
    "print(trainData[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The data is now normalized and the values are in a respectable range from each other.\n",
    "Now to create a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0811 13:08:35.964575 19556 deprecation_wrapper.py:119] From C:\\Users\\theda\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "W0811 13:08:35.966574 19556 deprecation_wrapper.py:119] From C:\\Users\\theda\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0811 13:08:36.088013 19556 deprecation_wrapper.py:119] From C:\\Users\\theda\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0811 13:08:36.092993 19556 deprecation_wrapper.py:119] From C:\\Users\\theda\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0811 13:08:36.100971 19556 deprecation_wrapper.py:119] From C:\\Users\\theda\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0811 13:08:36.195421 19556 deprecation_wrapper.py:119] From C:\\Users\\theda\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0811 13:08:36.238178 19556 deprecation.py:323] From C:\\Users\\theda\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 284 samples, validate on 142 samples\n",
      "Epoch 1/300\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.7374 - acc: 0.2887 - val_loss: 0.6806 - val_acc: 0.4930\n",
      "Epoch 2/300\n",
      "284/284 [==============================] - 0s 33us/step - loss: 0.6724 - acc: 0.6232 - val_loss: 0.6167 - val_acc: 0.8592\n",
      "Epoch 3/300\n",
      "284/284 [==============================] - 0s 54us/step - loss: 0.6113 - acc: 0.8592 - val_loss: 0.5606 - val_acc: 0.8944\n",
      "Epoch 4/300\n",
      "284/284 [==============================] - 0s 63us/step - loss: 0.5589 - acc: 0.8697 - val_loss: 0.5119 - val_acc: 0.9155\n",
      "Epoch 5/300\n",
      "284/284 [==============================] - 0s 41us/step - loss: 0.5140 - acc: 0.8732 - val_loss: 0.4700 - val_acc: 0.9085\n",
      "Epoch 6/300\n",
      "284/284 [==============================] - 0s 51us/step - loss: 0.4729 - acc: 0.8732 - val_loss: 0.4343 - val_acc: 0.9085\n",
      "Epoch 7/300\n",
      "284/284 [==============================] - 0s 33us/step - loss: 0.4381 - acc: 0.8732 - val_loss: 0.4036 - val_acc: 0.9085\n",
      "Epoch 8/300\n",
      "284/284 [==============================] - 0s 42us/step - loss: 0.4087 - acc: 0.8768 - val_loss: 0.3768 - val_acc: 0.9085\n",
      "Epoch 9/300\n",
      "284/284 [==============================] - 0s 33us/step - loss: 0.3823 - acc: 0.8768 - val_loss: 0.3538 - val_acc: 0.9085\n",
      "Epoch 10/300\n",
      "284/284 [==============================] - 0s 46us/step - loss: 0.3597 - acc: 0.8768 - val_loss: 0.3342 - val_acc: 0.9085\n",
      "Epoch 11/300\n",
      "284/284 [==============================] - 0s 31us/step - loss: 0.3396 - acc: 0.8768 - val_loss: 0.3179 - val_acc: 0.9085\n",
      "Epoch 12/300\n",
      "284/284 [==============================] - 0s 41us/step - loss: 0.3220 - acc: 0.8768 - val_loss: 0.3041 - val_acc: 0.9085\n",
      "Epoch 13/300\n",
      "284/284 [==============================] - 0s 57us/step - loss: 0.3072 - acc: 0.8768 - val_loss: 0.2927 - val_acc: 0.9085\n",
      "Epoch 14/300\n",
      "284/284 [==============================] - 0s 38us/step - loss: 0.2937 - acc: 0.8768 - val_loss: 0.2839 - val_acc: 0.9085\n",
      "Epoch 15/300\n",
      "284/284 [==============================] - 0s 31us/step - loss: 0.2838 - acc: 0.8768 - val_loss: 0.2773 - val_acc: 0.9085\n",
      "Epoch 16/300\n",
      "284/284 [==============================] - 0s 52us/step - loss: 0.2748 - acc: 0.8768 - val_loss: 0.2728 - val_acc: 0.9085\n",
      "Epoch 17/300\n",
      "284/284 [==============================] - 0s 31us/step - loss: 0.2682 - acc: 0.8768 - val_loss: 0.2697 - val_acc: 0.8944\n",
      "Epoch 18/300\n",
      "284/284 [==============================] - 0s 44us/step - loss: 0.2623 - acc: 0.8803 - val_loss: 0.2673 - val_acc: 0.8944\n",
      "Epoch 19/300\n",
      "284/284 [==============================] - 0s 32us/step - loss: 0.2585 - acc: 0.8803 - val_loss: 0.2652 - val_acc: 0.8944\n",
      "Epoch 20/300\n",
      "284/284 [==============================] - 0s 47us/step - loss: 0.2554 - acc: 0.8803 - val_loss: 0.2636 - val_acc: 0.9085\n",
      "Epoch 21/300\n",
      "284/284 [==============================] - 0s 36us/step - loss: 0.2531 - acc: 0.8803 - val_loss: 0.2630 - val_acc: 0.9014\n",
      "Epoch 22/300\n",
      "284/284 [==============================] - 0s 30us/step - loss: 0.2513 - acc: 0.8803 - val_loss: 0.2616 - val_acc: 0.9085\n",
      "Epoch 23/300\n",
      "284/284 [==============================] - 0s 35us/step - loss: 0.2499 - acc: 0.8803 - val_loss: 0.2602 - val_acc: 0.9085\n",
      "Epoch 24/300\n",
      "284/284 [==============================] - 0s 51us/step - loss: 0.2488 - acc: 0.8803 - val_loss: 0.2592 - val_acc: 0.9085\n",
      "Epoch 25/300\n",
      "284/284 [==============================] - 0s 33us/step - loss: 0.2480 - acc: 0.8803 - val_loss: 0.2585 - val_acc: 0.9085\n",
      "Epoch 26/300\n",
      "284/284 [==============================] - 0s 40us/step - loss: 0.2471 - acc: 0.8803 - val_loss: 0.2580 - val_acc: 0.9085\n",
      "Epoch 27/300\n",
      "284/284 [==============================] - 0s 32us/step - loss: 0.2461 - acc: 0.8803 - val_loss: 0.2579 - val_acc: 0.9085\n",
      "Epoch 28/300\n",
      "284/284 [==============================] - 0s 40us/step - loss: 0.2453 - acc: 0.8803 - val_loss: 0.2578 - val_acc: 0.9085\n",
      "Epoch 29/300\n",
      "284/284 [==============================] - 0s 31us/step - loss: 0.2444 - acc: 0.8838 - val_loss: 0.2579 - val_acc: 0.9085\n",
      "Epoch 30/300\n",
      "284/284 [==============================] - 0s 39us/step - loss: 0.2437 - acc: 0.8838 - val_loss: 0.2569 - val_acc: 0.9085\n",
      "Epoch 31/300\n",
      "284/284 [==============================] - 0s 45us/step - loss: 0.2428 - acc: 0.8838 - val_loss: 0.2555 - val_acc: 0.9085\n",
      "Epoch 32/300\n",
      "284/284 [==============================] - 0s 48us/step - loss: 0.2420 - acc: 0.8873 - val_loss: 0.2541 - val_acc: 0.9085\n",
      "Epoch 33/300\n",
      "284/284 [==============================] - 0s 48us/step - loss: 0.2415 - acc: 0.8838 - val_loss: 0.2528 - val_acc: 0.9085\n",
      "Epoch 34/300\n",
      "284/284 [==============================] - 0s 42us/step - loss: 0.2410 - acc: 0.8838 - val_loss: 0.2522 - val_acc: 0.9085\n",
      "Epoch 35/300\n",
      "284/284 [==============================] - 0s 33us/step - loss: 0.2407 - acc: 0.8838 - val_loss: 0.2517 - val_acc: 0.9085\n",
      "Epoch 36/300\n",
      "284/284 [==============================] - 0s 32us/step - loss: 0.2403 - acc: 0.8838 - val_loss: 0.2514 - val_acc: 0.9085\n",
      "Epoch 37/300\n",
      "284/284 [==============================] - 0s 38us/step - loss: 0.2397 - acc: 0.8838 - val_loss: 0.2508 - val_acc: 0.9085\n",
      "Epoch 38/300\n",
      "284/284 [==============================] - 0s 33us/step - loss: 0.2393 - acc: 0.8838 - val_loss: 0.2507 - val_acc: 0.9085\n",
      "Epoch 39/300\n",
      "284/284 [==============================] - 0s 30us/step - loss: 0.2388 - acc: 0.8838 - val_loss: 0.2504 - val_acc: 0.9085\n",
      "Epoch 40/300\n",
      "284/284 [==============================] - 0s 49us/step - loss: 0.2384 - acc: 0.8838 - val_loss: 0.2496 - val_acc: 0.9085\n",
      "Epoch 41/300\n",
      "284/284 [==============================] - 0s 45us/step - loss: 0.2380 - acc: 0.8838 - val_loss: 0.2481 - val_acc: 0.9085\n",
      "Epoch 42/300\n",
      "284/284 [==============================] - 0s 46us/step - loss: 0.2378 - acc: 0.8838 - val_loss: 0.2473 - val_acc: 0.9085\n",
      "Epoch 43/300\n",
      "284/284 [==============================] - 0s 38us/step - loss: 0.2376 - acc: 0.8838 - val_loss: 0.2470 - val_acc: 0.9085\n",
      "Epoch 44/300\n",
      "284/284 [==============================] - 0s 43us/step - loss: 0.2369 - acc: 0.8838 - val_loss: 0.2474 - val_acc: 0.9085\n",
      "Epoch 45/300\n",
      "284/284 [==============================] - 0s 30us/step - loss: 0.2363 - acc: 0.8838 - val_loss: 0.2485 - val_acc: 0.9085\n",
      "Epoch 46/300\n",
      "284/284 [==============================] - 0s 30us/step - loss: 0.2356 - acc: 0.8873 - val_loss: 0.2493 - val_acc: 0.9085\n",
      "Epoch 47/300\n",
      "284/284 [==============================] - 0s 37us/step - loss: 0.2351 - acc: 0.8873 - val_loss: 0.2497 - val_acc: 0.9085\n",
      "Epoch 48/300\n",
      "284/284 [==============================] - 0s 29us/step - loss: 0.2346 - acc: 0.8908 - val_loss: 0.2509 - val_acc: 0.9085\n",
      "Epoch 49/300\n",
      "284/284 [==============================] - 0s 29us/step - loss: 0.2341 - acc: 0.8873 - val_loss: 0.2515 - val_acc: 0.9085\n",
      "Epoch 50/300\n",
      "284/284 [==============================] - 0s 39us/step - loss: 0.2334 - acc: 0.8873 - val_loss: 0.2526 - val_acc: 0.9085\n",
      "Epoch 51/300\n",
      "284/284 [==============================] - 0s 45us/step - loss: 0.2334 - acc: 0.8838 - val_loss: 0.2535 - val_acc: 0.9014\n",
      "Epoch 52/300\n",
      "284/284 [==============================] - 0s 35us/step - loss: 0.2330 - acc: 0.8838 - val_loss: 0.2535 - val_acc: 0.8944\n",
      "Epoch 53/300\n",
      "284/284 [==============================] - 0s 31us/step - loss: 0.2327 - acc: 0.8838 - val_loss: 0.2533 - val_acc: 0.8944\n",
      "Epoch 54/300\n",
      "284/284 [==============================] - 0s 29us/step - loss: 0.2322 - acc: 0.8838 - val_loss: 0.2526 - val_acc: 0.8944\n",
      "Epoch 55/300\n",
      "284/284 [==============================] - 0s 40us/step - loss: 0.2315 - acc: 0.8838 - val_loss: 0.2516 - val_acc: 0.9014\n",
      "Epoch 56/300\n",
      "284/284 [==============================] - 0s 45us/step - loss: 0.2311 - acc: 0.8838 - val_loss: 0.2508 - val_acc: 0.9014\n",
      "Epoch 57/300\n",
      "284/284 [==============================] - 0s 44us/step - loss: 0.2304 - acc: 0.8838 - val_loss: 0.2508 - val_acc: 0.9014\n",
      "Epoch 58/300\n",
      "284/284 [==============================] - 0s 24us/step - loss: 0.2299 - acc: 0.8838 - val_loss: 0.2499 - val_acc: 0.9014\n",
      "Epoch 59/300\n",
      "284/284 [==============================] - 0s 46us/step - loss: 0.2293 - acc: 0.8838 - val_loss: 0.2504 - val_acc: 0.9014\n",
      "Epoch 60/300\n",
      "284/284 [==============================] - 0s 42us/step - loss: 0.2289 - acc: 0.8838 - val_loss: 0.2506 - val_acc: 0.9014\n",
      "Epoch 61/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284/284 [==============================] - 0s 39us/step - loss: 0.2286 - acc: 0.8838 - val_loss: 0.2506 - val_acc: 0.9014\n",
      "Epoch 62/300\n",
      "284/284 [==============================] - 0s 27us/step - loss: 0.2282 - acc: 0.8838 - val_loss: 0.2505 - val_acc: 0.9014\n",
      "Epoch 63/300\n",
      "284/284 [==============================] - 0s 32us/step - loss: 0.2277 - acc: 0.8873 - val_loss: 0.2498 - val_acc: 0.9085\n",
      "Epoch 64/300\n",
      "284/284 [==============================] - 0s 33us/step - loss: 0.2273 - acc: 0.8838 - val_loss: 0.2484 - val_acc: 0.9085\n",
      "Epoch 65/300\n",
      "284/284 [==============================] - 0s 33us/step - loss: 0.2266 - acc: 0.8838 - val_loss: 0.2467 - val_acc: 0.9085\n",
      "Epoch 66/300\n",
      "284/284 [==============================] - 0s 36us/step - loss: 0.2264 - acc: 0.8873 - val_loss: 0.2448 - val_acc: 0.9085\n",
      "Epoch 67/300\n",
      "284/284 [==============================] - 0s 35us/step - loss: 0.2257 - acc: 0.8908 - val_loss: 0.2436 - val_acc: 0.9085\n",
      "Epoch 68/300\n",
      "284/284 [==============================] - 0s 30us/step - loss: 0.2252 - acc: 0.8908 - val_loss: 0.2428 - val_acc: 0.9085\n",
      "Epoch 69/300\n",
      "284/284 [==============================] - 0s 28us/step - loss: 0.2248 - acc: 0.8908 - val_loss: 0.2420 - val_acc: 0.9085\n",
      "Epoch 70/300\n",
      "284/284 [==============================] - 0s 28us/step - loss: 0.2243 - acc: 0.8908 - val_loss: 0.2414 - val_acc: 0.9085\n",
      "Epoch 71/300\n",
      "284/284 [==============================] - 0s 32us/step - loss: 0.2240 - acc: 0.8908 - val_loss: 0.2415 - val_acc: 0.9085\n",
      "Epoch 72/300\n",
      "284/284 [==============================] - 0s 41us/step - loss: 0.2233 - acc: 0.8908 - val_loss: 0.2418 - val_acc: 0.9085\n",
      "Epoch 73/300\n",
      "284/284 [==============================] - 0s 39us/step - loss: 0.2228 - acc: 0.8908 - val_loss: 0.2422 - val_acc: 0.9085\n",
      "Epoch 74/300\n",
      "284/284 [==============================] - 0s 33us/step - loss: 0.2223 - acc: 0.8873 - val_loss: 0.2436 - val_acc: 0.9085\n",
      "Epoch 75/300\n",
      "284/284 [==============================] - 0s 62us/step - loss: 0.2221 - acc: 0.8873 - val_loss: 0.2454 - val_acc: 0.9085\n",
      "Epoch 76/300\n",
      "284/284 [==============================] - 0s 28us/step - loss: 0.2217 - acc: 0.8908 - val_loss: 0.2464 - val_acc: 0.9014\n",
      "Epoch 77/300\n",
      "284/284 [==============================] - 0s 35us/step - loss: 0.2216 - acc: 0.8908 - val_loss: 0.2472 - val_acc: 0.9014\n",
      "Epoch 78/300\n",
      "284/284 [==============================] - 0s 40us/step - loss: 0.2214 - acc: 0.8908 - val_loss: 0.2471 - val_acc: 0.9085\n",
      "Epoch 79/300\n",
      "284/284 [==============================] - 0s 42us/step - loss: 0.2208 - acc: 0.8908 - val_loss: 0.2455 - val_acc: 0.9085\n",
      "Epoch 80/300\n",
      "284/284 [==============================] - 0s 43us/step - loss: 0.2199 - acc: 0.8908 - val_loss: 0.2438 - val_acc: 0.9085\n",
      "Epoch 81/300\n",
      "284/284 [==============================] - 0s 50us/step - loss: 0.2199 - acc: 0.8908 - val_loss: 0.2422 - val_acc: 0.9085\n",
      "Epoch 82/300\n",
      "284/284 [==============================] - 0s 41us/step - loss: 0.2193 - acc: 0.8944 - val_loss: 0.2406 - val_acc: 0.9085\n",
      "Epoch 83/300\n",
      "284/284 [==============================] - 0s 47us/step - loss: 0.2194 - acc: 0.8944 - val_loss: 0.2392 - val_acc: 0.9085\n",
      "Epoch 84/300\n",
      "284/284 [==============================] - 0s 46us/step - loss: 0.2190 - acc: 0.8944 - val_loss: 0.2386 - val_acc: 0.9085\n",
      "Epoch 85/300\n",
      "284/284 [==============================] - 0s 37us/step - loss: 0.2185 - acc: 0.8979 - val_loss: 0.2387 - val_acc: 0.9085\n",
      "Epoch 86/300\n",
      "284/284 [==============================] - 0s 35us/step - loss: 0.2180 - acc: 0.8979 - val_loss: 0.2399 - val_acc: 0.9085\n",
      "Epoch 87/300\n",
      "284/284 [==============================] - 0s 21us/step - loss: 0.2173 - acc: 0.8944 - val_loss: 0.2409 - val_acc: 0.9085\n",
      "Epoch 88/300\n",
      "284/284 [==============================] - 0s 33us/step - loss: 0.2169 - acc: 0.8944 - val_loss: 0.2416 - val_acc: 0.9085\n",
      "Epoch 89/300\n",
      "284/284 [==============================] - 0s 43us/step - loss: 0.2162 - acc: 0.8944 - val_loss: 0.2428 - val_acc: 0.9085\n",
      "Epoch 90/300\n",
      "284/284 [==============================] - 0s 46us/step - loss: 0.2160 - acc: 0.8944 - val_loss: 0.2448 - val_acc: 0.9155\n",
      "Epoch 91/300\n",
      "284/284 [==============================] - 0s 33us/step - loss: 0.2157 - acc: 0.8944 - val_loss: 0.2468 - val_acc: 0.9155\n",
      "Epoch 92/300\n",
      "284/284 [==============================] - 0s 31us/step - loss: 0.2159 - acc: 0.8979 - val_loss: 0.2496 - val_acc: 0.9155\n",
      "Epoch 93/300\n",
      "284/284 [==============================] - 0s 34us/step - loss: 0.2158 - acc: 0.8979 - val_loss: 0.2503 - val_acc: 0.9155\n",
      "Epoch 94/300\n",
      "284/284 [==============================] - 0s 49us/step - loss: 0.2156 - acc: 0.8979 - val_loss: 0.2496 - val_acc: 0.9155\n",
      "Epoch 95/300\n",
      "284/284 [==============================] - 0s 34us/step - loss: 0.2149 - acc: 0.8979 - val_loss: 0.2479 - val_acc: 0.9155\n",
      "Epoch 96/300\n",
      "284/284 [==============================] - 0s 38us/step - loss: 0.2141 - acc: 0.9014 - val_loss: 0.2452 - val_acc: 0.9155\n",
      "Epoch 97/300\n",
      "284/284 [==============================] - 0s 45us/step - loss: 0.2133 - acc: 0.8944 - val_loss: 0.2418 - val_acc: 0.9155\n",
      "Epoch 98/300\n",
      "284/284 [==============================] - 0s 36us/step - loss: 0.2124 - acc: 0.8944 - val_loss: 0.2385 - val_acc: 0.9155\n",
      "Epoch 99/300\n",
      "284/284 [==============================] - 0s 29us/step - loss: 0.2119 - acc: 0.8979 - val_loss: 0.2346 - val_acc: 0.9085\n",
      "Epoch 100/300\n",
      "284/284 [==============================] - 0s 36us/step - loss: 0.2122 - acc: 0.8944 - val_loss: 0.2324 - val_acc: 0.9085\n",
      "Epoch 101/300\n",
      "284/284 [==============================] - 0s 42us/step - loss: 0.2121 - acc: 0.8908 - val_loss: 0.2326 - val_acc: 0.9085\n",
      "Epoch 102/300\n",
      "284/284 [==============================] - 0s 32us/step - loss: 0.2117 - acc: 0.8908 - val_loss: 0.2340 - val_acc: 0.9085\n",
      "Epoch 103/300\n",
      "284/284 [==============================] - 0s 31us/step - loss: 0.2107 - acc: 0.8944 - val_loss: 0.2348 - val_acc: 0.9085\n",
      "Epoch 104/300\n",
      "284/284 [==============================] - 0s 52us/step - loss: 0.2100 - acc: 0.8944 - val_loss: 0.2361 - val_acc: 0.9085\n",
      "Epoch 105/300\n",
      "284/284 [==============================] - 0s 28us/step - loss: 0.2096 - acc: 0.8979 - val_loss: 0.2380 - val_acc: 0.9155\n",
      "Epoch 106/300\n",
      "284/284 [==============================] - 0s 41us/step - loss: 0.2090 - acc: 0.9014 - val_loss: 0.2388 - val_acc: 0.9155\n",
      "Epoch 107/300\n",
      "284/284 [==============================] - 0s 27us/step - loss: 0.2090 - acc: 0.9014 - val_loss: 0.2394 - val_acc: 0.9085\n",
      "Epoch 108/300\n",
      "284/284 [==============================] - 0s 38us/step - loss: 0.2088 - acc: 0.9014 - val_loss: 0.2388 - val_acc: 0.9085\n",
      "Epoch 109/300\n",
      "284/284 [==============================] - 0s 26us/step - loss: 0.2084 - acc: 0.8979 - val_loss: 0.2360 - val_acc: 0.9155\n",
      "Epoch 110/300\n",
      "284/284 [==============================] - 0s 32us/step - loss: 0.2082 - acc: 0.8944 - val_loss: 0.2344 - val_acc: 0.9155\n",
      "Epoch 111/300\n",
      "284/284 [==============================] - 0s 29us/step - loss: 0.2078 - acc: 0.8944 - val_loss: 0.2347 - val_acc: 0.9155\n",
      "Epoch 112/300\n",
      "284/284 [==============================] - 0s 34us/step - loss: 0.2072 - acc: 0.8944 - val_loss: 0.2350 - val_acc: 0.9085\n",
      "Epoch 113/300\n",
      "284/284 [==============================] - 0s 31us/step - loss: 0.2067 - acc: 0.8944 - val_loss: 0.2358 - val_acc: 0.9085\n",
      "Epoch 114/300\n",
      "284/284 [==============================] - 0s 37us/step - loss: 0.2060 - acc: 0.8944 - val_loss: 0.2373 - val_acc: 0.9155\n",
      "Epoch 115/300\n",
      "284/284 [==============================] - 0s 30us/step - loss: 0.2060 - acc: 0.8979 - val_loss: 0.2383 - val_acc: 0.9155\n",
      "Epoch 116/300\n",
      "284/284 [==============================] - 0s 26us/step - loss: 0.2054 - acc: 0.9014 - val_loss: 0.2375 - val_acc: 0.9155\n",
      "Epoch 117/300\n",
      "284/284 [==============================] - 0s 45us/step - loss: 0.2049 - acc: 0.9014 - val_loss: 0.2368 - val_acc: 0.9155\n",
      "Epoch 118/300\n",
      "284/284 [==============================] - 0s 34us/step - loss: 0.2045 - acc: 0.9014 - val_loss: 0.2370 - val_acc: 0.9155\n",
      "Epoch 119/300\n",
      "284/284 [==============================] - 0s 38us/step - loss: 0.2041 - acc: 0.9014 - val_loss: 0.2376 - val_acc: 0.9155\n",
      "Epoch 120/300\n",
      "284/284 [==============================] - 0s 34us/step - loss: 0.2038 - acc: 0.8979 - val_loss: 0.2375 - val_acc: 0.9155\n",
      "Epoch 121/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284/284 [==============================] - 0s 30us/step - loss: 0.2034 - acc: 0.8944 - val_loss: 0.2375 - val_acc: 0.9225\n",
      "Epoch 122/300\n",
      "284/284 [==============================] - 0s 34us/step - loss: 0.2031 - acc: 0.8944 - val_loss: 0.2362 - val_acc: 0.9225\n",
      "Epoch 123/300\n",
      "284/284 [==============================] - 0s 30us/step - loss: 0.2027 - acc: 0.8944 - val_loss: 0.2333 - val_acc: 0.9225\n",
      "Epoch 124/300\n",
      "284/284 [==============================] - 0s 38us/step - loss: 0.2021 - acc: 0.9049 - val_loss: 0.2310 - val_acc: 0.9225\n",
      "Epoch 125/300\n",
      "284/284 [==============================] - 0s 27us/step - loss: 0.2020 - acc: 0.9014 - val_loss: 0.2306 - val_acc: 0.9225\n",
      "Epoch 126/300\n",
      "284/284 [==============================] - 0s 43us/step - loss: 0.2015 - acc: 0.9014 - val_loss: 0.2319 - val_acc: 0.9225\n",
      "Epoch 127/300\n",
      "284/284 [==============================] - 0s 32us/step - loss: 0.2009 - acc: 0.9049 - val_loss: 0.2335 - val_acc: 0.9225\n",
      "Epoch 128/300\n",
      "284/284 [==============================] - 0s 33us/step - loss: 0.2004 - acc: 0.9014 - val_loss: 0.2341 - val_acc: 0.9225\n",
      "Epoch 129/300\n",
      "284/284 [==============================] - 0s 40us/step - loss: 0.2002 - acc: 0.9085 - val_loss: 0.2350 - val_acc: 0.9225\n",
      "Epoch 130/300\n",
      "284/284 [==============================] - 0s 40us/step - loss: 0.1997 - acc: 0.9049 - val_loss: 0.2355 - val_acc: 0.9225\n",
      "Epoch 131/300\n",
      "284/284 [==============================] - 0s 50us/step - loss: 0.1993 - acc: 0.9085 - val_loss: 0.2357 - val_acc: 0.9225\n",
      "Epoch 132/300\n",
      "284/284 [==============================] - 0s 42us/step - loss: 0.1986 - acc: 0.9120 - val_loss: 0.2353 - val_acc: 0.9225\n",
      "Epoch 133/300\n",
      "284/284 [==============================] - 0s 31us/step - loss: 0.1982 - acc: 0.9120 - val_loss: 0.2348 - val_acc: 0.9225\n",
      "Epoch 134/300\n",
      "284/284 [==============================] - 0s 29us/step - loss: 0.1978 - acc: 0.9120 - val_loss: 0.2343 - val_acc: 0.9225\n",
      "Epoch 135/300\n",
      "284/284 [==============================] - 0s 35us/step - loss: 0.1974 - acc: 0.9120 - val_loss: 0.2348 - val_acc: 0.9225\n",
      "Epoch 136/300\n",
      "284/284 [==============================] - 0s 39us/step - loss: 0.1970 - acc: 0.9155 - val_loss: 0.2367 - val_acc: 0.9225\n",
      "Epoch 137/300\n",
      "284/284 [==============================] - 0s 44us/step - loss: 0.1966 - acc: 0.9155 - val_loss: 0.2392 - val_acc: 0.9225\n",
      "Epoch 138/300\n",
      "284/284 [==============================] - 0s 43us/step - loss: 0.1963 - acc: 0.9120 - val_loss: 0.2414 - val_acc: 0.9225\n",
      "Epoch 139/300\n",
      "284/284 [==============================] - 0s 44us/step - loss: 0.1964 - acc: 0.9085 - val_loss: 0.2440 - val_acc: 0.9155\n",
      "Epoch 140/300\n",
      "284/284 [==============================] - 0s 49us/step - loss: 0.1966 - acc: 0.9085 - val_loss: 0.2446 - val_acc: 0.9155\n",
      "Epoch 141/300\n",
      "284/284 [==============================] - 0s 41us/step - loss: 0.1963 - acc: 0.9120 - val_loss: 0.2427 - val_acc: 0.9225\n",
      "Epoch 142/300\n",
      "284/284 [==============================] - 0s 34us/step - loss: 0.1953 - acc: 0.9120 - val_loss: 0.2388 - val_acc: 0.9225\n",
      "Epoch 143/300\n",
      "284/284 [==============================] - 0s 48us/step - loss: 0.1942 - acc: 0.9085 - val_loss: 0.2339 - val_acc: 0.9225\n",
      "Epoch 144/300\n",
      "284/284 [==============================] - 0s 47us/step - loss: 0.1937 - acc: 0.9085 - val_loss: 0.2304 - val_acc: 0.9225\n",
      "Epoch 145/300\n",
      "284/284 [==============================] - 0s 28us/step - loss: 0.1929 - acc: 0.9120 - val_loss: 0.2289 - val_acc: 0.9225\n",
      "Epoch 146/300\n",
      "284/284 [==============================] - 0s 32us/step - loss: 0.1927 - acc: 0.9085 - val_loss: 0.2279 - val_acc: 0.9225\n",
      "Epoch 147/300\n",
      "284/284 [==============================] - 0s 53us/step - loss: 0.1925 - acc: 0.9085 - val_loss: 0.2273 - val_acc: 0.9225\n",
      "Epoch 148/300\n",
      "284/284 [==============================] - 0s 26us/step - loss: 0.1922 - acc: 0.9120 - val_loss: 0.2258 - val_acc: 0.9155\n",
      "Epoch 149/300\n",
      "284/284 [==============================] - 0s 28us/step - loss: 0.1921 - acc: 0.9155 - val_loss: 0.2252 - val_acc: 0.9155\n",
      "Epoch 150/300\n",
      "284/284 [==============================] - 0s 37us/step - loss: 0.1918 - acc: 0.9155 - val_loss: 0.2252 - val_acc: 0.9155\n",
      "Epoch 151/300\n",
      "284/284 [==============================] - 0s 39us/step - loss: 0.1910 - acc: 0.9155 - val_loss: 0.2246 - val_acc: 0.9225\n",
      "Epoch 152/300\n",
      "284/284 [==============================] - 0s 31us/step - loss: 0.1904 - acc: 0.9190 - val_loss: 0.2262 - val_acc: 0.9225\n",
      "Epoch 153/300\n",
      "284/284 [==============================] - 0s 40us/step - loss: 0.1900 - acc: 0.9155 - val_loss: 0.2295 - val_acc: 0.9155\n",
      "Epoch 154/300\n",
      "284/284 [==============================] - 0s 43us/step - loss: 0.1889 - acc: 0.9120 - val_loss: 0.2311 - val_acc: 0.9225\n",
      "Epoch 155/300\n",
      "284/284 [==============================] - 0s 48us/step - loss: 0.1883 - acc: 0.9155 - val_loss: 0.2323 - val_acc: 0.9225\n",
      "Epoch 156/300\n",
      "284/284 [==============================] - 0s 38us/step - loss: 0.1878 - acc: 0.9190 - val_loss: 0.2312 - val_acc: 0.9225\n",
      "Epoch 157/300\n",
      "284/284 [==============================] - 0s 50us/step - loss: 0.1874 - acc: 0.9190 - val_loss: 0.2295 - val_acc: 0.9225\n",
      "Epoch 158/300\n",
      "284/284 [==============================] - 0s 41us/step - loss: 0.1867 - acc: 0.9155 - val_loss: 0.2298 - val_acc: 0.9225\n",
      "Epoch 159/300\n",
      "284/284 [==============================] - 0s 33us/step - loss: 0.1862 - acc: 0.9190 - val_loss: 0.2302 - val_acc: 0.9225\n",
      "Epoch 160/300\n",
      "284/284 [==============================] - 0s 39us/step - loss: 0.1858 - acc: 0.9190 - val_loss: 0.2288 - val_acc: 0.9155\n",
      "Epoch 161/300\n",
      "284/284 [==============================] - 0s 38us/step - loss: 0.1857 - acc: 0.9155 - val_loss: 0.2265 - val_acc: 0.9225\n",
      "Epoch 162/300\n",
      "284/284 [==============================] - 0s 22us/step - loss: 0.1856 - acc: 0.9155 - val_loss: 0.2256 - val_acc: 0.9225\n",
      "Epoch 163/300\n",
      "284/284 [==============================] - 0s 36us/step - loss: 0.1850 - acc: 0.9155 - val_loss: 0.2252 - val_acc: 0.9225\n",
      "Epoch 164/300\n",
      "284/284 [==============================] - 0s 27us/step - loss: 0.1846 - acc: 0.9155 - val_loss: 0.2249 - val_acc: 0.9225\n",
      "Epoch 165/300\n",
      "284/284 [==============================] - 0s 36us/step - loss: 0.1840 - acc: 0.9190 - val_loss: 0.2263 - val_acc: 0.9225\n",
      "Epoch 166/300\n",
      "284/284 [==============================] - 0s 34us/step - loss: 0.1830 - acc: 0.9190 - val_loss: 0.2285 - val_acc: 0.9225\n",
      "Epoch 167/300\n",
      "284/284 [==============================] - 0s 34us/step - loss: 0.1821 - acc: 0.9190 - val_loss: 0.2300 - val_acc: 0.9225\n",
      "Epoch 168/300\n",
      "284/284 [==============================] - 0s 44us/step - loss: 0.1819 - acc: 0.9190 - val_loss: 0.2308 - val_acc: 0.9155\n",
      "Epoch 169/300\n",
      "284/284 [==============================] - 0s 31us/step - loss: 0.1816 - acc: 0.9190 - val_loss: 0.2301 - val_acc: 0.9155\n",
      "Epoch 170/300\n",
      "284/284 [==============================] - ETA: 0s - loss: 0.1865 - acc: 0.937 - 0s 43us/step - loss: 0.1811 - acc: 0.9190 - val_loss: 0.2292 - val_acc: 0.9155\n",
      "Epoch 171/300\n",
      "284/284 [==============================] - 0s 32us/step - loss: 0.1801 - acc: 0.9190 - val_loss: 0.2285 - val_acc: 0.9155\n",
      "Epoch 172/300\n",
      "284/284 [==============================] - 0s 41us/step - loss: 0.1796 - acc: 0.9225 - val_loss: 0.2283 - val_acc: 0.9225\n",
      "Epoch 173/300\n",
      "284/284 [==============================] - 0s 41us/step - loss: 0.1791 - acc: 0.9225 - val_loss: 0.2279 - val_acc: 0.9225\n",
      "Epoch 174/300\n",
      "284/284 [==============================] - 0s 44us/step - loss: 0.1787 - acc: 0.9190 - val_loss: 0.2280 - val_acc: 0.9366\n",
      "Epoch 175/300\n",
      "284/284 [==============================] - 0s 26us/step - loss: 0.1783 - acc: 0.9190 - val_loss: 0.2291 - val_acc: 0.9296\n",
      "Epoch 176/300\n",
      "284/284 [==============================] - 0s 36us/step - loss: 0.1781 - acc: 0.9190 - val_loss: 0.2330 - val_acc: 0.9085\n",
      "Epoch 177/300\n",
      "284/284 [==============================] - 0s 27us/step - loss: 0.1778 - acc: 0.9190 - val_loss: 0.2357 - val_acc: 0.9085\n",
      "Epoch 178/300\n",
      "284/284 [==============================] - 0s 46us/step - loss: 0.1774 - acc: 0.9190 - val_loss: 0.2356 - val_acc: 0.9085\n",
      "Epoch 179/300\n",
      "284/284 [==============================] - 0s 33us/step - loss: 0.1769 - acc: 0.9225 - val_loss: 0.2350 - val_acc: 0.9085\n",
      "Epoch 180/300\n",
      "284/284 [==============================] - 0s 35us/step - loss: 0.1762 - acc: 0.9190 - val_loss: 0.2331 - val_acc: 0.9225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/300\n",
      "284/284 [==============================] - 0s 35us/step - loss: 0.1753 - acc: 0.9190 - val_loss: 0.2307 - val_acc: 0.9225\n",
      "Epoch 182/300\n",
      "284/284 [==============================] - 0s 36us/step - loss: 0.1745 - acc: 0.9190 - val_loss: 0.2283 - val_acc: 0.9225\n",
      "Epoch 183/300\n",
      "284/284 [==============================] - 0s 50us/step - loss: 0.1742 - acc: 0.9225 - val_loss: 0.2267 - val_acc: 0.9225\n",
      "Epoch 184/300\n",
      "284/284 [==============================] - 0s 43us/step - loss: 0.1738 - acc: 0.9225 - val_loss: 0.2265 - val_acc: 0.9225\n",
      "Epoch 185/300\n",
      "284/284 [==============================] - 0s 45us/step - loss: 0.1732 - acc: 0.9225 - val_loss: 0.2274 - val_acc: 0.9155\n",
      "Epoch 186/300\n",
      "284/284 [==============================] - 0s 41us/step - loss: 0.1720 - acc: 0.9261 - val_loss: 0.2300 - val_acc: 0.9155\n",
      "Epoch 187/300\n",
      "284/284 [==============================] - 0s 49us/step - loss: 0.1719 - acc: 0.9261 - val_loss: 0.2332 - val_acc: 0.9085\n",
      "Epoch 188/300\n",
      "284/284 [==============================] - 0s 44us/step - loss: 0.1713 - acc: 0.9296 - val_loss: 0.2351 - val_acc: 0.9085\n",
      "Epoch 189/300\n",
      "284/284 [==============================] - 0s 46us/step - loss: 0.1711 - acc: 0.9261 - val_loss: 0.2348 - val_acc: 0.9085\n",
      "Epoch 190/300\n",
      "284/284 [==============================] - 0s 41us/step - loss: 0.1707 - acc: 0.9296 - val_loss: 0.2343 - val_acc: 0.9085\n",
      "Epoch 191/300\n",
      "284/284 [==============================] - 0s 42us/step - loss: 0.1702 - acc: 0.9296 - val_loss: 0.2331 - val_acc: 0.9085\n",
      "Epoch 192/300\n",
      "284/284 [==============================] - 0s 39us/step - loss: 0.1696 - acc: 0.9296 - val_loss: 0.2349 - val_acc: 0.9085\n",
      "Epoch 193/300\n",
      "284/284 [==============================] - 0s 41us/step - loss: 0.1692 - acc: 0.9331 - val_loss: 0.2359 - val_acc: 0.9085\n",
      "Epoch 194/300\n",
      "284/284 [==============================] - 0s 37us/step - loss: 0.1687 - acc: 0.9296 - val_loss: 0.2342 - val_acc: 0.9085\n",
      "Epoch 195/300\n",
      "284/284 [==============================] - 0s 40us/step - loss: 0.1682 - acc: 0.9296 - val_loss: 0.2315 - val_acc: 0.9155\n",
      "Epoch 196/300\n",
      "284/284 [==============================] - 0s 46us/step - loss: 0.1673 - acc: 0.9296 - val_loss: 0.2306 - val_acc: 0.9155\n",
      "Epoch 197/300\n",
      "284/284 [==============================] - 0s 36us/step - loss: 0.1671 - acc: 0.9296 - val_loss: 0.2319 - val_acc: 0.9155\n",
      "Epoch 198/300\n",
      "284/284 [==============================] - 0s 42us/step - loss: 0.1664 - acc: 0.9296 - val_loss: 0.2317 - val_acc: 0.9155\n",
      "Epoch 199/300\n",
      "284/284 [==============================] - 0s 49us/step - loss: 0.1666 - acc: 0.9296 - val_loss: 0.2310 - val_acc: 0.9085\n",
      "Epoch 200/300\n",
      "284/284 [==============================] - 0s 42us/step - loss: 0.1654 - acc: 0.9296 - val_loss: 0.2333 - val_acc: 0.9085\n",
      "Epoch 201/300\n",
      "284/284 [==============================] - 0s 36us/step - loss: 0.1654 - acc: 0.9261 - val_loss: 0.2370 - val_acc: 0.9155\n",
      "Epoch 202/300\n",
      "284/284 [==============================] - 0s 42us/step - loss: 0.1657 - acc: 0.9261 - val_loss: 0.2399 - val_acc: 0.9155\n",
      "Epoch 203/300\n",
      "284/284 [==============================] - 0s 36us/step - loss: 0.1669 - acc: 0.9261 - val_loss: 0.2421 - val_acc: 0.9155\n",
      "Epoch 204/300\n",
      "284/284 [==============================] - 0s 42us/step - loss: 0.1652 - acc: 0.9296 - val_loss: 0.2395 - val_acc: 0.9155\n",
      "Epoch 205/300\n",
      "284/284 [==============================] - 0s 45us/step - loss: 0.1631 - acc: 0.9331 - val_loss: 0.2356 - val_acc: 0.9155\n",
      "Epoch 206/300\n",
      "284/284 [==============================] - 0s 48us/step - loss: 0.1633 - acc: 0.9331 - val_loss: 0.2301 - val_acc: 0.8944\n",
      "Epoch 207/300\n",
      "284/284 [==============================] - 0s 49us/step - loss: 0.1635 - acc: 0.9261 - val_loss: 0.2279 - val_acc: 0.8944\n",
      "Epoch 208/300\n",
      "284/284 [==============================] - 0s 36us/step - loss: 0.1640 - acc: 0.9225 - val_loss: 0.2229 - val_acc: 0.9014\n",
      "Epoch 209/300\n",
      "284/284 [==============================] - 0s 44us/step - loss: 0.1632 - acc: 0.9261 - val_loss: 0.2230 - val_acc: 0.9014\n",
      "Epoch 210/300\n",
      "284/284 [==============================] - 0s 33us/step - loss: 0.1619 - acc: 0.9261 - val_loss: 0.2264 - val_acc: 0.8944\n",
      "Epoch 211/300\n",
      "284/284 [==============================] - 0s 38us/step - loss: 0.1597 - acc: 0.9296 - val_loss: 0.2304 - val_acc: 0.8944\n",
      "Epoch 212/300\n",
      "284/284 [==============================] - 0s 39us/step - loss: 0.1579 - acc: 0.9331 - val_loss: 0.2335 - val_acc: 0.8944\n",
      "Epoch 213/300\n",
      "284/284 [==============================] - 0s 38us/step - loss: 0.1575 - acc: 0.9331 - val_loss: 0.2353 - val_acc: 0.9014\n",
      "Epoch 214/300\n",
      "284/284 [==============================] - 0s 52us/step - loss: 0.1576 - acc: 0.9296 - val_loss: 0.2361 - val_acc: 0.8944\n",
      "Epoch 215/300\n",
      "284/284 [==============================] - 0s 48us/step - loss: 0.1574 - acc: 0.9261 - val_loss: 0.2334 - val_acc: 0.8873\n",
      "Epoch 216/300\n",
      "284/284 [==============================] - 0s 35us/step - loss: 0.1567 - acc: 0.9225 - val_loss: 0.2297 - val_acc: 0.8873\n",
      "Epoch 217/300\n",
      "284/284 [==============================] - 0s 34us/step - loss: 0.1565 - acc: 0.9225 - val_loss: 0.2266 - val_acc: 0.8944\n",
      "Epoch 218/300\n",
      "284/284 [==============================] - 0s 39us/step - loss: 0.1553 - acc: 0.9261 - val_loss: 0.2274 - val_acc: 0.8944\n",
      "Epoch 219/300\n",
      "284/284 [==============================] - 0s 52us/step - loss: 0.1545 - acc: 0.9296 - val_loss: 0.2258 - val_acc: 0.8944\n",
      "Epoch 220/300\n",
      "284/284 [==============================] - 0s 49us/step - loss: 0.1544 - acc: 0.9296 - val_loss: 0.2225 - val_acc: 0.8944\n",
      "Epoch 221/300\n",
      "284/284 [==============================] - 0s 32us/step - loss: 0.1545 - acc: 0.9296 - val_loss: 0.2228 - val_acc: 0.8944\n",
      "Epoch 222/300\n",
      "284/284 [==============================] - 0s 46us/step - loss: 0.1535 - acc: 0.9296 - val_loss: 0.2252 - val_acc: 0.8944\n",
      "Epoch 223/300\n",
      "284/284 [==============================] - 0s 39us/step - loss: 0.1518 - acc: 0.9296 - val_loss: 0.2276 - val_acc: 0.8944\n",
      "Epoch 224/300\n",
      "284/284 [==============================] - 0s 32us/step - loss: 0.1519 - acc: 0.9261 - val_loss: 0.2296 - val_acc: 0.8944\n",
      "Epoch 225/300\n",
      "284/284 [==============================] - 0s 39us/step - loss: 0.1523 - acc: 0.9225 - val_loss: 0.2315 - val_acc: 0.8944\n",
      "Epoch 226/300\n",
      "284/284 [==============================] - 0s 46us/step - loss: 0.1526 - acc: 0.9261 - val_loss: 0.2314 - val_acc: 0.8944\n",
      "Epoch 227/300\n",
      "284/284 [==============================] - ETA: 0s - loss: 0.1537 - acc: 0.914 - 0s 32us/step - loss: 0.1510 - acc: 0.9261 - val_loss: 0.2334 - val_acc: 0.8944\n",
      "Epoch 228/300\n",
      "284/284 [==============================] - 0s 31us/step - loss: 0.1492 - acc: 0.9366 - val_loss: 0.2331 - val_acc: 0.9014\n",
      "Epoch 229/300\n",
      "284/284 [==============================] - 0s 32us/step - loss: 0.1486 - acc: 0.9401 - val_loss: 0.2332 - val_acc: 0.9014\n",
      "Epoch 230/300\n",
      "284/284 [==============================] - 0s 30us/step - loss: 0.1497 - acc: 0.9401 - val_loss: 0.2334 - val_acc: 0.9014\n",
      "Epoch 231/300\n",
      "284/284 [==============================] - 0s 37us/step - loss: 0.1504 - acc: 0.9437 - val_loss: 0.2319 - val_acc: 0.9014\n",
      "Epoch 232/300\n",
      "284/284 [==============================] - 0s 42us/step - loss: 0.1504 - acc: 0.9437 - val_loss: 0.2315 - val_acc: 0.9014\n",
      "Epoch 233/300\n",
      "284/284 [==============================] - 0s 28us/step - loss: 0.1496 - acc: 0.9437 - val_loss: 0.2322 - val_acc: 0.9014\n",
      "Epoch 234/300\n",
      "284/284 [==============================] - 0s 32us/step - loss: 0.1480 - acc: 0.9401 - val_loss: 0.2343 - val_acc: 0.9014\n",
      "Epoch 235/300\n",
      "284/284 [==============================] - 0s 31us/step - loss: 0.1472 - acc: 0.9437 - val_loss: 0.2361 - val_acc: 0.9014\n",
      "Epoch 236/300\n",
      "284/284 [==============================] - 0s 32us/step - loss: 0.1463 - acc: 0.9401 - val_loss: 0.2361 - val_acc: 0.9014\n",
      "Epoch 237/300\n",
      "284/284 [==============================] - 0s 34us/step - loss: 0.1460 - acc: 0.9401 - val_loss: 0.2329 - val_acc: 0.9014\n",
      "Epoch 238/300\n",
      "284/284 [==============================] - 0s 31us/step - loss: 0.1453 - acc: 0.9331 - val_loss: 0.2262 - val_acc: 0.8944\n",
      "Epoch 239/300\n",
      "284/284 [==============================] - 0s 34us/step - loss: 0.1457 - acc: 0.9296 - val_loss: 0.2236 - val_acc: 0.8944\n",
      "Epoch 240/300\n",
      "284/284 [==============================] - 0s 38us/step - loss: 0.1456 - acc: 0.9296 - val_loss: 0.2246 - val_acc: 0.8944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/300\n",
      "284/284 [==============================] - 0s 34us/step - loss: 0.1448 - acc: 0.9261 - val_loss: 0.2296 - val_acc: 0.8873\n",
      "Epoch 242/300\n",
      "284/284 [==============================] - 0s 30us/step - loss: 0.1444 - acc: 0.9261 - val_loss: 0.2362 - val_acc: 0.8944\n",
      "Epoch 243/300\n",
      "284/284 [==============================] - 0s 35us/step - loss: 0.1442 - acc: 0.9331 - val_loss: 0.2396 - val_acc: 0.9014\n",
      "Epoch 244/300\n",
      "284/284 [==============================] - 0s 27us/step - loss: 0.1442 - acc: 0.9366 - val_loss: 0.2428 - val_acc: 0.9014\n",
      "Epoch 245/300\n",
      "284/284 [==============================] - 0s 38us/step - loss: 0.1440 - acc: 0.9366 - val_loss: 0.2401 - val_acc: 0.9014\n",
      "Epoch 246/300\n",
      "284/284 [==============================] - 0s 29us/step - loss: 0.1424 - acc: 0.9366 - val_loss: 0.2315 - val_acc: 0.8944\n",
      "Epoch 247/300\n",
      "284/284 [==============================] - 0s 36us/step - loss: 0.1430 - acc: 0.9331 - val_loss: 0.2289 - val_acc: 0.8944\n",
      "Epoch 248/300\n",
      "284/284 [==============================] - 0s 28us/step - loss: 0.1425 - acc: 0.9331 - val_loss: 0.2303 - val_acc: 0.8873\n",
      "Epoch 249/300\n",
      "284/284 [==============================] - 0s 44us/step - loss: 0.1417 - acc: 0.9401 - val_loss: 0.2348 - val_acc: 0.8944\n",
      "Epoch 250/300\n",
      "284/284 [==============================] - 0s 35us/step - loss: 0.1408 - acc: 0.9401 - val_loss: 0.2383 - val_acc: 0.8944\n",
      "Epoch 251/300\n",
      "284/284 [==============================] - 0s 41us/step - loss: 0.1407 - acc: 0.9366 - val_loss: 0.2385 - val_acc: 0.8944\n",
      "Epoch 252/300\n",
      "284/284 [==============================] - 0s 39us/step - loss: 0.1403 - acc: 0.9401 - val_loss: 0.2361 - val_acc: 0.8944\n",
      "Epoch 253/300\n",
      "284/284 [==============================] - 0s 28us/step - loss: 0.1396 - acc: 0.9401 - val_loss: 0.2355 - val_acc: 0.8873\n",
      "Epoch 254/300\n",
      "284/284 [==============================] - 0s 29us/step - loss: 0.1392 - acc: 0.9401 - val_loss: 0.2352 - val_acc: 0.8873\n",
      "Epoch 255/300\n",
      "284/284 [==============================] - 0s 29us/step - loss: 0.1390 - acc: 0.9401 - val_loss: 0.2343 - val_acc: 0.8944\n",
      "Epoch 256/300\n",
      "284/284 [==============================] - 0s 42us/step - loss: 0.1381 - acc: 0.9401 - val_loss: 0.2354 - val_acc: 0.8944\n",
      "Epoch 257/300\n",
      "284/284 [==============================] - 0s 45us/step - loss: 0.1383 - acc: 0.9401 - val_loss: 0.2350 - val_acc: 0.9014\n",
      "Epoch 258/300\n",
      "284/284 [==============================] - 0s 46us/step - loss: 0.1386 - acc: 0.9401 - val_loss: 0.2358 - val_acc: 0.8944\n",
      "Epoch 259/300\n",
      "284/284 [==============================] - 0s 42us/step - loss: 0.1381 - acc: 0.9401 - val_loss: 0.2386 - val_acc: 0.9014\n",
      "Epoch 260/300\n",
      "284/284 [==============================] - 0s 49us/step - loss: 0.1377 - acc: 0.9437 - val_loss: 0.2382 - val_acc: 0.9014\n",
      "Epoch 261/300\n",
      "284/284 [==============================] - 0s 38us/step - loss: 0.1364 - acc: 0.9437 - val_loss: 0.2368 - val_acc: 0.8944\n",
      "Epoch 262/300\n",
      "284/284 [==============================] - 0s 38us/step - loss: 0.1355 - acc: 0.9401 - val_loss: 0.2328 - val_acc: 0.8944\n",
      "Epoch 263/300\n",
      "284/284 [==============================] - 0s 43us/step - loss: 0.1361 - acc: 0.9437 - val_loss: 0.2309 - val_acc: 0.8873\n",
      "Epoch 264/300\n",
      "284/284 [==============================] - 0s 32us/step - loss: 0.1368 - acc: 0.9437 - val_loss: 0.2274 - val_acc: 0.8873\n",
      "Epoch 265/300\n",
      "284/284 [==============================] - 0s 34us/step - loss: 0.1385 - acc: 0.9366 - val_loss: 0.2267 - val_acc: 0.8944\n",
      "Epoch 266/300\n",
      "284/284 [==============================] - 0s 36us/step - loss: 0.1387 - acc: 0.9366 - val_loss: 0.2287 - val_acc: 0.8944\n",
      "Epoch 267/300\n",
      "284/284 [==============================] - 0s 48us/step - loss: 0.1375 - acc: 0.9366 - val_loss: 0.2322 - val_acc: 0.8944\n",
      "Epoch 268/300\n",
      "284/284 [==============================] - 0s 45us/step - loss: 0.1361 - acc: 0.9401 - val_loss: 0.2332 - val_acc: 0.8873\n",
      "Epoch 269/300\n",
      "284/284 [==============================] - 0s 38us/step - loss: 0.1351 - acc: 0.9366 - val_loss: 0.2324 - val_acc: 0.8873\n",
      "Epoch 270/300\n",
      "284/284 [==============================] - 0s 41us/step - loss: 0.1349 - acc: 0.9331 - val_loss: 0.2321 - val_acc: 0.8873\n",
      "Epoch 271/300\n",
      "284/284 [==============================] - 0s 35us/step - loss: 0.1343 - acc: 0.9331 - val_loss: 0.2309 - val_acc: 0.8944\n",
      "Epoch 272/300\n",
      "284/284 [==============================] - 0s 37us/step - loss: 0.1348 - acc: 0.9331 - val_loss: 0.2312 - val_acc: 0.8944\n",
      "Epoch 273/300\n",
      "284/284 [==============================] - 0s 45us/step - loss: 0.1339 - acc: 0.9366 - val_loss: 0.2341 - val_acc: 0.8873\n",
      "Epoch 274/300\n",
      "284/284 [==============================] - 0s 57us/step - loss: 0.1329 - acc: 0.9366 - val_loss: 0.2408 - val_acc: 0.8944\n",
      "Epoch 275/300\n",
      "284/284 [==============================] - 0s 40us/step - loss: 0.1327 - acc: 0.9437 - val_loss: 0.2459 - val_acc: 0.8944\n",
      "Epoch 276/300\n",
      "284/284 [==============================] - 0s 38us/step - loss: 0.1341 - acc: 0.9437 - val_loss: 0.2488 - val_acc: 0.8944\n",
      "Epoch 277/300\n",
      "284/284 [==============================] - 0s 35us/step - loss: 0.1333 - acc: 0.9437 - val_loss: 0.2463 - val_acc: 0.8944\n",
      "Epoch 278/300\n",
      "284/284 [==============================] - 0s 37us/step - loss: 0.1320 - acc: 0.9401 - val_loss: 0.2405 - val_acc: 0.8944\n",
      "Epoch 279/300\n",
      "284/284 [==============================] - 0s 44us/step - loss: 0.1307 - acc: 0.9437 - val_loss: 0.2378 - val_acc: 0.8944\n",
      "Epoch 280/300\n",
      "284/284 [==============================] - 0s 37us/step - loss: 0.1301 - acc: 0.9401 - val_loss: 0.2366 - val_acc: 0.8944\n",
      "Epoch 281/300\n",
      "284/284 [==============================] - 0s 34us/step - loss: 0.1306 - acc: 0.9437 - val_loss: 0.2350 - val_acc: 0.8873\n",
      "Epoch 282/300\n",
      "284/284 [==============================] - 0s 38us/step - loss: 0.1311 - acc: 0.9437 - val_loss: 0.2363 - val_acc: 0.8873\n",
      "Epoch 283/300\n",
      "284/284 [==============================] - 0s 47us/step - loss: 0.1300 - acc: 0.9437 - val_loss: 0.2410 - val_acc: 0.9014\n",
      "Epoch 284/300\n",
      "284/284 [==============================] - 0s 39us/step - loss: 0.1294 - acc: 0.9472 - val_loss: 0.2441 - val_acc: 0.9014\n",
      "Epoch 285/300\n",
      "284/284 [==============================] - 0s 26us/step - loss: 0.1293 - acc: 0.9472 - val_loss: 0.2455 - val_acc: 0.8944\n",
      "Epoch 286/300\n",
      "284/284 [==============================] - 0s 44us/step - loss: 0.1292 - acc: 0.9437 - val_loss: 0.2445 - val_acc: 0.8944\n",
      "Epoch 287/300\n",
      "284/284 [==============================] - 0s 33us/step - loss: 0.1287 - acc: 0.9437 - val_loss: 0.2419 - val_acc: 0.8944\n",
      "Epoch 288/300\n",
      "284/284 [==============================] - 0s 34us/step - loss: 0.1281 - acc: 0.9437 - val_loss: 0.2419 - val_acc: 0.8944\n",
      "Epoch 289/300\n",
      "284/284 [==============================] - 0s 29us/step - loss: 0.1281 - acc: 0.9437 - val_loss: 0.2441 - val_acc: 0.9014\n",
      "Epoch 290/300\n",
      "284/284 [==============================] - 0s 29us/step - loss: 0.1283 - acc: 0.9472 - val_loss: 0.2441 - val_acc: 0.8944\n",
      "Epoch 291/300\n",
      "284/284 [==============================] - 0s 41us/step - loss: 0.1275 - acc: 0.9472 - val_loss: 0.2407 - val_acc: 0.8944\n",
      "Epoch 292/300\n",
      "284/284 [==============================] - 0s 34us/step - loss: 0.1274 - acc: 0.9401 - val_loss: 0.2383 - val_acc: 0.8873\n",
      "Epoch 293/300\n",
      "284/284 [==============================] - 0s 38us/step - loss: 0.1271 - acc: 0.9366 - val_loss: 0.2369 - val_acc: 0.8873\n",
      "Epoch 294/300\n",
      "284/284 [==============================] - 0s 24us/step - loss: 0.1277 - acc: 0.9366 - val_loss: 0.2339 - val_acc: 0.8803\n",
      "Epoch 295/300\n",
      "284/284 [==============================] - 0s 30us/step - loss: 0.1278 - acc: 0.9366 - val_loss: 0.2364 - val_acc: 0.8944\n",
      "Epoch 296/300\n",
      "284/284 [==============================] - 0s 52us/step - loss: 0.1275 - acc: 0.9366 - val_loss: 0.2393 - val_acc: 0.8873\n",
      "Epoch 297/300\n",
      "284/284 [==============================] - 0s 32us/step - loss: 0.1259 - acc: 0.9366 - val_loss: 0.2425 - val_acc: 0.8944\n",
      "Epoch 298/300\n",
      "284/284 [==============================] - 0s 29us/step - loss: 0.1253 - acc: 0.9401 - val_loss: 0.2500 - val_acc: 0.8873\n",
      "Epoch 299/300\n",
      "284/284 [==============================] - 0s 38us/step - loss: 0.1276 - acc: 0.9366 - val_loss: 0.2570 - val_acc: 0.8873\n",
      "Epoch 300/300\n",
      "284/284 [==============================] - 0s 32us/step - loss: 0.1284 - acc: 0.9401 - val_loss: 0.2525 - val_acc: 0.9014\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(5,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_hist = model.fit(trainData, trainTarget, epochs=300, batch_size=128, validation_data=(valData, valTarget))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network has been trained with 2 hidden layers of 64 nodes. Let's look at the results through a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_acc = model_hist.history['acc']\n",
    "model_loss = model_hist.history['loss']\n",
    "model_val_acc = model_hist.history['val_acc']\n",
    "model_val_loss = model_hist.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(model_acc) + 1)\n",
    "# b+ is for \"blue cross\"\n",
    "plt.plot(epochs, model_val_loss, 'b+', label='Val Loss')\n",
    "plt.plot(epochs, model_loss, 'b', label='Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a first glance, I can tell that the data is being over fit at ~25 epochs. Thus, I will retrain the data up to 50 epochs to combat overfitting to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 284 samples, validate on 142 samples\n",
      "Epoch 1/25\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 0.7124 - acc: 0.5317 - val_loss: 0.6733 - val_acc: 0.5634\n",
      "Epoch 2/25\n",
      "284/284 [==============================] - 0s 41us/step - loss: 0.6562 - acc: 0.6620 - val_loss: 0.6180 - val_acc: 0.7113\n",
      "Epoch 3/25\n",
      "284/284 [==============================] - 0s 38us/step - loss: 0.6030 - acc: 0.8099 - val_loss: 0.5682 - val_acc: 0.8451\n",
      "Epoch 4/25\n",
      "284/284 [==============================] - 0s 28us/step - loss: 0.5548 - acc: 0.8521 - val_loss: 0.5238 - val_acc: 0.8592\n",
      "Epoch 5/25\n",
      "284/284 [==============================] - 0s 51us/step - loss: 0.5103 - acc: 0.8803 - val_loss: 0.4852 - val_acc: 0.8592\n",
      "Epoch 6/25\n",
      "284/284 [==============================] - 0s 35us/step - loss: 0.4723 - acc: 0.8803 - val_loss: 0.4512 - val_acc: 0.8803\n",
      "Epoch 7/25\n",
      "284/284 [==============================] - 0s 33us/step - loss: 0.4381 - acc: 0.8732 - val_loss: 0.4210 - val_acc: 0.8803\n",
      "Epoch 8/25\n",
      "284/284 [==============================] - 0s 25us/step - loss: 0.4073 - acc: 0.8697 - val_loss: 0.3943 - val_acc: 0.8803\n",
      "Epoch 9/25\n",
      "284/284 [==============================] - 0s 51us/step - loss: 0.3812 - acc: 0.8768 - val_loss: 0.3710 - val_acc: 0.8873\n",
      "Epoch 10/25\n",
      "284/284 [==============================] - 0s 29us/step - loss: 0.3570 - acc: 0.8768 - val_loss: 0.3509 - val_acc: 0.8873\n",
      "Epoch 11/25\n",
      "284/284 [==============================] - 0s 40us/step - loss: 0.3369 - acc: 0.8768 - val_loss: 0.3337 - val_acc: 0.8873\n",
      "Epoch 12/25\n",
      "284/284 [==============================] - 0s 39us/step - loss: 0.3198 - acc: 0.8803 - val_loss: 0.3187 - val_acc: 0.8873\n",
      "Epoch 13/25\n",
      "284/284 [==============================] - 0s 35us/step - loss: 0.3046 - acc: 0.8873 - val_loss: 0.3058 - val_acc: 0.8873\n",
      "Epoch 14/25\n",
      "284/284 [==============================] - 0s 36us/step - loss: 0.2920 - acc: 0.8873 - val_loss: 0.2949 - val_acc: 0.8944\n",
      "Epoch 15/25\n",
      "284/284 [==============================] - 0s 45us/step - loss: 0.2814 - acc: 0.8873 - val_loss: 0.2860 - val_acc: 0.8944\n",
      "Epoch 16/25\n",
      "284/284 [==============================] - 0s 36us/step - loss: 0.2729 - acc: 0.8873 - val_loss: 0.2787 - val_acc: 0.9085\n",
      "Epoch 17/25\n",
      "284/284 [==============================] - 0s 56us/step - loss: 0.2662 - acc: 0.8873 - val_loss: 0.2729 - val_acc: 0.9085\n",
      "Epoch 18/25\n",
      "284/284 [==============================] - 0s 57us/step - loss: 0.2612 - acc: 0.8838 - val_loss: 0.2687 - val_acc: 0.9085\n",
      "Epoch 19/25\n",
      "284/284 [==============================] - 0s 36us/step - loss: 0.2569 - acc: 0.8838 - val_loss: 0.2660 - val_acc: 0.9085\n",
      "Epoch 20/25\n",
      "284/284 [==============================] - 0s 39us/step - loss: 0.2543 - acc: 0.8838 - val_loss: 0.2645 - val_acc: 0.9085\n",
      "Epoch 21/25\n",
      "284/284 [==============================] - 0s 29us/step - loss: 0.2510 - acc: 0.8838 - val_loss: 0.2642 - val_acc: 0.9085\n",
      "Epoch 22/25\n",
      "284/284 [==============================] - 0s 30us/step - loss: 0.2501 - acc: 0.8873 - val_loss: 0.2641 - val_acc: 0.9085\n",
      "Epoch 23/25\n",
      "284/284 [==============================] - 0s 33us/step - loss: 0.2481 - acc: 0.8873 - val_loss: 0.2625 - val_acc: 0.9085\n",
      "Epoch 24/25\n",
      "284/284 [==============================] - 0s 40us/step - loss: 0.2468 - acc: 0.8908 - val_loss: 0.2618 - val_acc: 0.9085\n",
      "Epoch 25/25\n",
      "284/284 [==============================] - 0s 35us/step - loss: 0.2455 - acc: 0.8908 - val_loss: 0.2600 - val_acc: 0.9085\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(5,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_hist = model.fit(trainData, trainTarget, epochs=25, batch_size=128, validation_data=(valData, valTarget))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_acc = model_hist.history['acc']\n",
    "model_loss = model_hist.history['loss']\n",
    "model_val_acc = model_hist.history['val_acc']\n",
    "model_val_loss = model_hist.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(model_acc) + 1)\n",
    "# b+ is for \"blue cross\"\n",
    "plt.plot(epochs, model_val_loss, 'b+', label='Val Loss')\n",
    "plt.plot(epochs, model_loss, 'b', label='Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above graph looks so much better and doesn't seem to overfit the train data yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 0s 99us/step\n",
      "Test Loss:  0.22390625287186017\n",
      "Test Accuracy:  0.9230769230769231\n",
      "Number of patients who have breast cancer in test data percentage:  63.63636363636363\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(testData, testTarget)\n",
    "print(\"Test Loss: \", test_loss)\n",
    "print(\"Test Accuracy: \", test_acc)\n",
    "\n",
    "numBC = 0\n",
    "for i in range(len(testTarget)):\n",
    "    if testTarget[i] == 1:\n",
    "        numBC += 1\n",
    "BCPer = numBC/len(testTarget)\n",
    "print(\"Number of patients who have breast cancer in test data percentage: \", BCPer*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the fact that over 60% of the patients has breast cancer and our model diagnosed 92% of the patients correctly, we are very happy with the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
